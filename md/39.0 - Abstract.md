# Dedication

To those in the osu! tournament community, without whom I would have never embarked on this journey;

To my numerous teachers and professors, especially Keith Lightfoot, Rodney Rogers, Marc Miller, and Gabbi Bachand, who I have limitless and appreciation and admiration for;

To those part of the United States Cyber Team and the broader CTF community, for igniting my interest in digital forensics and supporting me even when I flailed like a fish out of water;

And, of course, to my friends, family, and bed, who provided motivation when there was none.

# Acknowledgments

~~i acknowledge i am lazy...~~

I want to express my immense gratitude to Nancy Latourrette for her support, guidance, and mentorship throughout the development of this thesis. This thesis would be nowhere without her ideas and experience, and I am truly grateful and honored to have been able to work with her throughout this experience. 

I would also like to thank Bill Doherty for his review of a prior paper, from which some of this content is derived. 

# Abstract

As our world becomes increasingly dependent on technology, the advancement of digital forensics has become a key focus in the fight against cybercrime. The field depends greatly on the availability of disk images, network captures, and other forensic datasets for education, tool validation, and research. However, real-world datasets often contain sensitive information that may be difficult to remove, making them difficult to distribute publicly. As a result, researchers and educators can encounter gaps in available datasets, often leading to the manual development of new, suitable datasets. While viable, this approach is time-consuming and rarely produces datasets that accurately reflect real-world scenarios suitable for comprehensive training and education. In turn, there is ongoing research into forensic synthesizers, which automate the process of creating unique synthetic datasets that can be publicly distributed without legal and logistical concerns. 

This thesis introduces the *automated kinetic framework*, or AKF, a modular synthesizer for creating and interacting with virtualized environments to simulate user activity. AKF significantly improves upon the architectural designs of prior synthesizers while largely maintaining feature parity and usability. Additionally, AKF leverages the CASE standard to provide human- and machine-readable reporting, exposing low-level details in a searchable format. Finally, AKF provides options for leveraging generative AI to develop high-level scenarios as well as individual artifacts. These contributions are intended to not only improve the speed at which synthetic datasets can be created, but also ensure the long-term usefulness of AKF-generated datasets and the framework as a whole.

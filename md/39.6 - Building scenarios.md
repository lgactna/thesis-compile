

This chapter addresses the modules responsible for allowing users to invoke the framework through a standard Python script and a high-level YAML file. It also addresses the generative AI modules that can assist a user in building a scenario. These modules are depicted in **!fig:scenarios-simple** below, with a more detailed diagram located in **39.A - Architectural diagrams** as **!fig:architecture-full-a**.

![Simplified AKF architecture diagram for scenario construction modules](scenarios-simple.png){#fig:scenarios-simple}

At this point, we have provided the functionality for automating artifact generation in a near-deterministic manner with comprehensive logging and reporting. However, there is still the challenge of exposing this functionality in a user-friendly manner. At a high level, there are two primary ways to define an input to a synthesizer. These formats describe the process (the *how*) of placing artifacts and performing actions:

- An **imperative** format, in which the synthesizer is provided instructions in an imperative programming language, and the developer must provide the exact instructions for the synthesizer to take through some exposed API. 
- A **declarative** format, in which the synthesizer is provided a file that describes the desired elements of the result, and it is up to the synthesizer to execute the instructions necessary to achieve the result.

However, the challenge of deciding *what* actions to perform remains. It is still mainly the responsibility of instructors to provide background noise and other realistic artifacts to insert into a scenario. Additionally, although synthesizers make it easier to place files at desired locations or visit websites, these scenario elements must be defined and created beforehand. More precisely, these represent challenges in two specific areas:

- **Generating standalone artifacts**: Users must include email conversations, documents, and other application-specific artifacts to be generated as part of a scenario.
- **Creating the scenario itself**: Users must combine these standalone artifacts to generate a cohesive scenario, following a theme such as corporate espionage or ransomware attacks.

This chapter addresses the challenges of providing APIs for complex GUI-driven applications and creating background noise. More precisely, we address two questions – how do we invoke AKF's automation systems, and how does AKF assist a user in building a scenario? Here, we explore AKF's imperative and declarative syntaxes and the viability of using generative AI to assist in building individual files and complete scenario descriptions. 

# 6.1 - Scripting background

We begin by analyzing how synthesizers accept instructions for execution – more precisely, how do users define the sequence of operations that the synthesizer should take to create the dataset? 

For many of the frameworks created in the last decade, users define scenarios by using a Python library to interact with the framework. The library is responsible for setting up the virtualized environment and performing high-level actions on the environment, abstracting away the underlying calls to the hypervisor from the scenario developer. This code-based approach represents an *imperative* strategy for scenario creation, where the user describes how the dataset should be created by defining the exact order and means to perform synthesizer actions. (It is worth noting that the language used to interact with the synthesizer's API does not need to match the language used to implement the synthesizer itself (although this is often the case). For example, the automation framework Playwright is implemented in TypeScript, initially exposing a Node.JS API [@MicrosoftPlaywrightpython2025]. Today, Playwright provides APIs in Python, Java, and C#.)

In contrast, custom scenario formats provided by D-FET [@williamCloudbasedDigitalForensics2011], SFX [@russellForensicImageDescription2012], and Yannikos et al. [@yannikosDataCorporaDigital2014] follow a different approach. For these synthesizers, a custom high-level language describes the desired final state of the dataset. Instead of importing libraries and writing code, users state the desired elements of the forensic dataset, allowing the synthesizer to decide how to create the desired dataset – a *declarative* strategy for scenario creation. The specifics of state management and execution are delegated to the synthesizer. 

Consider the following declarative SFX code taken from Russell et al. shown in **!lst:6.1a** [@russellForensicImageDescription2012]:

```xml
!lst:6.1a|Declarative SFX scenario with web browsing
<disk>
	<partition index="p1" hidden="0" size="48M" type="ntfs">
		<base os="windows7x64"/>
		<user username = "Gordon">
			 <browserhistory browser="firefox">
				 <url link="bbc.co.uk" time="13:14:00 1 Jan 2013"/>
			 </browserhistory>
		</user>
	</partition>
</disk>
```

Here, a Windows 7 partition is created as part of a larger disk image. The partition is loaded in a virtual machine to create a user called "Gordon," who uses Firefox to browse the internet. (This simple web browsing scenario will be reused throughout this chapter to demonstrate several code examples.)

The same might be expressed in ForTrace [@gobelForTraceHolisticForensic2022] as shown in **!lst:6.1b**, excluding additional code required for ground truth and disk image generation:

```python
!lst:6.1b|Imperative ForTrace scenario with web browsing [@gobelForTraceHolisticForensic2022]
from fortrace.core.vmm import Vmm
from fortrace.utility.logger_helper import create_logger
from fortrace.core.vmm import GuestListener

if __name__ == "__main__":
	logger = create_logger('fortraceManager', logging.DEBUG)
	macsInUse = []
	guests = []
	
	guestListener = GuestListener(guests, logger)
	virtual_machine_monitor1 = Vmm(macsInUse, guests, logger)
	# boottime expressed as "%Y-%m-%d %H:%M:%S"
	guest = virtual_machine_monitor1.create_guest(guest_name="w-guest01", platform="windows", boottime="2013-01-01 13:14:00")

	browser_obj = guest.application("webBrowserFirefox", {'webBrowser': "firefox"})
	browser_obj.open(url="bbc.co.uk")
	while browser_obj.is_busy:
		time.sleep(2)
	browser_obj.close()
```

Although these two code blocks have the same expressive power (that is, they achieve the same overall outcomes), there is a clear difference in the complexity and length between them. It is significantly easier to read and write the declarative XML in the first code block, as it abstracts away the need to instantiate various synthesizer objects and call specific methods. By extension, this also allows for a common declarative syntax to be used across multiple synthesizers since low-level synthesizer details do not need to be exposed as part of the declarative syntax. 

The primary benefit of an imperative approach to generation is its flexibility; on a Python-based synthesizer, one can simply import another library to extend the functionality of the base scenario definition. This flexibility naturally comes at the expense of a greater learning curve. Although many digital forensic specialists are likely to have programming experience, it is far easier to learn a restricted declarative specification (like XML) than an entire programming language, which may entail additional setup (such as installing an IDE, dependencies, and so on). 

When accessibility is preferred over functionality, declarative syntaxes can be more valuable than imperative syntaxes. Some scenario developers, such as classroom instructors, may not need the low-level control provided by an imperative syntax or a complete programming language such as Python. It also takes time to learn about the functionality exposed by the synthesizer's library, not to mention learning the programming language itself. These are the primary motivators behind supporting well-defined declarative syntaxes. 

Of course, low-level control is still important, especially when external libraries must be used to implement functionality not inherently exposed by a synthesizer. For this reason, some synthesizers support both declarative and imperative scripts to generate scenarios. For example, the Python-based hystck and ForTrace frameworks [@gobelNovelApproachGenerating2020; @gobelForTraceHolisticForensic2022] allow users to write YAML scripts to execute actions. Support for both formats can be implemented through various means, such as declarative-to-imperative translators. (While not explored in this thesis, it is also worth noting the GUI-based interfaces provided by Yannikos et al. [@yannikosDataCorporaDigital2014] and ForGe [@vistiAutomaticCreationComputer2015] for building scenarios.)

AKF supports an imperative syntax (through its Python API) and a custom declarative syntax. Unlike prior synthesizers, AKF's declarative syntax supports both execution and declarative-to-imperative translation, allowing users to quickly create and modify imperative scripts from high-level declarative descriptions.

# 6.2 - Setup and basic usage

Like many of its predecessors, AKF implements its functionality and exposes its API in the same language, Python 3. There are numerous advantages to a Python-based API; besides the relatively low difficulty of setting up and using Python, its rich ecosystem allows scenarios to be extended through other libraries from the Python ecosystem. For example, if a user wanted to conditionally execute certain parts of a scenario by testing if a particular remote service is currently online, a user could use the Requests library [@Requests31Documentation] to issue an HTTP request out-of-band before performing the same action in a virtualized environment. 

Users must install two foundational technologies for AKF to operate – Python 3.11 (or later) and a supported hypervisor. AKF currently only supports VirtualBox as its hypervisor, though QEMU/libvirt has also been used in prior synthesizers. AKF uses `pyproject.toml` to define Python library dependencies, which can be installed into a virtual environment using a package manager such as `pip` or `uv`. 

At this point, a virtual machine must be prepared for use with AKF. As with prior synthesizers, it is possible to manually configure a machine by downloading a supported operating system and creating a new virtual machine from scratch. The manual process, which is similar to that of other synthesizers, is as follows:

- Download an ISO or pre-prepared virtual machine from a distributor with the desired operating system.
- If necessary, install the operating system on a new virtual machine.
- Configure the virtual machine with the desired host resources, including two network interfaces – one connected to the NAT adapter for general internet usage and one connected to the host-only adapter for agent communications.
- Create an administrative user with known credentials. Configure the operating system as desired to reduce friction with the synthesizer (such as disabling UAC prompts, enabling auto-logon, and so on.)
- Build and copy the OS-specific AKF agent to the virtual machine, configuring it as a startup application. Add firewall rules to ensure that the host and agent are able to communicate.

After this process, the virtual machine can be cloned and reused in multiple AKF scenarios. Although relatively straightforward, this process is still time-consuming, especially when adapting this process to new operating systems. While a prepared AKF virtual machine can theoretically be distributed (in a virtual appliance format such as OVF), this can run into legal issues if the software on the underlying operating system is copyrighted. 

As a result, AKF uses modern infrastructure-as-code solutions to vastly simplify the setup of new virtual machines. Vagrant, developed by HashiCorp, is a tool for rapidly building development environments [@HashicorpVagrant2025]. It allows users to define and build virtual machines on several virtualization platforms, including VirtualBox and VMWare. Virtual machines are built by configuring a base image according to a Vagrantfile, which describes hypervisor-specific configuration options and instructions to configure the machine. The Vagrantfile can be distributed to users, allowing them to build the same virtual machine without distributing virtual drives. (A similar approach of distributing the "differences" of base images is used to reduce the size of distributed forensic datasets by EviPlant [@scanlonEviPlantEfficientDigital2017], as described in **39.8 - Future work#8.4 - Distribution**.) 

The AKF Windows agent includes a Vagrantfile for creating a new Windows 11 virtual machine with the agent installed and configured, which can easily be adapted for other platforms and hypervisors. The Vagrantfile(s) used to generate a dataset should be included with the dataset itself to maximize reproducibility, as described in **39.5 - Output and validation#5.4 - Distribution and community reproducibility**. A robust ecosystem of Vagrant boxes for varying Linux distributions and Windows versions exists, many of which can be pulled from the Vagrant public registry [@hashicorpHashiCorpCloudPlatform]. When combined with the flexibility of Vagrant over multiple virtualization platforms, this can significantly improve the reproducibility and usability of AKF across many platforms. It should also be noted that Vagrant can configure and build larger environments with multiple machines. For organizations that can express corporate environments as Vagrantfiles, AKF could perform artifact generation at scale, allowing for incident response scenarios reflecting real-world networks and events.

Following setup, developers can build scenarios using the AKF core libraries (`akflib`) and the API of the platform-specific agent installed onto the virtual machine. This reflects typical imperative usage, in which environment setup, artifact generation, and output generation are handled explicitly through a script executed through the Python interpreter. 

A simple AKF script achieving the same outcomes as the ForTrace and SFX scripts above is shown in **!lst:6.2a**:

```python
!lst:6.2a|Example of an imperative AKF scenario
from akf_windows.api.chromium import ChromiumServiceAPI
from akflib.core.hypervisor.vbox import VBoxExportFormatEnum
from akflib.core.hypervisor.vbox import VBoxHypervisor
from pathlib import Path

# Instantiate a hypervisor object tied to a specific virtual machine

vbox_obj = VBoxHypervisor("akf-windows-1")

# Start the virtual machine

vbox_obj.start_vm(wait_for_guest_additions=True)

# Visit a single website

with ChromiumServiceAPI.auto_connect(vbox_obj.get_maintenance_ip()) as chromium_service:
    chromium_service.kill_edge()
    chromium_service.set_browser("msedge")
    page = chromium_service.browser.new_page()
    page.goto("[https://bbc.co.uk")](https://bbc.co.uk"))  

# Stop the virtual machine

vbox_obj.stop_vm(force=False)

# Export the virtual machine to a disk image

vbox_obj.create_disk_image(
    Path("C:/Users/kisun/Desktop/akf-windows_1.raw"),
    VBoxExportFormatEnum.RAW
)
```

Here, we create a new `VBoxHypervisor` instance that will allow us to start and interact with a VirtualBox machine called  `akf-windows-1` (created previously with Vagrant as described above). To initialize the Chromium subservice, we query VirtualBox for the IP address of the host-only adapter, then use the `auto_connect` convenience method to automatically perform the required RPyC setup and return an instance of `ChromiumServiceAPI`. This instantiated API allows us to interact with Microsoft Edge on the virtual machine over RPyC (which is abstracted away from the user); as part of the context manager, the RPyC connection is automatically closed at the end of the `with` block. Finally, we create a raw disk image and save it to the desktop.

With this in mind, how do we expose this same functionality through a declarative syntax?

# 6.3 - Declarative usage

As described previously, declarative inputs are well-structured files with a fixed set of available actions – effectively forming an API – where each entry in the file specifies an action or artifact to be generated as part of a dataset. Individual entries may contain additional configuration data that modifies how that specific action or artifact is generated. An *interpreter* is responsible for parsing and acting on the entries in the declarative file.

Interpreters can act on declarative formats in one of two ways. The first is *execution*, in which the elements of the declarative script are directly interpreted to generate imperative API calls. This is characteristic of synthesizers that only support declarative script inputs, exposing no low-level APIs. Execution takes advantage of the high-level nature of declarative scripts; a declarative script can remain the same even if the libraries that execute it change, so long as the interpreter is updated accordingly. The second is *translation*, in which the declarative script is used to generate an equivalent imperative script adhering to a particular synthesizer's API. This allows the declarative script to be used as a "base" for creating imperative scripts, such that an experienced scenario developer can modify the generated imperative script as needed. Imperative scripts can be regenerated from the same declarative script to reflect updates in library usage so long as the interpreter is updated accordingly, inheriting the perennial nature of declarative scripts.

It is important to note that a declarative syntax, which may be more "rigid" in structure, does not preclude the use of non-determinism or randomness. One notable example of this is the discrete-time Markov chains used by Yannikos et al. to express scenarios in a probabilistic manner, with each state of the Markov chain representing a particular action taken by the synthesizer (such as sending an email or deleting a file) [@yannikosDataCorporaDigital2014]. These chains are evaluated at runtime to generate multiple unique datasets from a single description.

The challenges of defining a suitable declarative syntax for a particular synthesizer are not unlike the challenges faced in general programming language design. There are several key factors to the success of imperative programming languages that extend to declarative syntaxes, some of which are derived from Finkel and described as follows [@finkel1996advanced]:

- The language should be **simple**, using as few basic concepts as possible. This makes code easier to read and write, an important aspect for users with limited programming experience. 
- The language should be **modular**, such that the role and interfaces of individual program units are clear.
- The language should be **predictable**, such that users can apply their existing knowledge of a synthesizer to quickly implement or add new features to a scenario. 
- The language should **abstract** as much as possible away from the user, such that the minimum information needed to fulfill artifact generation is exposed to the user. 

The most important factor, however, is an awareness of the *purpose* of the declarative syntax. The purpose of a synthesizer is to make it easier to generate forensic artifacts and datasets. The declarative language should reflect this, focusing on making actions and artifacts as easy to declare and customize as possible. 

In designing the AKF declarative syntax, the declarative syntaxes of prior synthesizers and unrelated technologies were evaluated. An analysis of some of these syntaxes, with examples, is described briefly in **39.B - Code samples#Historical declarative syntaxes**. However, two syntaxes in particular contributed the most to the AKF declarative syntax: those of ForTrace and Ansible, described in the following section.

## 6.3.1 - Existing declarative syntaxes

ForTrace [@gobelForTraceHolisticForensic2022] uses YAML to express scenarios in a declarative manner. ForTrace significantly influenced both the AKF declarative syntax itself and the implementation of the declarative interpreter, primarily because it was the sole open-source synthesizer with both imperative and declarative support. **!lst:6.3.1a** demonstrates a simple example of a ForTrace declarative scenario:

```yaml
!lst:6.3.1a|Declarative ForTrace scenario with web browsing [@gobelForTraceHolisticForensic2022]
name: haystack-example
description: A example action suite to generate a haystack (traffic)
author: MPSE Group
seed: 1234
collections:
  c-http-0:
	type: http
	urls: ./generator/friendly_urls.txt
settings:
  host_nfs_path:
  guest_nfs_path:
applications:
hay:
  h-http-0:
	application: http
	amount: 3
	collection: c-http-0
needles:
  n-http-0:
	application: http
	file: [https://dasec.h-da.de/](https://dasec.h-da.de/)
	amount: 1
dumps:
  d-dump-0:
	dump-type: mem
	dump-path: /home/fortrace/gendump.file
```

At a high level, ForTrace scenarios contain five distinct elements:

- Metadata about the scenario, such as the scenario's name, description, and author.
- "Collections" of data that can be reused throughout the scenario in supported application types, such as a newline-separated list of URLs.
- Configuration options that may be applied to all actions of a particular action type or the entire scenario.
- The actual artifacts to create as part of the `hay` and `needles` sections, where `hay` includes artifacts that should be considered background noise, and `needles` includes artifacts that should be considered significant. Each artifact contains a unique ID, an application type (the `application` key), and arguments specific to the application responsible for generating the artifact, such as the URLs for web browsing. 
- Any core outputs that should be created as part of the scenario.

This file is passed into a "generator," which parses the contents of the YAML file to prepare various internal data structures, initialize the virtual machine, and then execute the actions specified in the `hay` and `needles` sections in a random order according to the `seed` key. Depending on the value of the `application` key, the data for that action is passed to an application-specific handler that interacts with the running virtual machine using existing ForTrace libraries. Once all the actions have been executed, the generator creates any requested outputs (such as volatile memory dumps) and shuts down the virtual machine.

This analysis provided insight into the design decisions and functionality required to execute actions from the high-level descriptions of a scenario. In particular, it demonstrates the need for actions or artifacts to be defined consistently and flexibly so that program state and other data can be passed to application-specific libraries as needed. It also demonstrates the need for various levels of configuration, including scenario-wide configuration, application-specific configuration, and action/artifact-specific configuration. ForTrace implements these features in a somewhat awkward manner; in fact, nearly all declarative language support is contained in a single file, with a hardcoded "router" handling each unique `application` type. This makes it difficult to add support for new applications without significant effort, particularly because the generator must be aware of every possible action/artifact type ahead of time.

With these priorities and issues from ForTrace identified, are there ideas from other technologies that can be used to address them? That is, are there other technologies designed to execute a large set of complex actions using a simple but flexible and configurable syntax, and how does it work? Ansible, the second major inspiration for the AKF declarative syntax, precisely fills this need.

Ansible [@AnsibleAnsible2025] is an open-source automation framework often used to remotely configure Windows and Linux machines at scale, allowing organizations to manage many machines at once without installing orchestration software on these machines in advance. To achieve this, users write *playbooks*, which are simple YAML files that contain one or more *plays*. Each play is simply a set of *tasks* run on multiple machines simultaneously, and each task depends on a *module* designed to achieve a single, specific outcome.

An example of an Ansible playbook can be seen in **!lst:6.3.1b**, which contains a single play to configure an Apache webserver.

```yaml
!lst:6.3.1b|Minimal Ansible playbook for updating an Apache server [@ansibleprojectcontributorsAnsiblePlaybooks]

- name: Update web servers
  hosts: webservers
  remote_user: root
  tasks:
  - name: Ensure apache is at the latest version
    ansible.builtin.yum:
      name: httpd
      state: latest
  - name: Write the apache config file
    ansible.builtin.template:
      src: /srv/httpd.j2
      dest: /etc/httpd.conf
```

This play uses the `yum` package manager to install Apache before copying a local configuration file to the remote host. `ansible.builtin.yum` and `ansible.builtin.template` are both modules, which accept parameters passed as a YAML dictionary. These modules are part of the `ansible.builtin` collection included with all default Ansible installations.

Although Ansible contains many features that contribute to its flexibility, the two important concepts relevant to AKF are *roles* and *modules*. Roles are a collection of Ansible resources that typically achieve some "larger" reproducible goal, typically by running multiple tasks and leveraging variables, configuration options, and files included as part of the role. Roles can include *modules*, which are standalone imperative scripts (typically in Python) that accept arguments, execute code based on those arguments, and return data using well-structured interfaces. As shown above, these modules can be called and executed from playbooks; they can also be executed independently on the command line. 

These concepts are highly relevant to synthesizers, which must support application-specific actions and group these actions together in a flexible, well-defined manner. As described in **39.4 - Action automation#4.2.2 - The AKF agent**, each RPyC subservice of an AKF agent exposes a group of application-specific automation methods. The functionality of each of these groups must be re-exposed in a declarative manner, which can be achieved by adapting the concept of Ansible roles and modules to AKF. 

Together, the ForTrace and Ansible syntaxes provide three concepts that are reflected in the AKF syntax, described further in the following section:

- The overall structure and contents of the declarative YAML file.
- An action syntax that allows us to declare individual actions, referring to those actions by name, and pass arguments directly to that action for *translation* or *execution*.
- A modular architecture that allows us to define the supported arguments of each action and expose them to the declarative interpreter while also being decoupled from the standard imperative library as much as possible.

## 6.3.2 - The AKF declarative syntax

The AKF declarative syntax is very similar to the Ansible playbook syntax. Declarative scripts are comprised of metadata, global configuration, libraries to import, and individual tasks to execute as part of the scenario. Each task refers to a single *module* by name using a qualified Python import path, accepting a dictionary of arguments in addition to global configuration overrides. 

Like Ansible, individual modules are implemented as well-defined subclasses of `AKFModule`, an abstract base class that serves as the root of all AKF declarative modules. Each module must define Pydantic models that specify the arguments accepted by the module. The arguments declared in the YAML file are then passed to these module-specific Pydantic models for validation, after which the `AKFModule` must perform one of two tasks:

- **Execution**: When instructed to perform actions directly from the declarative script, the `AKFModule` should import AKF core libraries and agent APIs to perform the required actions.
- **Translation**: When instructed to translate the declarative script, the `AKFModule` should generate the equivalent code that *would* perform related actions if executed through a standard Python interpreter with the necessary libraries installed.

The ability of AKF to both execute and translate declarative scripts provides significant flexibility to scenario developers. To the best of our knowledge, prior synthesizers have only supported direct execution from declarative scripts, which limits the opportunities to use declarative scripts as a "starting point" for writing more complex imperative scripts. (In fact, the code in **!lst:6.2a** was primarily derived from the auto-generated code using **!lst:6.3.2a**.)

An example of a minimal AKF scenario, carrying out the same actions as the SFX, ForTrace, and imperative AKF script above, can be seen in **!lst:6.3.2a**:

```yaml
!lst:6.3.2a|Example of a declarative AKF scenario
name: Minimal scenario
description: Sample included with thesis
author: Lloyd Gonzales
seed: "0"
libraries:
  - akflib.modules
  - akf_windows.modules
actions:
  # Create a new CASE bundle and start a virtual machine.
  - name: Instantiate a hypervisor object tied to a specific virtual machine
    module: vbox_start
    args:
      machine_name: "akf-windows-1"
  - name: Start the virtual machine
    module: vbox_start_machine

  # Visit a website using Microsoft Edge. A temporary instance of the Chromium
  # subservice API is created for the lifetime of this module.
  - name: Visit a single website
    module: chromium_visit_urls
    args:
      browser: "msedge"
      urls: 
       - "[https://www.google.com"](https://www.google.com")

  # Stop the virtual machine and export the virtual machine to a disk image.
  - name: Stop the virtual machine
    module: vbox_stop_machine
    args:
      force: false
  - name: Export the virtual machine to a disk image
    module: vbox_create_disk_image
    args:
      output_path: "C:/Users/user/Desktop/akf-windows_1.raw"
      image_format: "raw"
```

The `akf-translate` interpreter, which is added to PATH when installing `akflib`, handles both execution and translation from a declarative script. It uses the same underlying library calls as the imperative code in **!lst:6.2a**. To use this declarative script with `akf-translate`, one of the following commands in **!lst:6.3.2b** can be used:

```sh
!lst:6.3.2b|Sample usage of the akf-translate utility

# Translate this scenario to a valid Python script and save it to disk

akf-translate scenarios/thesis.yaml --translate --output-file scenarios/thesis.py

# Execute this scenario directly

akf-translate scenarios/thesis.yaml --execute
```

AKF declarative scripts, which are simply large YAML dictionaries, contain three distinct elements. The first is a set of high-level metadata keys associated with the scenario. The second is a set of scenario-wide configurations; in particular, it lists the libraries containing the necessary modules to execute or translate this imperative script. Finally, scripts list a sequence of actions, typically a `module` specified by name and a dictionary of `args`.

The execution flow of `akf-translate` itself is straightforward. Given a path to a YAML script, the interpreter will load the necessary libraries and configuration keys defined in the file and instantiate resources accordingly. This may include setting aliases for module names, verifying that all required modules are available, setting the `random` seed, and so on. Then, the interpreter runs each module under the `actions` key with the provided arguments and configuration in order, continuing until all actions have been processed.

Modules can read and modify a global state dictionary that allows otherwise independent modules to cooperate. For example, suppose that a module creates a new context manager in a `with` block, causing the indentation level of the translated code to increase. Successive modules can read the state dictionary to retrieve context variables and correctly indent generated code. Additionally, this allows for "outputs," such as CASE bundles, to be passed and gradually constructed across modules. This design allows for context-aware code generation and action execution.

Modules are located and executed using Python's dynamic import system. These modules can be located in any library so long as they can be found through Python's import system. For example, both `akflib` and the AKF Windows agent contain their own declarative module libraries, leveraging functionality specific to each code repository. All modules in the script are located and "cached" at the start of script execution, which allows for verification and runtime efficiency.

This design allows declarative modules to be written independently of the libraries they depend on, reducing the "impact" of supporting declarative features on the core imperative libraries. In fact, this independence allows for the AKF module system to be used in general-purpose scripting, similar to Ansible; it is not tightly bound to the creation of forensic scenarios and artifacts. That said, the existing declarative modules of AKF expose nearly all existing functionality provided by `akflib`, including PDF report generation, virtual machine interaction, and more.

The list of declarative modules available through `akflib` and the Windows agent is described in **!tbl:akf-declarative-modules** below. Note that these modules are referred to by their alias, not their fully-qualified module paths.

*!tbl:akf-declarative-modules|Available AKF declarative modules|0.3,0.7*

| Name                     | Description                                                                                                       |
| ------------------------ | ----------------------------------------------------------------------------------------------------------------- |
| `create_akf_bundle`      | Creates a new CASE bundle for use throughout the declarative scenario.                                            |
| `write_akf_bundle`       | Write the contents of the currently active CASE bundle to disk as a JSON-LD file.                                 |
| `render_akf_bundle`      | Pass the currently active CASE bundle through a set of provided renderers and construct a valid PDF using Pandoc. |
| `vbox_start`             | Create a new VirtualBox instance bound to a specific virtual machine by name.                                     |
| `vbox_start_machine`     | Power on the currently active virtual machine.                                                                    |
| `vbox_stop_machine`      | Power off the currently active virtual machine.                                                                   |
| `vbox_create_disk_image` | Export a disk image of the current virtual machine.                                                               |
| `artifact_service_start` | Remotely start and connect to the subservice responsible for collecting Windows artifacts.                        |
| `artifact_service_stop`  | Disconnect from the subservice responsible for collecting Windows artifacts.                                      |
| `prefetch`               | Analyze all prefetch files on disk and construct their corresponding CASE objects.                                |
| `chromium_service_start` | Remotely start and connect to the subservice responsible for interacting with Chromium browsers.                  |
| `chromium_service_stop`  | Disconnect from the subservice responsible for interacting with Chromium browsers.                                |
| `chromium_visit_urls`    | Visit one or more URLs using a specified web browser.                                                             |
| `chromium_history`       | Collect browsing history from a specified web browser and construct their corresponding CASE objects.             |

Although these declarative modules (and the imperative library) provide users with significant flexibility in *using* AKF, the challenge of building artifacts and scenarios to generate through these execution options remains. The remainder of this chapter addresses this challenge.

# 6.4 - Using generative AI for individual artifacts

## 6.4.1 - Challenges of standalone artifacts

Users of synthesizers must still perform a significant amount of work when generating individual artifacts. More precisely – while AKF and other synthesizers can streamline the process of placing and generating artifacts, users must still provide some of the artifacts themselves. For example:

- If a user wants to place 100 photos on the drive to simulate real usage, the user needs to create and provide 100 realistic images.
- If a user wants to simulate an email or other online conversation, the user needs to provide the entirety of the conversation to simulate. Such conversations would need to be consistent with the "theme" of a scenario.
- If a user wants to generate "proprietary" documents to emulate some form of corporate sabotage, the user would need to create and provide a variety of Microsoft Office, PDF, or other files in these formats.

This is particularly relevant when adding background noise intended to emulate benign activity. Such datasets are valuable training material in forensic courses that encapsulate a long period of study, allowing students to apply many different techniques learned throughout a curriculum to analyze a realistic scenario. 

Indeed, the artifacts in the bulleted list above are all elements of the NIST CFReDS Data Leakage Case [@nationalinstituteofstandardsandtechnologyCFReDSDataLeakage], which contains web browsing history, images, documents, and other media relevant to the theft of proprietary information related to a product in development. However, many of these documents are not specific to the scenario; in particular, the "technical documents" present in the scenario are actually files from the general-purpose Govdocs corpora [@garfinkelBringingScienceDigital2009], with a cover page denoting their intended role in the scenario. For example, one PowerPoint file is portrayed as a presentation of the detailed design of the product, but its actual contents are that of a Yale University presentation on brain physiology.

The use of "irrelevant" files to portray "relevant" files can be acceptable when the actual content of the artifact is not important in analyzing the scenario as a whole. This practice significantly reduces the time spent creating individual artifacts, both those directly related to the focus of the scenario and those that are intended to be background noise. However, this technique still detracts from the realism of the scenario and may not be sufficient to cover all use cases. One notable example in the Data Leakage Case is the lack of emails on the device; the sole email conversation is strictly related to the scenario. A real-world suspect would almost certainly have many other email conversations consistent with their background, such as employer-related emails or marketing emails from websites the user has visited.

Recent advancements in AI models have made it significantly easier to generate text, images, and other media from high-level descriptions. Generative AI can quickly populate forensic datasets with realistic conversations and images consistent with an arbitrary scenario, such as the scenario presented by the Data Leakage Case. For example, a large language model (LLM) could be prompted to generate text describing a fictional piece of machinery in both technical and conversational styles. The text could then be combined with images generated by another model to create documents and emails. This provides a pipeline through which thematically-consistent artifacts can be generated and placed onto a forensic dataset. The same process can be performed for artifacts that are simply background noise; generating hundreds of "irrelevant" emails would be impractical for a human but trivial for an LLM.

This idea can be extended further by training models on specific datasets; for example, if an instructor wished to create a fictional scenario in which a user frequently interacts with users of a particular online community, a large language model could be trained on (or otherwise provided) existing conversations to provide a degree of realism to the scenario. However, this faces the challenges of ownership, privacy, and legality behind works derived from publicly available information that was not published with the expectation of its usage in an AI model – issues very similar to those described in **39.1 - Introduction#1.3.3 - Motivation for synthetic datasets**.

It is important to note that the inclusion of generative AI into synthesizers does not necessarily require deep integration with the framework itself. Many existing synthesizers could be extended to use documents, images, or other data sourced from generative AI instead of user-defined files without the need to change the synthesizer's architecture. Directly integrating AI-driven actions into synthesizers is addressed further in **39.8 - Future work#8.1 - Open-ended automation with AI**.

## 6.4.2 - Building sample standalone artifacts

This section explores the use of DeepSeek-R1 [@deepseek-aiDeepSeekR1IncentivizingReasoning2025] and SDXL 1.0 [@podellSDXLImprovingLatent2023] in constructing various scenario artifacts. In particular, we focus on building artifacts similar in quality and theme to those found in the NIST CFReDS Data Leakage Case. Note that the approach detailed below can be generalized to other models, not just DeepSeek-R1 and SDXL.

As described previously, the Data Leakage Case contained files presented as technical documentation but had content that was irrelevant to the scenario itself. Our goal is to create comparable documents containing text and images consistent with a theme provided to the models responsible for generating this content. We propose a three-step process for generating multimedia artifacts:

- **Body text**: A prompt is provided to DeepSeek-R1 to create Markdown document(s) consistent with the scenario. Using a special syntax, the model is instructed to insert suitable image generation prompts throughout the document. The syntax allows the model to configure image parameters, such as the size of the image.
- **Image generation**: The prompts are extracted from the Markdown document and passed to SDXL 1.0. The resulting images are saved to a temporary folder, and the special syntax is replaced with the standard Markdown syntax for embedding images at a known path.
- **Rendering**: The resulting Markdown document is passed to a tool such as Pandoc to generate the final artifact, similar to the approach in **39.5 - Output and validation#5.3 - Human readable reporting**. Pandoc supports various formats, including PDFs, Microsoft Word, and Microsoft PowerPoint. Other tools could be used to convert Markdown to other formats.

The document snippet shown in **!fig:artifact-document-example** was produced by following the steps above on locally-hosted instances of DeepSeek-R1 and SDXL 1.0, using Ollama [@OllamaOllama2025] and ComfyUI [@comfyanonymousComfyanonymousComfyUI2025] as interfaces. An instance of `deepseek-r1:32b` was directed to create a document describing the technical details of a fictional semiconductor manufacturing product under various constraints.

![Sample of an AI-generated PDF artifact to be used in a themed scenario](pdf-example.png){#fig:artifact-document-example}

Of course, the process can be simplified to exclusively generating images. For example, the images in **!fig:artifact-image-example** were generated by instructing DeepSeek-R1 to provide a list of image generation prompts related to semiconductor manufacturing. These were then passed to SDXL 1.0.

![Collage of AI-generated images for use in a themed scenario](collage.png){#fig:artifact-image-example}

Finally, we noted that the Data Leakage case lacks background email conversations. Email conversations as part of a scenario can be divided into three categories:

- Emails that are relevant to the focus or topic of the scenario, such as corporate espionage
- Emails that are irrelevant to the scenario but are consistent with the persona included as part of the scenario, such as benign emails to coworkers
- Emails wholly irrelevant to the scenario, such as marketing or spam emails

To generate these emails, DeepSeek-R1 was prompted to generate email conversations according to the three email categories above in a well-structured format that could easily be converted to a valid `.eml` file using the Python standard library. DeepSeek-R1 was provided details such as the email address to use when sending/receiving emails and the corporate email domain to use in relevant emails. An excerpt of a sample email conversation can be seen in **!fig:artifact-email-example**.

![Sample of an AI-generated email conversation](email_snippet.png){#fig:artifact-email-example}

Consistent with the architectural diagram depicted in **!fig:architecture-full-a**, the use of generative AI in AKF is almost entirely decoupled from other parts of the framework and can be applied outside of synthesizer contexts. Although the three-step process to generate the artifacts above was carried out manually, the same process could be implemented in standalone modules in the AKF library. There are various libraries and solutions for interfacing with Ollama and ComfyUI through Python; these libraries could be leveraged in such an implementation.

# 6.5 - Using LLMs for high-level scenarios

The prior section addressed the challenge of generating individual artifacts for a dataset. However, there remains the challenge of using these artifacts to generate a cohesive scenario. That is, with an overall scenario theme or goal in mind, what specific synthesizer actions should be taken to construct a dataset that adheres to that theme? Even with the automation routines provided by AKF and individual artifacts prepared (whether created manually or using generative AI), it still takes time to define a scenario, especially one that involves many different types of actions that cannot easily be simplified into a small number of synthesizer actions. This is true even with a simplified declarative syntax. 

One solution is to provide the details of AKF's declarative syntax to an LLM and instruct it to generate valid declarative scripts. In particular, there are two features of the declarative syntax that lends itself well to this process:

- The role of each module is well-defined and corresponds one-to-one to a specific action that a human might take, such as visiting a set of websites. 
- The expected inputs for each module are well-defined, as they are documented by the required Pydantic argument model for each module.

To explore this, an instance of `deepseek-r1:32b` was provided with the following information. (The actual prompts and outputs used to derive the content in this section are included with the scenario examples for the AKF Windows agent.)

- An overview of the purpose of the declarative syntax.
- The overall structure of the syntax, such as the required top-level keys and the structure of individual actions under the `actions` key.
- A Markdown-formatted table in which each row contains the name, description, and arguments of available modules. Except for the arguments column, the table was equivalent to **!tbl:akf-declarative-modules**.
- A prompt delimited by `<scenario_prompt>` tags that should be used to build the overall scenario.

We observed that this information was generally sufficient for DeepSeek-R1 to "understand" the AKF syntax, allowing it to generate simple declarative scripts. Although the prompt containing this information was manually written, much of the prompt could be used as a template and substituted with the contents of automatically generated documentation. For example, the table of available modules could be generated through existing code metadata, such as the contents of docstrings and Pydantic argument models.

For example, we directed DeepSeek-R1 to create a simple scenario where a user visited news-related websites with Microsoft Edge and entertainment-related websites with Google Chrome, ensuring that the machine went through multiple power cycles throughout the scenario. Although DeepSeek-R1 was not provided with examples specific to invoking the `chromium_visit_urls` module, it correctly constructed an argument dictionary containing several thematically consistent websites for both browsers. Additionally, it correctly used the `vbox_start_machine` and `vbox_stop_machine` actions to perform multiple power cycles.

However, DeepSeek-R1 struggled to understand the interactions between related modules without additional guidance. For example, it did not infer that the use of the `vbox_stop_machine` and `vbox_start_machine` actions required that a hypervisor instance had been previously created using `vbox_start`. Even when the module descriptions were modified to state these requirements explicitly, it continued to invoke `vbox_start_machine` without a preceding `vbox_start` action. In contrast, DeepSeek-R1 appeared to understand that `chromium_service_start` should be called before calling any other Chromium-related modules, even when this requirement was not stated. Indeed, the Chromium-related modules support, but do not require, the use of `chromium_service_start` to explicitly connect to the Chromium RPyC subservice.

Additionally, DeepSeek-R1 would sometimes invoke modules incorrectly, such as providing arguments that did not exist or invoking them with the incorrect name. One notable example was that it incorrectly used `vbox_start_machine` to both start and stop the virtual machine, adding an `action: "stop"` argument when stopping the virtual machine. In some cases, DeepSeek-R1 deviated significantly from the declarative syntax and used keys that did not exist, such as specifying module arguments outside of the `args` dictionary.

Despite these issues related to correctness, it was clear that DeepSeek could convert abstract prompts into a sequence of actions based on the available modules provided. Much like using declarative scripts as a starting point for imperative scripts, these outputs can likely be used as a starting point for more complex declarative scripts. Furthermore, these issues could be alleviated by improving the details of the prompt, such as providing specific examples of module usage and the overall results they cause. In particular, DeepSeek-R1 is a reasoning model, which means that it "thinks out loud" before responding; the contents of the model's thoughts could be used to identify and eliminate vagueness or uncertainty in the provided prompt. Writing precise, detailed documentation with this LLM-oriented pipeline in mind could further improve the quality of generated outputs.

As more declarative modules and functionality are added to AKF, it is clear that LLMs can significantly accelerate converting abstract scenario goals into a script that will carry out the necessary actions to fulfill these goals. This is especially true given continuing advancements in generative AI, which are likely to improve the performance of these models in adhering to specific instructions.


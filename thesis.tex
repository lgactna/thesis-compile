\documentclass[letterpaper,12pt]{report}

% == Packages ==========================================================
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{lipsum}
\usepackage{multicol}
\usepackage{accents}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{tabularx}
\usepackage{euscript}
\usepackage{color}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage{epsfig}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage[T1]{fontenc} 
\usepackage{hyperref}
\usepackage[left = 1.5in, right = 1in, top = 1in, bottom = 1.25in, head = 0.5in]{geometry}
\usepackage{textcomp}
\usepackage{cite}
\usepackage{pdfpages}
\usepackage[linesnumbered,lined,boxruled,commentsnumbered]{algorithm2e}
\usepackage[acronym]{glossaries}
\graphicspath{{Figures/}}
\usepackage{attachfile}

% Below are ones I addded

% Warn on unused citations (and labels, too - this makes it very noisy). By default,
% unused citations don't get added to the bibliography anyways.
% 
% There's also a \nocite{*} command at the end of the document that creates the funny
% looking labels in the bibliography - if it's surrounded by question marks, then it wasn't
% used anywhere in the diagram (and should be removed from the .bib file). Comment this line
% out, as well as the \nocite command, when compiling for readers.
% 
% See https://tex.stackexchange.com/questions/43276/unused-bibliography-entries-how-to-check-which-entries-were-not-used
\usepackage{refcheck}

% Table support.
\usepackage{booktabs}

% Pandoc uses tightlist for bullet lists, you can just do nothing
% See https://tex.stackexchange.com/questions/257418/error-tightlist-converting-md-file-into-pdf-using-pandoc
\def\tightlist{}

% Listings and inline code

% Pandoc generates a bunch of these for inline code, just do nothing with them
\newcommand{\passthrough}[1]{#1}

\usepackage{listings}
\usepackage{inconsolata} % Use the Inconsolata font
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% \lstdefinestyle{mystyle}{
%     backgroundcolor=\color{backcolour},   
%     commentstyle=\color{codegreen},
%     keywordstyle=\color{magenta},
%     numberstyle=\tiny\color{codegray},
%     stringstyle=\color{codepurple},
%     basicstyle=\ttfamily\footnotesize\linespread{0.8},
%     breakatwhitespace=false,         
%     breaklines=true,                 
%     captionpos=b,                    
%     keepspaces=true,                 
%     numbers=left,                    
%     numbersep=5pt,                  
%     showspaces=false,                
%     showstringspaces=false,
%     showtabs=false,                  
%     tabsize=2,
% }
% \lstset{
%     style=mystyle,
% }

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\ttfamily\tiny\color{codegray},
    stringstyle=\color{codepurple},
    % Adjust line spacing with \setstretch
    % Font set to Inconsolata
    basicstyle=\ttfamily\footnotesize\setstretch{1}\fontfamily{zi4}\selectfont, 
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    aboveskip=5pt, % Adjust spacing above listings
    belowskip=0pt, % Adjust spacing below listings
}
\lstset{
    style=mystyle,
}

% == End packages

\renewcommand*\ttdefault{pcr}


\makeglossaries
\loadglsentries{lloyd_gonzales_thesis_glossary.tex}

\bibliographystyle{IEEEtran}

% == Customizations ====================================================
\allowdisplaybreaks

% == Page Style ========================================================
\pagestyle{fancyplain}
\renewcommand{\headrulewidth}{0.0pt}				% gets rid of lines on header
\fancyhead{}								      	% clears all header and footer fields
\fancyfoot{}
\fancyhead[R]{\thepage}						      	% inserts page number in top right

\begin{document}

% == Use fake numbering until abstract =================================
\pagenumbering{Alph}

% == Title Page +=======================================================
\newpage
\thispagestyle{empty}
\singlespacing

\begin{center}

\null

\vspace{1.5in}

University of Nevada, Reno \\

\vspace{1.5in}

\textbf{AKF: A modern synthesis framework for building digital forensic datasets}

\vspace{1.5in}

A thesis submitted in partial fulfillment of the\\
requirements for the degree of Master of Science in\\
Computer Science and Engineering\\

\vspace{1in}

by

\vspace{0.25in}
Lloyd Gonzales
\vspace{0.5in}

Nancy LaTourrette, Advisor \\
May 2025\\

\end{center}

% == Committee Approval =================================================

% == Set page numbering style ============================================
\pagenumbering{roman}
\setcounter{page}{0}

% == Abstract ===========================================================
\newpage
\onehalfspace

\begin{abstract}

\thispagestyle{fancyplain}
\doublespacing
As our world becomes increasingly dependent on technology, the
advancement of digital forensics has become a key focus in the fight
against cybercrime. The field depends greatly on the availability of
disk images, network captures, and other forensic datasets for
education, tool validation, and research. However, real-world datasets
often contain sensitive information that may be difficult to remove,
making them difficult to distribute publicly. As a result, researchers
and educators can encounter gaps in available datasets, often leading to
the manual development of new, suitable datasets. While viable, this
approach is time-consuming and rarely produces datasets that accurately
reflect real-world scenarios suitable for comprehensive training and
education. In turn, there is ongoing research into forensic
synthesizers, which automate the process of creating unique synthetic
datasets that can be publicly distributed without legal and logistical
concerns.

This thesis introduces the \emph{automated kinetic framework}, or AKF, a
modular synthesizer for creating and interacting with virtualized
environments to simulate user activity. AKF significantly improves upon
the architectural designs of prior synthesizers while maintaining
feature parity and usability. Additionally, AKF leverages the CASE
standard to provide human- and machine-readable reporting, exposing
low-level details in a searchable format. Finally, AKF provides options
for leveraging generative AI to develop high-level scenarios as well as
individual artifacts. These contributions are intended to not only
improve the speed at which synthetic datasets can be created, but also
ensure the long-term usefulness of AKF-generated datasets and the
framework as a whole.
\end{abstract}

% == Set page number ====================================================
\setcounter{page}{2}
\doublespacing


% == Dedication ==========================================================

% \begin{center}

\section*{Dedication}

To those in the osu! tournament community, without whom I would have
never embarked on this journey;

To my numerous teachers and professors, especially Keith Lightfoot,
Rodney Rogers, Marc Miller, and Gabbi Bachand, who I have limitless and
appreciation and admiration for;

To those part of the United States Cyber Team and the broader CTF
community, for igniting my interest in digital forensics and supporting
me even when I flailed like a fish out of water;

And, of course, to my friends, family, and bed, who provided motivation
when there was none.

% \end{center}

% == Acknowledgments ====================================================
\newpage

\section*{Acknowledgments}

I want to express my immense gratitude to Nancy Latourrette for her
support, guidance, and mentorship throughout the development of this
thesis. This thesis would be nowhere without her ideas and experience,
and I am truly grateful and honored to have been able to work with her
throughout this experience.

I would also like to thank Bill Doherty for his review of a prior paper,
from which some of this content is derived.


% == Table of contents ===================================================
\newpage
\renewcommand*\contentsname{Table of Contents}
\tableofcontents

\newpage
\listoftables

\newpage
\listoffigures

% == Set new page number style ===========================================
\newpage
\pagenumbering{arabic}
\setcounter{page}{1}
\linespread{2}

% == Begin thesis content
\chapter{Introduction}\label{chapter-one}

\section{History of digital
forensics}\label{history-of-digital-forensics}

Digital forensics is a relatively new field that rose out of the growing
need to address computer crimes. Through the explosion of computing that
has occurred over the last 30 years, computers have become critical in
virtually every modern industry. In turn, they have become the target of
many attacks. Much like conventional crimes, these cyberattacks leave
behind traces of digital evidence that can be analyzed to determine the
methodologies of the attackers, the scope and extent of the damage, and
more. Today, digital forensics is also applicable to a variety of
non-criminal contexts, including research and corporate investigations,
further highlighting the need for quality educational material and
hands-on training for new forensic analysts.

The origins of digital forensics can be traced to the 1980s, during
which computers began to be adopted by the general public. Prior to
this, computers were largely restricted to industry, academia, and
governments with dedicated infrastructure and staff
\cite{pollittHistoryDigitalForensics2010}. However, with the
introduction of PCs more accessible to the typical consumer, such as the
Commodore 64 and the IBM PC, computer usage within the public grew. And
in turn, so did crimes committed with computers; people discovered that
they could hack telephone networks to illegally obtain software and
``free'' telephone services \cite{jonesInsightDigitalForensics2022}.
In 1983, Canada would be the first government to amend its criminal code
to cover cybercrime; the United States, the United Kingdom, and
Australia would follow suit in the following decade.

During this time, investigations were relatively ad-hoc and very simple.
Teams often built their own software to analyze devices, though a few
hobbyists and software vendors began to develop dedicated forensic
tools. However, computers had yet to truly enter the mainstream; the
vast majority of individual computer owners were (relatively wealthy)
computer hobbyists rather than the general public. In 1984, the U.S.
Census Bureau determined that only about 8.2\% of U.S. households owned
a computer \cite{robertkominskiComputerUseUnited1988}, nearly
doubling to 15\% in 1989 \cite{robertkominskiComputerUseUnited1991}.

As technology continued to advance throughout the 1990s, digital
forensics -- and computing as a whole -- began to quickly grow in scope
and importance. The rise of mobile devices and the internet drastically
changed the role of computing in the eyes of the public. With it came
the rise of cybercrime and the importance of computer investigations.
Early investigations were often performed by investigators who happened
to be experienced with computers, rather than those with formal digital
forensics training \cite{hargreavesDigitalForensicsEducation2017}.

By the turn of the century, digital forensics had grown beyond law
enforcement and niche cases to a major focus of research and education.
The first use of the phrase ``computer forensics'' in ACM literature
appeared in 1999 \cite{cooperStandardsDigitalForensics2010}, with
the first Digital Forensic Research Workshop (DFRWS) held in 2001 to
identify priorities in the growing field of digital forensics
\cite{palmerRoadMapDigital2001}. A particularly notable case was
that of the September 11, 2001 attacks, in which computers were found to
contain meaningful evidence related to the organization and planning of
the attack. Intelligence communities and law enforcement agencies
throughout the world began establishing digital forensics teams,
shifting the agenda of digital forensics from individuals and small
teams to governments and professional organizations.

It was during this time that tools such as EnCase and Forensic Toolkit
(better known as FTK) evolved to become dedicated products that remain a
mainstay of the digital forensics field today
\cite{pollittHistoryDigitalForensics2010}. At the same time,
anti-forensics began to grow in popularity; numerous tools and resources
were developed with the express goal to exploit and hinder the digital
forensics process, generally with the stated motivation of guarding
users' privacy and protecting users from punishment for undesirable
computer activity
\cite{geigerEvaluatingCommercialCounterForensic2005,harrisArrivingAntiforensicsConsensus2006}.

By this time, it had become abundantly clear that there needed to be
dedicated training to develop specialists in digital forensics.
Undergraduate and graduate programs dedicated to the study of digital
forensics were gradually developed
\cite{andersonComparativeStudyTeaching2006}, along with numerous
efforts to standardize and improve digital forensics curricula
\cite{cooperStandardsDigitalForensics2010,nanceDigitalForensicsDefining2009,nanceDigitalForensicsDefining2010}.
This became particularly important due to the growing importance of
digital forensics from a legal perspective; analysts must follow a
strict procedure that ensures the admissibility of digital evidence into
court \cite{conklinComputerForensics2022}. Simultaneously, analysts
must have the experience needed to provide an unbiased, accurate opinion
of digital evidence in court as an expert witness.

Digital forensics continues to be an important focus in industry -- and
in turn, education and research. Broadly, the Bureau of Labor Statistics
projects that information security employment will grow 32\% over the
next decade from 2022 to 2032, adding over 50,000 new jobs
\cite{bureauoflaborstatisticsu.s.departmentoflaborInformationSecurityAnalysts2023}.
The diversity and depth of digital forensics continues to grow with the
broad variety of devices and software involved in modern computing.

With the growing importance and complexity of digital forensics, there
is a clear need for high-quality, realistic data for researchers and
instructors alike. Yet, there continues to be significant gaps in both
the quantity and variety of material suitable for digital forensic
training. The primary barrier to availability has been the privacy and
legal concerns associated with releasing real-world data
\cite{garfinkelForensicCorporaChallenge2007}. From a research
perspective, the result is that many researchers develop their own
datasets, often with a very narrow scope and a very low degree of
reproducibility
\cite{garfinkelBringingScienceDigital2009,grajedaAvailabilityDatasetsDigital2017}.
From an education perspective, the result is that most training material
is either manually created by instructors or reused from existing
sources. That is, researchers and instructors alike often create their
own datasets because publicly available corpora is insufficient;
however, this is a time-consuming process that responds slowly to
changes in technology and software.

Various efforts have been made to automate or streamline the process of
creating new forensic datasets from high-level descriptions and
predefined forensic artifacts. These forensic synthesis frameworks (also
referred to as ``synthesizers'' throughout this paper) include a variety
of features that are geared towards research and education. This
includes the rapid creation of datasets for a large classroom, the
generation of metadata useful in tool validation, and the ability to
export a variety of forensic artifacts from the synthesizer. In
particular, these frameworks enable instructors to create images that
allow students to learn about a specific forensic technique while
emulating some of the real-world challenges that forensic analysts face
in industry.

However, there is still much work to be done towards increasing the
accessibility and flexibility of these frameworks. Before exploring
synthesizers in greater detail, it is necessary to first understand the
purpose and characteristics of forensic datasets in a variety of
contexts. Doing so will not only outline \emph{why} the development of
synthesizers is necessary, but also \emph{what} features these
synthesizers must provide. Once we have established this foundation, we
can begin exploring \emph{how} a synthesizer should be architected --
the focus of the reminder of this thesis.

\section{Purpose of forensic
datasets}\label{purpose-of-forensic-datasets}

\subsection{In industry}\label{in-industry}

Before considering the use of forensic datasets in research and
education, we begin by exploring how these datasets are acquired and
used in the ``real world'' -- that is, investigations made by
professionals in industry.

In practice, forensic datasets -- and digital forensics as a whole --
are used for a variety of purposes. In particular, Conklin et
al.~identify three primary cases in which digital forensics may be
performed \cite{conklinComputerForensics2022}:

\begin{itemize}
\tightlist
\item
  \emph{To investigate computer systems related to a violation of law}:
  This includes cases such as the distribution and storage of illegal
  content, the use of a computer to launch denial of service attacks
  against an individual or organization, and the proliferation of
  harmful malware within an organization.
\item
  \emph{To investigate computer systems for compliance (or a violation
  of) an organization's policies}: This primarily covers internal
  investigations in which a user may act well within the laws of their
  jurisdiction, but may have violated a company policy. For example,
  many companies restrict access to computing systems based on the time
  of day as a security measure. Although a user may normally have
  authorization to access the organization's network, an unexpected
  weekend access to the network may require investigation to determine
  if the activity was done with malicious intent.
\item
  \emph{Responding to a legal (or internal) request for digital
  evidence}: This process is known as e-discovery, in which an
  organization preserves and produces digital information typically as
  part of the discovery process in civil lawsuits. With court approval,
  organizations can be compelled to turn over relevant information to a
  particular lawsuit, including digital documents and digital artifacts
  such as file metadata. (The \emph{Federal Rules of Civil Procedure}
  were amended in December 2006 to include ``electronically stored
  information'' as part of civil discovery
  \cite{withersj.ElectronicallyStoredInformation2006}.)
\end{itemize}

Additionally, it is important to note that the acquisition of forensic
images are only one specific part of the overall digital forensics
process, in which analysts must consider the priority of obtaining
evidence and local requirements for ensuring the admissibility of
digital evidence into a court of law. Again, Conklin et al.~identify
several steps throughout the lifespan of a digital forensic
investigation that are summarized here
\cite{conklinComputerForensics2022}:

\begin{itemize}
\tightlist
\item
  \emph{Identification}: While outside of the scope of forensics itself,
  it is important to determine the scope of the devices that need to be
  analyzed as a result of some incident. Necessarily, it is impossible
  to investigate an incident until an organization can ascertain that
  one has occurred (which, in turn, requires implementing detection and
  protection as described in the NIST Cybersecurity Framework
  \cite{nistNISTCybersecurityFramework2023}). Similarly, if the
  scope of an incident is poorly-defined, the subsequent forensic
  investigation may fail to find significant related evidence, which has
  a direct outcome on the success of the investigation (and often, the
  response of the organization as a whole).
\item
  \emph{Preservation}: After identifying the relevant machines, analysts
  must secure and preserve the physical device itself. With guidance
  from the organization and an analyst's judgment, this often involves
  prioritizing the devices that must be imaged first; for example, a
  critical server may be more likely to cycle out important logs first,
  or an employee's device may only hold important information in
  volatile memory.
\item
  \emph{Collection:} At this point, an analyst must now duplicate the
  digital evidence, in addition to any relevant physical evidence. This
  must be done in a way that passes legal scrutiny; that is to say, it
  must meet requirements for accuracy, reliability, and relevance
  \cite{conklinComputerForensics2022,garfinkelBringingScienceDigital2009}
  (in the United States). In the case of disk imaging, this is typically
  done with a write blocker and cryptographic hashing algorithms to
  ensure a faithful copy has been created of nonvolatile memory.
\item
  \emph{Analysis}: Here, an analyst uses tools and their own knowledge
  to identify significant pieces of information within collected images,
  reconstructing data fragments and drawing conclusions to form a
  coherent timeline and scenario. In many cases, this is done using
  tools such as Sleuth Kit, Autopsy, EnCase, and other domain-specific
  software \cite{jonesInsightDigitalForensics2022}, which often
  parse and automatically identify data of interest on a reconstructed
  file system.
\item
  \emph{Reporting}: After an analyst (or a team of analysts) has
  completed their analysis of the collected data, they must summarize
  and provide a non-technical overview of the conclusions drawn from the
  investigation.
\end{itemize}

Throughout this process, analysts must adhere to well-established norms
for ensuring the admissibility of any digital evidence in court. One
notable example is the use of a chain of custody, which details who has
had access to the evidence. This applies to ``conventional'' and digital
forensics alike, as this documentation asserts that the evidence has not
been tampered with as it is transported between analysts and locations.
Without proper documentation such as a chain of custody, critical
evidence may not meet the legal requirements for admissibility, changing
the outcome of a trial.

Generally speaking, a forensic investigation is only part of a larger
incident response effort; for example, if a data breach occurs, a
digital forensic investigation may be used to determine the scope and
methodology of the breach itself. However, other teams within the
organization may be responsible for patching the devices responsible for
the breach, determining legal consequences, and engaging in other
recovery-related activities. Additionally, a forensic analyst may have
other responsibilities outside of this ``chain''; for example, in
addition to developing a written report, they may be called as an expert
witness to testify and defend conclusions drawn from the investigation
\cite{andersonComparativeStudyTeaching2006,conklinComputerForensics2022,cooperStandardsDigitalForensics2010}.

In the context of research and education, most hands-on or practical
training typically focuses on the analysis and reporting steps, rather
than the rest of the investigation process. That is to say, although the
theory provided in training covers this entire process in detail, it is
relatively rare for students to secure data from an organization, use a
write blocker and other hardware needed to image devices, and then
present the conclusions as an expert witness
\cite{cooperStandardsDigitalForensics2010}. While experience here is
important, it is often impractical; not every university will have a
large forensics lab or a courtroom regularly available. Additionally,
the techniques of analysis and reporting are arguably the most
important; while the other steps can be learned ``on the job''
relatively quickly, all students must be familiar with modern software
and tooling, as well as recent advancements in forensic and
anti-forensic techniques. (As a more concrete example: deep technical
knowledge of operating systems is not required to acquire disk images,
but it is certainly required for effective analysis and reporting.)

It is for this reason that the vast majority of education and research
focuses primarily on the analysis step. Improvements to the distribution
and creation of training material -- such as the use of online labs and
synthesizers -- are largely driven by the fact that no physical lab is
strictly necessary, focusing instead on the software and skillset needed
to effectively analyze images and draw conclusions
\cite{bruecknerAutomatedComputerForensics2008,lawrenceFrameworkDesignWebbased2009}.

\subsection{In research}\label{in-research}

In many cases, the focus of research in digital forensics is on
improving specific processes in the analytic step of a forensic
investigation. This includes the development of analysis techniques for
niche platforms, direct improvements to existing techniques, or novel
methodologies for performing forensic investigations for a particular
platform. For example, recent publications from the Digital Forensic
Research Conference detail a new hashing technique for detecting
similarities in arbitrary files \cite{changFbHashNewSimilarity2019},
an analysis of the NAND memory of the Nintendo 3DS
\cite{pessolanoForensicAnalysisNintendo2019}, and the development of
a new tool for the automated analysis of Android mobile devices
\cite{linAutomatedForensicAnalysis2018}. Necessarily, each of these
publications require forensic datasets; in most cases, these are
obtained or developed manually by the authors, as opposed to using an
existing public or private dataset.

Besides novel contributions to the field, other research focuses on
upholding the quality of the investigation process as new technologies
and tools to analyze datasets are developed. For example, NIST maintains
the \emph{Computer Forensics Tool Testing} (CFTT) program, which
provides a common methodology and test corpora for evaluating specific
tool capabilities
\cite{nationalinstituteofstandardsandtechnologyComputerForensicsTool2017}.
Specific capabilities tested include the ability of a tool to perform
string searching, disk imaging, and the recovery of deleted files. The
project also includes catalogs for forensic algorithms, software, and
tools, as well as the \emph{Computer Forensic Reference Data Sets}
(CFReDS) project, which is a repository of forensic datasets contributed
by NIST and other organizations that is often used by both instructors
and researchers
\cite{nationalinstituteofstandardsandtechnologyCFReDSPortal}.

\subsection{In education}\label{in-education}

Finally, we address the generation of forensic datasets as it applies to
education. Datasets in an educational setting typically focus on
covering a range of techniques and tools, allowing students to practice
applying theoretical concepts learned through lectures and recitations
\cite{adelsteinAutomaticallyCreatingRealistic2005}. Often, this is
done in the context of a specific scenario, such as a user stealing
files from a protected company server, using steganography to hide
information, or recovering a file from volatile memory. Besides the the
specific technical skills covered by these scenarios, the overall goal
of these images is to develop the analytical skills needed to adapt to
new technologies. as tools and the technologies they analyze continue to
evolve \cite{cooperStandardsDigitalForensics2010}. In other words,
students should be familiar with common tools and patterns in digital
forensics, providing a foundation on which more niche techniques can be
learned \cite{lawrenceFrameworkDesignWebbased2009}.

Although the focus of individual forensic images is often to improve the
skills of students in the analysis phase, these images have a direct
impact on the reporting phase as well. Indeed, students must be able to
accurately summarize their conclusions, using their judgement to
describe a scenario and identify topics of interest in a manner that is
consistent with law. Although important, digital forensics is not just
the effective use of tools and techniques; a student should also aim to
be well-rounded in the legal, social, and professional aspects of
digital forensics \cite{andersonComparativeStudyTeaching2006}.

The challenge, however, is providing hands-on labs that comprehensively
support the ideas learned in theoretical courses. Indeed, instructors
face numerous challenges when providing realistic lab material,
including limited access to the necessary software and hardware; the
extensive time needed to develop, distribute and grade labs; and the
high variability between different forensics programs
\cite{adelsteinAutomaticallyCreatingRealistic2005,guptaDigitalForensicsLab2022,lawrenceFrameworkDesignWebbased2009}.
Much of this difficulty in providing high-quality images can be traced
to the issues associated with acquiring images for educational purposes,
as described in the following section.

\section{Real and synthetic
datasets}\label{real-and-synthetic-datasets}

Now that we have identified the purpose of forensic datasets and why
they are needed, we now move to a discussion of how these datasets are
acquired, as well as various issues encountered when using these
datasets. This section focuses on the qualities of real and synthetic
datasets, including some examples. A more detailed survey of existing
datasets is presented in \autoref{existing-forensic-corpora}.

Broadly speaking, there are two types of forensic datasets summarized by
Park \cite{parkTREDEVMPOPCultivating2018}, based on the original
taxonomy described by Garfinkel et al.
\cite{garfinkelBringingScienceDigital2009}. The first is ``real''
data, which is data that was organically created by human beings without
the explicit intent of being used in a forensic investigation. The other
is ``synthetic'' data, which is data generated for specific forensic
purposes, including research and education. Synthetic datasets are
considerably more common than real datasets in education for the reasons
discussed in this section, though their manual creation still poses a
significant problem.

It should also be noted that this section provides a general overview of
forensic datasets as it relates to education and research, providing the
necessary context for forensic synthesizers. However, a more exhaustive
description of dataset construction and usage has been performed by
Horsman and Lyle in \cite{horsmanDatasetConstructionChallenges2021}.

\subsection{Real datasets}\label{real-datasets}

Real data is inherently the most ``realistic'' form of forensic data,
containing extensive background noise as a result of typical computer
usage in addition to a broad variety of software, operating system
artifacts, and other files that might be of interest in a real forensic
investigation. These are most reflective of the scenarios that industry
professionals face, and allow students to train themselves in separating
relevant content from irrelevant content while identifying and
synthesizing details from both a system and a human perspective.

In general, real data can be sourced from far more than just the hard
drives of computers. Other potential and previously used data sources
include:

\begin{itemize}
\tightlist
\item
  social media (which often contains a variety of artifacts with
  revealing metadata and open source information)
  \cite{baggiliDataSourcesAdvancing2015};
\item
  packet sniffers and dedicated forensic tools for IoT devices
  \cite{meffertForensicStateAcquisition2017};
\item
  video game consoles
  \cite{grajedaAvailabilityDatasetsDigital2017,pessolanoForensicAnalysisNintendo2019};
\item
  cloud web server logs \cite{rahmanNewWebForensic2020};
\item
  honeypots \cite{mochForensicImageGenerator2009};
\item
  and the Apache and Python mailing archives
  \cite{grajedaAvailabilityDatasetsDigital2017}.
\end{itemize}

Many more public forensic repositories are described by Grajeda et al.,
though just as many datasets used in research remain private
\cite{grajedaAvailabilityDatasetsDigital2017}. Within reason, with
the billions of internet-connected devices today, there should be no
shortage of sources for real datasets. However, real datasets are the
most challenging to work with in education for a variety of reasons,
particularly the legal and privacy concerns surrounding the use and
distribution of these datasets, as well as their broad scope and lack of
prior analysis.

Necessarily, real datasets were not created to educate students about a
particular technique, and may not adequately supplement an instructor's
material without significant effort. For example, Garfinkel identifies
several requirements for forensic datasets to be suitable for a broad
variety of uses in research and industry
\cite{garfinkelForensicCorporaChallenge2007}. In particular,
Garfinkel notes that in addition to the lack of variety in publicly
available forensic corpora, many real datasets suffer from a lack of
complexity, supplemental annotations, or ongoing maintenance - all of
which are often needed in an educational context.

With respect to availability, some of these datasets are inherently
publicly available; in other cases, their access is restricted to within
an organization. However, just because the underlying data is public
does not necessarily mean that it exists in an aggregate form
immediately suitable for research. Even when aggregated, questions arise
from the use of public datasets for unintended purposes, such as the use
of social media as a source of forensic data; these questions are
already the focus of debate in generative AI, which use publicly
available data to train models for both commercial and research use
\cite{avrahamiOwnershipCreativityGenerative2021,eshraghianHumanOwnershipArtificial2020,rooseAIgeneratedPictureWon2022}.

That said, the primary barriers to the use of real forensic datasets are
privacy and legal issues. For example, between 1998 and 2006, Garfinkel
acquired over 1,000 hard drives through secondary markets, allowing
researchers to perform a range of studies on a large set of real-world
data \cite{garfinkelForensicCorporaChallenge2007}. Although legal
for use by private institutions, the same dataset was barred from use at
the Naval Postgraduate School due to concerns with federal privacy
legislation. Similarly, Grajeda et al.~noted that nearly half of all
digital forensic literature reviewed using a novel dataset did not
publish the dataset due to legal restrictions or NDAs. This was
typically because the datasets were obtained from government agencies,
corporations, and law enforcement agencies, and therefore could not be
publicly released \cite{grajedaAvailabilityDatasetsDigital2017}.
Another reason is that the dataset may contain objectionable or illicit
material, such as licensed software or pornography, preventing its
public distribution.

Certain organizations and researchers have made efforts to make real
datasets accessible to the public through various means. For example,
the emails seized during the Federal Energy Regulatory Commission's
investigation of Enron were purchased by the Massachusetts Institute of
Technology, which anonymized emails and attachments before distributing
the dataset to the public
\cite{yannikosDataCorporaDigital2014,garfinkelForensicCorporaChallenge2007}.
In other cases, institutions have aimed to ensure the data can be made
publicly available ahead of time, such as by requesting that individuals
sign an agreement before any data is collected.

Indeed, some educational institutions do use real-world datasets for
forensic labs. Besides the sources mentioned above, students may also
opt to image their own devices or the device of a friend with
permission; these approaches are mentioned in older works
\cite{andersonComparativeStudyTeaching2006,mochForensicImageGenerator2009},
prior to the advent of online platforms such as CFReDS that provided
easier access to education-focused datasets. Naturally, this method of
acquiring real-world images suffers from other issues as well; students
know exactly what they will find on their own computer, and although
individuals may consent to the use of their images for educational
purposes, there remains the risk of highly personal data being leaked
\cite{garfinkelBringingScienceDigital2009}. Simultaneously, they
often lack the ground truth or annotations needed to assert that a
student has found everything of interest.

\subsection{Overview of synthetic
datasets}\label{overview-of-synthetic-datasets}

Because of the various issues associated with real-world data, many
instructors opt to use synthetic data (or ``manually created data'')
instead, in which the scenario and data are artificially generated based
on some predetermined procedure. That is to say, the data exists with
the explicit intent of being used in research or education.

From an educational perspective, an ideal dataset should accurately
reflect the problems and challenges faced by industry professionals
while avoiding the problems of real-world datasets. In turn, synthetic
datasets are often created with the express intent of allowing students
to explore forensic techniques in realistic scenarios while avoiding the
privacy and legal restrictions of genuine real-world scenarios. (The
same largely holds true in research, as well.)

Again, as described by Park and Garfinkel et al., synthetic data can be
categorized into two distinct groups
\cite{garfinkelBringingScienceDigital2009,parkTREDEVMPOPCultivating2018}:

\begin{itemize}
\tightlist
\item
  \emph{Synthetic test data}, which refers to forensic corpora that have
  been developed to test specific features in a group of tools. These
  are well-annotated datasets with extensive reference information and
  ground truth data, and are typically used to assert that a tool is
  able to analyze and identify data of interest. One example discussed
  so far is the Computer Forensics Tool Testing program administered by
  NIST
  \cite{nationalinstituteofstandardsandtechnologyComputerForensicsTool2017}.
\item
  \emph{Synthetic realistic data}, which is designed to mimic a
  situation that a forensic examiner might encounter in a real-world
  investigation. These are typically much more applicable to an
  educational context than test data, though test data can be used as
  educational material for learning new tools or very specific
  techniques.
\end{itemize}

Synthetic realistic data varies greatly in scope. Simple realistic data
might form a realistic emulation of how a user might use a particular
piece of software or anti-forensic technique, which creates forensic
artifacts directly associated with these actions. In contrast, realistic
data could form a full simulation of a scenario, whether based on real
events (such as the one developed by Moch and Freiling based on the Arno
Funke blackmail case in Germany
\cite{mochForensicImageGenerator2009}) or on common industry themes,
such as the NIST CFReDS data leakage case
\cite{nationalinstituteofstandardsandtechnologyCFReDSDataLeakage}.

\subsection{Motivation for synthetic
datasets}\label{motivation-for-synthetic-datasets}

Besides the issues mentioned in \autoref{overview-of-synthetic-datasets}, one major motivation for the development of synthetic
datasets is the need to fill various gaps in publicly available corpora.
The survey done by Grajeda et al.~found that some researchers created
new datasets not because of legal or privacy hurdles, but simply because
there was no available dataset for their needs
\cite{grajedaAvailabilityDatasetsDigital2017}. The researchers
interviewed in the survey also recognized the value of publishing their
datasets, but were hindered by a lack of available resources to maintain
or distribute the datasets. Other researchers were prevented from doing
so due to a non-disclosure agreement. The survey concluded that there
was no preference for building new datasets from scratch in research,
though there was clear agreement that private datasets made it more
difficult for researchers to reproduce results; in general, publishing
datasets contributed to the advancement of the field.

Similar sentiment also exists in an educational context, though the
motivation for creating new images from scratch differs from that of
research. For example, the direct reuse of existing datasets is often
undesirable, as it is often the case that answer keys and walkthroughs
have already been published online
\cite{woodsCreatingRealisticCorpora2011}; additionally, students in
the same class can simply replicate the exact methodology of other
classmates to come to the same conclusions without needing to perform
meaningful analysis. Even in labs where students' actions can be
monitored to provide insight into their methodology and possible
mistakes, such as in the \emph{CYDEST} platform developed by ATC-NY
\cite{bruecknerAutomatedComputerForensics2008}, it is still possible
to exactly copy the methodology of another student if the labs
themselves do not differ.

Additionally, many datasets that may be reflective of real-world
scenarios are often unsuitable for academic purposes. For example, the
forensic challenges of the Honeynet Project, DFRWS, and DC3 are suitable
training material for experienced forensic analysts, but are often too
difficult for students to solve
\cite{woodsCreatingRealisticCorpora2011}. As mentioned before, many
real-world datasets lack ``ground truth'' or annotated information that
is suitable for determining what the contents of an image are,
preventing the development of an answer key and qualitative grading of
students' work based on what they find and report. The same is true of
certain synthetic datasets, which may have been developed without a
focus on detailed documentation.

Clearly, there is a need for images that allow students to explore the
techniques and tools used by forensic analysts in industry in a
realistic setting. Preferably, these images should be built in such a
way that they challenge students to explore the same technique, but have
distinguishable differences to dissuade cheating (for example, different
files of interest may be placed, the specific disk sector written to may
vary, or relevant metadata may be changed). Simultaneously, instructors
need to know what the contents of the image are (including these
variations), so that they can judge the accuracy of students' findings
and determine which images are suitable hands-on material to complement
lecture material.

As a result, many instructors ultimately turn to manually developing
realistic forensic scenarios that students can analyze, much like
researchers. Some of these efforts have lead to publicly available
datasets that encompass realistic scenarios, such as the datasets and
scenario published by Woods et al.~as a direct response to the issues
noted above \cite{woodsCreatingRealisticCorpora2011}.

\subsection{Challenges in developing synthetic
datasets}\label{challenges-in-developing-synthetic-datasets}

However, fulfilling all of the ideal characteristics of a forensic
dataset is difficult, especially if an instructor wants to create
variations of the same forensic scenario, multiplied by the number of
scenarios used throughout the course. There are three distinct issues
associated with the manual development of forensic datasets: the
significant amount of time involved in their creation, the inherent
non-determinism of human creation, and the lack of realistic background
usage.

The primary issue associated with manual development is the extensive
amount of work and time that must be put into not only planning and
developing the scenario, but also executing it. Virtually all prior
works recognize the time-consuming process involved in the manual
development of disk images suitable for education
\cite{adelsteinAutomaticallyCreatingRealistic2005,gobelForTraceHolisticForensic2022,guptaDigitalForensicsLab2022,mochForensicImageGenerator2009,russellForensicImageDescription2012,scanlonEviPlantEfficientDigital2017,woodsCreatingRealisticCorpora2011},
motivating research into possible solutions to streamlining their
development.

Depending on the artifacts involved in the scenario, it can also be
difficult to ``falsify'' metadata as a means of speeding development up.
More precisely, it can be challenging to produce the artifacts
associated with a scenario over a shorter period of time than is
suggested by the metadata attached to the artifacts. For example, it is
possible to directly change the time in virtualization software (the
approach often used by synthesizers
\cite{gobelForTraceHolisticForensic2022,mochForensicImageGenerator2009})
to allow time to pass without leaving any artifacts that suggest that
the system time has been tampered with. However, making this consistent
with online services can be challenging, since artifacts such as emails
or cached external websites would be based on ``real'' time; additional
work would then be needed to synchronize online content outside of the
control of the instructor. While there are workarounds that could be
considered -- such as saving the contents of various external websites
over a period of time and then ``replaying'' them at the time of
scenario development -- these are not trivial.

Finally, the long development period comes with the challenge of
addressing mistakes. In the event a mistake is made relative to the
desired scenario, instructors have several options. They may be able to
simply log the deviation and move on, or correct it after the fact by
directly editing the virtual hard drive after image creation. This was
the approach taken by Woods et al.~in which at least one researcher
logged into their personal email account while developing a fictional
scenario; the images were later scanned for personal identifiers and
stripped as needed \cite{woodsCreatingRealisticCorpora2011}.
However, consider a scenario in which it is absolutely essential that
certain files are deleted and written in a particular order, perhaps to
ensure the operating system behaves in a specific manner. In this case -
and possibly many others - the only option may be to start the process
of developing the image from the beginning.

The nondeterminism associated with human creation is a double-edged
sword. Given the same general scenario, this allows multiple images to
differ very slightly in nature (a desirable feature as described earlier
in this section); as described by Woods et al., the researchers all
acted out events in a pre-defined timeline, but did not do so at the
exact same time or in the exact same way
\cite{woodsCreatingRealisticCorpora2011}. This provides the
variability needed to prevent students from directly copying answers,
but allows students to explore the same techniques at a high level.

Simultaneously, however, this also adds a degree of uncertainty within
the images. Various artifacts or results on the images may be the result
of some confounding factor in the procedure; for example, suppose that
two researchers follow the same procedure to delete and overwrite a file
with the goal of making an image that allows students to explore file
carving in slack space. Due to variations in operating systems, drivers,
and other related software, it may be the case that the recoverable
contents in the slack space differ significantly between the two
researchers. Similarly, consider a case in which two researchers follow
the same procedure to develop volatile memory captures on a Linux
machine. It might be the case that the paging daemon evicts relevant
data for one image but not the other, causing significant differences.
Without careful preparation in creating a consistent environment for
development and testing, these differences might arise without a clear
reason. Where possible, sources of uncertainty should be minimized or at
least known to the scenario developer.

Finally, an inherent limitation of the long development time needed to
produce these images is the difficulty in populating realistic
background noise, in which a user performs normal activities on the
computer that are completely unrelated to the meaningful components of
the scenario. This must be done in a way that is realistic to the
scenario while avoiding any activity that could personally identify the
researcher or instructor. Although some of this background noise can be
automated - such as by scripting the process of browsing to websites or
sending emails - the fact remains that many realistic ``background''
activities, such as working in software such as Microsoft Office,
MATLAB, and SolidWorks, are not easy to perform without a human.
Furthermore, some of this background noise may comprise a significant
portion of the day, such as if these activities comprise a user's day
job in the scenario. Naturally, it is not the case that most instructors
or researchers have the time available to spend several hours each day
generating this information, much less over multiple variations of
multiple scenarios.

\section{Research objectives}\label{research-objectives}

At this point, we have clearly established a need for a more streamlined
method of developing scenarios for research and education, while still
providing the variability and content needed for images to be
interesting and adequately train students in forensic methodologies.
Additionally, these scenarios need to provide ground truth data to
determine the artifacts contained in each image, allowing instructors to
identify what should be contained in students' reports of the scenario.
Some research has been done into developing image synthesizers, which
aim to automate part or all of this work, as described in \autoref{chapter-two}.

This thesis aims to directly improve upon the foundations provided by
prior synthesizers, with the explicit goal of achieving feature parity
with all existing synthesizers. In particular, this thesis will describe
the components and architecture necessary to build an effective forensic
synthesizer for research and education with the following elements:

\begin{itemize}
\tightlist
\item
  The ability to create and export disk images, network captures, and
  volatile memory captures through arbitrary means;
\item
  the ability to generate forensic artifacts in a modular manner, both
  with and without operating system virtualization;
\item
  the ability to log every action in a standardized format, the
  Cyber-investigation Analysis Standard Expression (CASE)
  \cite{caseyAdvancingCoordinatedCyberinvestigations2017}, to
  address existing standardization concerns
  \cite{horsmanDatasetConstructionChallenges2021};
\item
  and the use of modern Python libraries, standards, and practices,
  promoting future development by reducing the overall complexity of the
  framework and abstracting specific concepts where possible.
\end{itemize}

Furthermore, this thesis aims to leverage recent advancements in
generative AI to streamline the development of full forensic scenarios,
as well as individual forensic artifacts to be added to a larger
forensic dataset. Finally, this thesis will evaluate the viability of
this framework in an actual classroom setting.

\section{Contribution}\label{contribution}

This thesis contributes the \emph{automated kinetic framework}, or AKF,
a modernized synthesizer framework that aims to provide the foundation
of a larger forensic dataset ecosystem. This framework can be used to
not only vastly reduce the time spent developing new datasets for
research and education, but also improve the discoverability of both
AKF-generated and non-AKF-generated datasets. Additionally, by focusing
on the long-term viability of AKF through its modular architecture, the
hope is that educators and researchers will have greater variety in the
datasets available to them, even as new developments and advancements in
technology occur.

In many ways, this thesis is intended to be the culmination of over 15
years of development in the field of synthesizers (beginning with
Forensig2 \cite{mochForensicImageGenerator2009}), re-implementing
the unique contributions of previous synthesizers using modern
techniques and technologies.

\chapter{Literature review}\label{chapter-two}

We begin with a literature review of two topics: the publicly available
forensic datasets that currently exist and the contributions of prior
synthesizers. In particular, we describe the datasets that currently
exist for digital forensics in greater detail, as well as gaps
identified in these datasets by various authors. This will provide
context for both the need of synthesizers, as well as the specific gaps
that these synthesizers have gradually filled.

\section{Existing forensic
corpora}\label{existing-forensic-corpora}

Forensic datasets have been available for public use (or by request)
since the early days of the digital forensics field, though their
sourcing and qualities have changed greatly over time. This is explored
in far greater detail by Grajeda et al., who performed a survey of over
700 research articles to identify the various datasets used by the
digital forensics field over time. However, it is still important to
recognize certain datasets identified by prior literature as it is
relevant to synthesizers.

Early datasets were largely derived from real sources, whether made
available to the public or otherwise. The earliest collections of real
datasets include the used hard drives collected by Garfinkel from 1998
to 2006 and the Enron email corpus obtained during the federal
investigation of Enron \cite{garfinkelForensicCorporaChallenge2007}.
Other early real datasets identified by Grajeda et al.~include the
public Apache mailing archive, the Reuters news corpora, and various
facial recognition collections such as the MORPH corpus, all of which
were made in the early to mid-2000s
\cite{yannikosDataCorporaDigital2014,grajedaAvailabilityDatasetsDigital2017}.

There were also a variety of synthetic datasets constructed during this
early period. This includes the network captures obtained from simulated
attacks conducted by the MIT Lincoln Laboratory from 1998 to 2000
\cite{garfinkelForensicCorporaChallenge2007}, as well as standalone
datasets used for tool validation developed as part of the early CFTT
program developed by NIST. Notably, some synthetic datasets during this
period were generated as part of challenges, such as those produced for
DFRWS conferences, which are noted as being difficult for students to
solve \cite{woodsCreatingRealisticCorpora2011}.

The variety of forensic datasets increased considerably towards the late
2000s, which can be credited to both the overall growth of the field
(including the broader field of incident response) and computing as a
whole. Various notable sources described by Grajeda et al.~include
malware samples discovered ``in the wild'', large natural language
collections from multiple languages, and file-specific datasets such as
collections of Microsoft Office files. It was also during this time that
non-disk datasets began to be more prevalent, such as volatile memory
dumps and network captures. Although not explored by this thesis in
great detail, mobile datasets -- such as smartphone disk images, mobile
malware and applications, and SIM card images -- grew in prevalence as
well.

Many of these datasets were not maintained as part of a larger
collection of datasets with the explicit intent of providing them for
digital forensics research. This began to change towards the late 2000s;
Garfinkel's collection would eventually evolve into the Real Data
Corpus, growing to 30 terabytes by 2013
\cite{garfinkelBringingScienceDigital2009a,yannikosDataCorporaDigital2014}.
The collection then included hard disk images, flash drive images, and a
variety of optical discs sourced from real-world usage, requiring
institutional review board approval to use. This collection would
eventually be part of the Digital Corpora platform, which includes a set
of purely synthetic datasets. Separately, NIST began developing the CFTT
and CFReDS projects, both of which provide forensic datasets for a
variety of purposes. Digital Corpora, CFTT, and CFReDS are actively
maintained as of writing.

There are other datasets that are rather unique in nature, and are
maintained as part of a larger niche collection. Collections of malware
samples have grown significantly in size, in part because of the modern
threat intelligence ecosystem with platforms such as VirusTotal. There
exists datasets focusing on the dark web, such as those operating on the
Tor network, including ``black market'' sites on which illegal goods are
bought and sold. Finally, there exist network captures from various
novel sources, including various iterations of the Collegiate Cyber
Defense Competition and various network captures from university IT
departments \cite{grajedaAvailabilityDatasetsDigital2017}.

These datasets span a wide range of technologies -- in particular,
versions of the same technology -- that require different methodologies
to effectively analyze. In particular, there is the challenge of making
datasets available that reflect current advancements in technology, such
as new operating system versions or new applications. For example,
instant messaging applications have changed considerably over the
history of the field, ranging from MSN Messenger in the early 2000s to
Skype, Discord, Telegram, Signal, Slack, and more. Artifacts from each
of these applications must be handled differently, even if the
underlying service provided is largely the same. Similarly, the
strategies for analyzing Windows artifacts have changed significantly
from version to version, as new registry keys become relevant in
analyzing applications while others become unused.

It is this gradual ``aging'' of datasets, in which their relevance
degrades over time, that contributes to the continuous need for new
datasets. Furthermore, there have been a broad variety of datasets that
have proved to be relevant in digital forensics research, even if not
typical or immediately evident. Indeed, this is one of the reasons
identified by Grajeda et al.~for the manual development of new datasets
by research authors; it would often be the case that a modern dataset
simply did not exist for their needs. The other motivation is that the
dataset used was never made public, either due to the author not having
the resources to distribute the dataset themselves, or because there
were legal or privacy concerns.

How are these datasets relevant to the development of synthesizers?
Clearly, there has been a broad variety of datasets that have proved to
be relevant in digital forensics research, even if not immediately
evident. It should be the goal, then, that any effort to streamline the
develop of forensic datasets be able to cover as many use cases as
possible. To do so, a synthesizer should aim to achieve two separate
goals:

\begin{itemize}
\tightlist
\item
  A synthesizer should be able to replicate the features of existing
  datasets, provided that the underlying technologies are still
  available; and
\item
  A synthesizer should be able to account for developments in operating
  systems, applications, or other technologies, without requiring
  significant changes to the underlying architecture.
\end{itemize}

Indeed, the synthesizers described in the following section have
explicitly addressed these two concerns. For example, many of these
synthesizers have focused on implementing features that reflect the
qualities of real-world datasets. This includes the ability to execute
malware samples in a virtualized environment, send emails to arbitrary
email servers, and insert data on removable drives -- all of which
generate forensic artifacts that have been used as key datasets in
forensics-related research as shown here. Similarly, modern synthesizers
are capable of larger generating disk images, network captures, and
volatile memory dumps (in addition to extracting specific artifacts,
such as application-specific files), which are also reflective of the
focus of existing datasets. The gradual growth in the ability of
synthesizers to generate various artifacts can be seen in the specific
contributions made by each synthesizer in \autoref{analysis-of-existing-synthesizers}.

Of note is the generation of similar datasets that typically involve
significant human interaction, such as public email distribution lists,
photographs of human faces, and other transcripts of conversations.
While this has not been explored in significant detail by prior
synthesizers, it is explored as part of the generative AI work done as
part of AKF in \autoref{chapter-six}.

The second issue, in which synthesizers must be extensible in such a way
that they can support new applications, has been approached in several
ways. This is described in greater detail in \autoref{chapter-three}, but has been a major design consideration in the
development of most synthesizers.

\section{Analysis of existing
synthesizers}\label{analysis-of-existing-synthesizers}

Numerous frameworks have been built over the last two decades that aim
to significantly reduce the effort involved in creating synthetic images
from scratch by automating various application- and OS-specific actions
at specified locations and times. The functionality and availability of
the frameworks described in literature have varied considerably over
time, but the goal of these frameworks has remained largely consistent:
they all aim to provide a rapid method for instructors to develop
forensic labs for students. Note that low-level studies are described in
later, more relevant sections throughout this thesis for clarity, rather
than providing detailed analyses as part of this chapter. Before
analyzing and adapting the low-level implementations of prior
synthesizers, it is best to first contextualize their broader
contributions.

The first identified effort to streamline the creation of forensic labs
through a high-level language was published by Adelstein et al.~in 2005
through the development of \emph{FALCON, the Framework of Laboratory
Exercises Conducted Over Networks}
\cite{adelsteinAutomaticallyCreatingRealistic2005}. It proposed an
architecture for not only the automatic creation of lab exercises, but
also their deployment to a virtual lab and the evaluation of students'
actions compared to the ground truth. Very few details of the
implementation are provided, though some examples of its usage are
provided. Although not primarily a synthesizer, a related work was in
the development of \emph{CYDEST, the CYber DEfenSe Trainer}
\cite{bruecknerAutomatedComputerForensics2008}, which similarly
provided complex virtualized lab environments to students via the
internet. Again, limited implementation details are provided, and it is
unclear if either \emph{FALCON} or \emph{CYDEST} are publicly available
or actively maintained.

\emph{Forensig2}, described by Moch and Freiling in 2009 and revisited
in 2012, appears to be the first detailed description of an image
synthesizer
\cite{mochForensicImageGenerator2009,mochEvaluatingForensicImage2012}.
Operators define scenarios through a Python 2 library provided by the
authors that abstracts various virtual machine operations (such as the
formatting of disks, the creation of partitions, and the copying of
files from the host to the virtual machine). Actions are performed live
through a Qemu-based VM, with the result being a ground truth report and
(effectively) an image to be analyzed by students. The source code for
\emph{Forensig2} is not currently maintained and appears to be
unavailable.

The \emph{Digital Forensic Evaluation Test (D-FET)} platform described
by William et al.~in 2011 provides a custom scripting language for users
to define system- and user-level actions
\cite{williamCloudbasedDigitalForensics2011}. Similar to
\emph{FALCON} and \emph{CYDEST}, it provided virtualized labs through
VMware ESXi. Unlike most other frameworks, it was primarily built to
provide a platform to efficiently evaluate digital forensic tools on a
scalable infrastructure, rather than streamline the process of building
and distributing labs for instructors (though it is mentioned that the
infrastructure was also used to host student labs). It is unclear if a
public implementation is available.

Russell et al.~define an XML specification, the \emph{Summarized
Forensic XML (SFX)} language, to describe scenarios as a sequence of
various actions on various partitions of a disk
\cite{russellForensicImageDescription2012}. This can be passed into
an interpreter to produce an image that can be analyzed by students.
Besides high-level operations such as disk partitioning and file
copying, it also provides Windows- and Linux-specific routines for
minimizing partition sizes with the goal of reducing the ``effective''
size of irrelevant files associated with the OS. It also briefly
describes approaches for updating web browsers and the Windows registry
to reflect certain actions. Limited implementation details are provided,
and it is unclear if a public implementation is available.

Yannikos et al.~define a framework for describing scenarios as a series
of Markov chains, allowing users to graphically define scenarios through
a graph view in which nodes and relationships can be easily edited
\cite{yannikosDataCorporaDigital2014}. Its design makes it
well-suited for quickly developing variations of the same scenario from
a large dataset of available artifacts to be placed on the image, though
it is not clear what application- or OS-specific routines are provided.

The remaining frameworks are all largely similar to each other in that
they are all built in Python, providing users with a Python library to
define and generate scenarios. Various functions in the library provide
abstractions to complex actions, such as Facebook browser activity or
sending emails on the Thunderbird application. Additionally, each of
these produce some form of detailed output that can be used as answer
keys or ground truth in an instructional setting. A brief summary of the
notable aspects of each of these frameworks relative to each other is
provided below:

\begin{itemize}
\tightlist
\item
  \emph{ForGeOSI} \cite{maxfraggMaxfraggForGeOSI2023} introduced the
  use of the VirtualBox SDK to automate various operations, forming the
  basis for much of the work done as part of VMPOP.
\item
  \emph{ForGe} \cite{vistiAutomaticCreationComputer2015} is
  specifically designed to generate NTFS and FAT32 images with a
  particular focus on placing data by maintaining and serializing custom
  data structures for supported filesystems.
\item
  \emph{EviPlant} \cite{scanlonEviPlantEfficientDigital2017}
  encompasses a novel method for the distribution of generated images in
  an educational context, whereby ``evidence packages'' are distributed
  to students to apply to a base image file. Then, an OS-native
  injection tool plants relevant artifacts according to the evidence
  package. Since the base image must only be downloaded once, each
  scenario is significantly smaller than if distributed as standalone
  images.
\item
  \emph{VMPOP} \cite{parkTREDEVMPOPCultivating2018} provides an
  architecture and routines for relatively elaborate VirtualBox control
  (such as attaching USB devices and starting video captures) in
  addition to a variety of OS-specific commands. Provided Windows
  routines include the ability to create restore points, install
  programs, map network drives, set registry values, and more. Although
  the architecture as a whole is platform-independent, the provided
  implementations operate with VirtualBox and Windows.
\item
  \emph{hystck} \cite{gobelNovelApproachGenerating2020} provides
  routines for automating OS- and application-specific commands through
  YAML configuration files (passed through a generator) and/or Python
  scripts (for low-level control). Similar to \emph{EviPlant}, it uses
  ``differential'' images to allow lightweight scenarios to be
  distributed relative to ``template'' images, and produces both network
  and disk captures.
\item
  \emph{ForTrace} \cite{gobelForTraceHolisticForensic2022} is the
  most recently developed synthesizer, which directly builds upon
  \emph{hystck} by providing volatile memory captures (alongside disk
  and network captures) in addition to various other new features (such
  as the ability to execute PowerShell scripts to create Windows
  artifacts), with a focus on a modular architecture. A variant focusing
  on Android artifact generation was developed in 2024
  \cite{demmelDataSynthesisGoing2024}.
\end{itemize}

Clearly, a considerable amount of work has been done in exploring ways
to streamline the process of developing images, although the
availability and functionality of each framework varies greatly.
Notably, with the exception of \emph{ForTrace}, none of these works are
direct extensions of past works, implying that at a fundamental level,
it was necessary for frameworks to be developed from scratch to support
the framework authors' needs. This is not directly stated in any of
these works with the exception of \emph{hystck}
\cite{gobelNovelApproachGenerating2020}, though it can be reasonably
concluded that the lack of maturity, availability, and maintenance of
prior works was a contributing factor to the independent development of
the other frameworks. This will be addressed in more detail in
\autoref{chapter-three}.

\chapter{Architecture and design}\label{chapter-three}

\section{Motivation}\label{motivation}

As described previously in \autoref{analysis-of-existing-synthesizers}, with the notable exception of
\textbf{ForTrace} and its related synthesizers, no synthesizer been an
extension of another synthesizer. This raises the question -- why
reinvent the wheel by developing yet another distinct architecture for
this thesis? This subchapter briefly explores the deficiencies in
existing synthesizers and explains why these are issues that warrant
building a new architecture from scratch, rather than extending an
existing synthesizer.

The motivations for developing completely new codebases instead of
extending existing synthesizers has varied considerably over the years.
One reason is that several synthesizers are not open-source and
therefore cannot easily be extended, as is the case with
\textbf{Forensig2} and \textbf{TraceGen}. Another reason is that the
focus of certain synthesizers resulted in a codebase that is simply
incompatible with the goals of newer works. For example,
\textbf{ForGe}'s architecture focuses largely on direct filesystem
manipulation to generate forensic artifacts, and is therefore not
suitable for a synthesizer that requires a virtualized operating system.
Synthesizers such as \textbf{VMPOP}, which exclusively leverages
agentless artifact generation as described in \autoref{agentless-artifact-generation}, require significant
architectural changes to support agent-based artifact generation.

However, perhaps the largest motivation for constructing new
synthesizers is simply the lack of ongoing support for virtually all
synthesizers. It appears that no synthesizer has gained significant
traction within the broader forensic community, possibly with the
exception of \textbf{ForTrace}; for the synthesizers that \emph{are}
open source, none of them are under active development and maintanance.
Additionally, the forensic datasets generated by these synthesizers have
not seen significant adoption in either education or research; many
instructors continue to use the human-generated datasets available on
public platforms.

The inflexibility of prior synthesizers, combined with the overall lack
of support and success of synthesizer-based datasets, contributes to the
disparate codebases that are now observed today. However, this is not to
say that the individual contributions of each prior synthesizer cannot
be merged into a single project that resolves many of the architectural
barriers that have reduced the adoption and extension of existing
synthesizers.

In turn, AKF is built on the following four pillars to help promote its
long-term usage. In particular, it allows it to generate datasets that
address several of the considerations raised by Horsman and Lyle in the
construction of various datasets, such as the need for comprehensive
documentation, an awareness of the ``realism'' of the resulting dataset,
and transparency in the scenario development process
\cite{horsmanDatasetConstructionChallenges2021}.

First, AKF conforms to modern Python development practices. This
includes the use of modern project management practices (such as the use
of \emph{uv} and \passthrough{\lstinline!pyproject.toml!}, rather than
the use of \passthrough{\lstinline!setup.py!} observed in older
synthesizers), as well as static linters (\emph{flake8}) and style
enforcers (\emph{black}, \emph{isort}) to promote adherence to the PEP8
standard. Additionally, AKF's libraries make heavy use of Python 3.11+
features, such as type hinting and special type annotations, which
allows for tools such as \emph{mypy} to perform static type checking. In
addition to greatly improving the development experience, these
practices also increase the likelihood of discovering errors earlier in
the development process.

Second, AKF takes a modular, agent-based approach to implementing
application-specific functionality. This allows AKF to use existing
automation frameworks for web browsers and other applications, greatly
simplifying the codebase when compared to the same features implemented
in other synthesizers. This focus on flexibility makes it significantly
easier to install the agent, implement new application-specific
features, and more - a particularly important design focus given AKF's
dependence on agents for the majority of its application-specific
functionality.

Third, AKF is architected to maintain feature parity with all prior
synthesizers. That is, although AKF deviates considerably from the
implementation details of prior synthesizers, this does not come at the
loss of prior advancements in the field. In particular, AKF supports all
three artifact generation techniques described in \autoref{chapter-four} using hypervisor-agnostic interfaces, reducing the tight
coupling that made certain features difficult to implement in prior
synthesizers. This reduces the likelihood that a new feature or
technique will require a significant architectural change to support it.

Finally, AKF is designed with the explicit intent of developing an
ecosystem of AKF-generated datasets, promoting long-term usage. This is
reflected in the design of AKF's logging and reporting mechanisms as
described in \autoref{chapter-five}; the use of an
RDF-based standard, CASE, allows for arbitrarily complex queries to be
made against AKF datasets. These queries can be made in bulk,
significantly improving the ability of researchers to find and use
datasets that may be relevant to them.

These four pillars reflect throughout the design of AKF's architecture,
which is described in the following section and the next three chapters
of this thesis.

\section{Overview}\label{overview}

At a minimum, every synthesizer must fulfill these high-level
requirements through some means, largely derived from the criteria
developed by Horsman and Lyle
\cite{horsmanDatasetConstructionChallenges2021}:

\begin{itemize}
\tightlist
\item
  The synthesizer must accept commands on how it should operate. These
  commands should serve as a form of self-documentation, in which it is
  clear to another person \emph{what} the expected contents of the image
  are, as well as the intent behind these commands.
\item
  The synthesizer must be able to accept external inputs, such as files
  and other binary data, to be included in any final outputs.
\item
  The synthesizer must implement the mechanisms and technologies
  necessary to carry out the commands it has been provided -- that is,
  it must be able to generate artifacts. Such mechanisms should be
  independently verifiable, and preferably leave as little extraneous
  information as possible.
\item
  The synthesizer must be able to generate a final output (such as a
  disk image), along with ground truth and reporting that describes the
  expected contents of that image. This should include a well-structured
  description of the dataset, a unique identifier for the dataset, and
  an explicit identification of any data of ``evidential value'' where
  possible.
\end{itemize}

These four requirements are fulfilled by various mechanisms throughout
AKF. A complete architecture diagram is shown in the figure below.

!\textbf{Architecture 2025-02-07 17.07.24.excalidraw}

\begin{itemize}
\tightlist
\item[$\square$]
  \#task move this to the appendix and break it up into individual
  colored boxes . . . simply can't be placed here
\end{itemize}

This can be simplified to the following high-level diagram, which
broadly groups AKF into a set of seven modules:

!\textbf{Architecture 2025-02-08 16.35.46.excalidraw}

At a high level, this architecture can be broken up into three distinct
concepts, each of which covers a distinct chapter.

The first set of modules are responsible for artifact generation. This
encompasses three major systems - a hypervisor and its associated SDK,
an OS-specific agent, and \passthrough{\lstinline!akflib!}.
\passthrough{\lstinline!akflib!} is a Python library containing the
abstract interfaces and concrete implementations necessary to generate
individual artifacts and complete datasets. This includes routines for
directly interacting with hypervisors, issuing commands to virtual
machines, and directly modifying virtual hard drives. This library is
also the foundation for OS-specific agents, which carry out actions on
the virtual machine on behalf of the host. This is described in greater
detail in \autoref{chapter-four}, and corresponds to the
\emph{action automation library}, the \emph{OS-specific agent}, and the
\emph{virtual machine} (as well as the \emph{user applications} running
on the machine).

The second set of modules are responsible for logging and reporting.
This encompasses both independent libraries for generating outputs and
ground truth, as well as the various logging-related mechanisms that are
contained throughout the artifact generation libraries. These modules
are responsible for exporting and documenting artifacts generated by
AKF; in particular, it makes heavy use of CASE, a standardized ontology
for documenting the contents of forensic datasets
\cite{caseyAdvancingCoordinatedCyberinvestigations2017}. This is
covered in \autoref{chapter-five}, and corresponds to the
\emph{output and validation library} and any associated components
within the \emph{action automation library}.

The final set of modules are responsible for invoking AKF itself and
supporting scenario development. AKF is an \emph{imperative}
synthesizer, which means that commands are written and executed using an
imperative language (here, Python 3) that dictates exactly \emph{how}
scenarios should be constructed. However, AKF also supports a
\emph{declarative} syntax, which allows users specifies \emph{what}
forensic artifacts and datasets are generated without the need to learn
the AKF libraries and write Python code. Additionally, AKF contains
generative AI tools for constructing scenarios in the declarative syntax
as well as individual artifacts. This is described in \autoref{chapter-six}, and covers the \emph{scenario construction library}
and the \emph{translation unit}, as well as any scripts that leverage
AKF libraries.

The following three chapters will focus on these module groups. In
simpler terms, this thesis addresses the following questions in order:

\begin{itemize}
\tightlist
\item
  How do we automate or streamline the generation of artifacts?
\item
  How do we document and report on the artifacts and datasets that are
  generated?
\item
  Given the solutions that address these two challenges, how do we
  actually use them to build scenarios?
\end{itemize}

\chapter{Action automation}\label{chapter-four}

This chapter addresses the modules responsible for automating artifact
generation, depicted in the partial architectural diagram below.

!\textbf{39.4 - Action automation 2025-02-07 17.15.30.excalidraw}

\begin{itemize}
\tightlist
\item[$\square$]
  \#task include semi-shaded versions of the simpler diagrams here

  \begin{itemize}
  \tightlist
  \item
    For each subsection, provide a ``complex'' diagram focusing
    exclusively on the rectangles implemented for that current section.
    For any interfaces that are nut relevant to the current section,
    have them go to the equivalent labeled ``colored'' section, but
    don't put any details for that section -- just the fact it's the
    ``action automation library'' or the ``virtual machine'' is fine.
  \item
    For each chapter, describe what it interfaces with to begin with.
    Individual sections, which are naturally going to have actual
    implementation details, can go into lower details. Keep the detailed
    diagrams close to the things they're describing -- that's only
    possible if you break the detailed diagram down further by
    subsection.

    \begin{itemize}
    \tightlist
    \item
      regularly use language and figures that reminds the reader of
      where they are in the broader scope of things
    \end{itemize}
  \item
    Consider looking into an ``interaction''-based diagram, that focuses
    more on what the components are \emph{doing} rather than the
    components themselves; I never explain the intentional fact that
    components on a horizontal line all work together to achieve one
    \emph{specific} thing. Can call it out explicitly by coloring lines
    or changing the lines of boxes, or by also just including it in the
    text; more viable in subsections
  \end{itemize}
\end{itemize}

These implement AKF's ability to generate individual artifacts as part
of a larger scenario dataset. This chapter begins with an overview of
the techniques used by prior synthesizers for generating artifacts. It
then moves to a low-level analysis of the actual implementation used by
AKF, while comparing these techniques to other synthesizers.

In particular, this chapter addresses the role of the action automation
library in generating artifacts by interacting with a live virtual
machine and disk images stored on the host. It describes how logical
artifacts are generated at a lower level, both directly through
hypervisor APIs and through an OS-specific agent installed on the
virtual machine. It concludes by describing the generation of physical
artifacts through direct filesystem and disk image editing.

\section{Overview}\label{overview}

At the core of every synthesizer is the ability to place or otherwise
generate forensic artifacts. Each of the synthesizers described in
\autoref{analysis-of-existing-synthesizers} takes one of three approaches to artifact generation, as
partly described by Scanlon et al.
\cite{scanlonEviPlantEfficientDigital2017}:

\begin{itemize}
\tightlist
\item
  \textbf{Physical}: No virtualization of software or hardware ever
  occurs; data is written directly to the target medium, such as a disk
  image or virtual hard drive.
\item
  \textbf{Agentless logical}: The synthesizer interacts with a live VM
  to generate artifacts. Interaction is achieved without the need for
  custom software to be installed on the VM, and is typically achieved
  through the hypervisor itself or a remote management tool native to
  the virtualized operating system.
\item
  \textbf{Agent-based logical:} The synthesizer interacts with a
  dedicated client (agent) on a live VM to carry out actions. The VM
  must have the agent installed before any interaction can occur.
\end{itemize}

These three approaches are not mutually exclusive within a single
synthesizer, though it is the case that most prior synthesizers have
used exactly one approach to generate artifacts. The table below denotes
the approaches used by each of the synthesizers previously discussed.
Where source code is not available, a best effort is made to identify
the approach used by a particular synthesizer based on its published
paper, if one exists; otherwise, the entire row contains question marks.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1799}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.4604}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1942}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.1655}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Synthesizer
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Physical
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Agentless
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Agent-based
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{FALCON}, 2005 & ? & ? & ? \\
\textbf{CYDEST}, 2008 & ? & ? & ? \\
\textbf{Forensig2}, 2009 & Yes (mounting filesystem, writing directly to
disk using offset) & Yes (over SSH only) & No \\
\textbf{D-FET}, 2011 & Yes (mounting filesystem, writing directly to
disk using offset) & No & No \\
\textbf{SFX}, 2012 & Yes (mounting filesystem) & No & No \\
\textbf{Yannikos et al.}, 2014 & ? & ? & ? \\
\textbf{ForGeOSI}, 2014 & No & Yes (hypervisor interfaces) & No \\
\textbf{ForGe}, 2015 & Yes (emulates complete filesystem in memory as
Python objects) & No & No \\
\textbf{ForGen}, 2016 & No & No & No \\
\textbf{EviPlant}, 2017 & Yes (write directly to image file using
offset) & No & Yes (unknown mechanism) \\
\textbf{VMPOP}, 2018 & No & Yes (hypervisor interfaces) & No \\
\textbf{hystck}, 2020 & No & No & Yes (Python agent) \\
\textbf{TraceGen}, 2021 & No & No & Yes (unknown mechanism) \\
\textbf{ForTrace}, 2022 & No & No & Yes (Python agent) \\
\end{longtable}

There are advantages and disadvantages to each approach, in addition to
requiring distinct implementation techniques for each. The remainder of
this chapter analyzes these each of these three approaches in greater
detail, describing the implementation details of prior synthesizers and
comparing them to those of AKF's.

\section{Agentless artifact
generation}\label{agentless-artifact-generation}

\textbf{Agentless artifact creation} describes one of two general
techniques. The first is emulating ``normal'' human interaction by
leveraging human interfaces -- such as the monitor, keyboard, and mouse
-- to manipulate a GUI-based operating system directly. The second is
using (remote) management utilities included with the operating system,
typically an interactive shell.

AKF allows users to perform agentless artifact creation through a
dedicated hypervisor-agnostic interface, for which a concrete
implementation using VirtualBox is provided. The VirtualBox-specific
functionality is largely derived from \textbf{ForGeOSI}, which uses the
VirtualBox SDK and a Python implementation of the VirtualBox COM API to
carry out the vast majority of its tasks. This was adapted in
\textbf{VMPOP}, which also introduced the notion of a generic hypervisor
interface that allows for synthesizer routines to use arbitrary
hypervisors so long as required functionality is implemented.

It is worth noting that throughout this thesis, the use of
hypervisor-specific guest software, such as VirtualBox Guest Additions
and VMWare Tools, is treated as an agentless approach. Although this
inherently requires the installation of ``unusual'' software on the
virtual machine, it is sufficiently distinct from typical user software
that it is unlikely to generate artifacts that complicate a scenario. In
many cases, artifacts generated by hypervisor guest software can be
noted, isolated, and ignored.

\subsection{Human interfaces}\label{human-interfaces}

The use of human input devices, namely the keyboard and mouse, is the
primary approach taken by \textbf{VMPOP} and \textbf{ForGeOSI} to
generate artifacts. For example, VMPOP leverages the VirtualBox API to
interact with supported applications. More precisely, it typically uses
a sequence of keyboard strokes to focus and interact with UI elements,
such as clearing User Account Control dialogs on Windows or starting
applications with Win+R.

Strictly speaking, this approach most accurately reflects how a real
human would interact with a machine. In many cases, this greatly reduces
the \textbf{Synthesis pollution} that occurs. However, this tends to
lead to verbose scripts that are only capable of performing very
specific actions. The keyboard and mouse actions required to fulfill a
particular action can change significantly between versions of the same
application, versions of the same operating system, and varying screen
sizes.

For example, VMPOP handles User Account Control (UAC) prompts on
machines prior to Windows 7 by sending an Alt+Tab keystroke to the
machine, clicking the mouse at the center of the screen to focus the UAC
prompt, and then sending Alt+C to accept the prompt. On machines running
Windows 7 or later, this is instead achieved by focusing the UAC prompt
with the mouse and then sending Alt+Y to accept the prompt.

Similarly, VMPOP interacts with browsers exclusively through the use of
keyboard shortcuts. VMPOP supports simple interactions with a small
number of websites, such as logging into Microsoft or Google, but is
dependent on the form elements remaining the same over time. That is,
VMPOP does not inspect the contents of the current webpage to perform
actions, and cannot react to design changes. If there are new focusable
elements on the page, the same keystroke sequence may no longer achieve
the desired effect. This lack of runtime logic, which amounts to
operating the machine with the monitor off, leads to brittle scripts
that can be tedious to fix.

AKF's VirtualBox implementation allows the user to issue mouse events at
absolute coordinates, though it does not support general mouse movement,
such as clicking while dragging. It also allows the user to issue a
sequence of press-and-release events while holding down specified keys.
While this is theoretically sufficient to emulate nearly all actions
that a human would normally perform with a mouse and keyboard, the AKF
agent (as described in \autoref{the-akf-agent}) also exposes a more flexible mouse and keyboard automation API.

\textbf{TraceGen} notes that the ideal future is to use some combination
of computer vision and AI to generalize user actions. Currently,
completing the action of ``performing a Google Search in Microsoft
Edge'' would likely be achieved through a sequence of predefined
keystrokes. While this is explored in greater detail in \autoref{open-ended-automation-with-ai}, recent advancements
in LLMs may make it possible to allow a machine to perform arbitrarily
complex tasks on a GUI-based operating system using natural language -
an approach that can be integrated into AKF in the future.

\subsection{Management utilities}\label{management-utilities}

The alternative is to use existing management utilities, typically a
shell, which are native to the virtualized operating system and are
capable of carrying out commands. This can be further broken down into
two categories: local management utilities, such as Bash and PowerShell,
and remote management utilities, such as SSH and WinRM.

Local management utilities typically refer to scripting languages that
are available as part of the operating system, and can be used to manage
most or all operating system resources. For example, PowerShell allows
users to modify registry keys, invoke applications, create users, and
more. Similarly, any standard Linux terminal program, such as Bash or
Zsh, can be used to install packages and run a variety of command-line
applications. This is typically invoked by either opening and focusing a
terminal window (such as the Win+R shortcut on Windows), or by directly
executing scripts through the hypervisor guest additions.

In particular, \textbf{VMPOP} makes heavy use of local management
utilities; it implements much of its functionality through a collection
of PowerShell and batch scripts. For example, VMPOP allows users to
focus a window by process ID, process name, or window title. This is
achieved by getting a PowerShell handle to the process using
\passthrough{\lstinline!Get-Process!}, using
\passthrough{\lstinline!Add-Type!} to add a local C\# function that is
capable of sending keyboard events through
\passthrough{\lstinline!user32.dll!}, and then holding the Alt key while
using \passthrough{\lstinline!AppActivate!} to focus the window and
bring it to the foreground. VMPOP leverages similar scripts for
launching and terminating processes by name, uninstalling programs,
creating a Windows restore point, and more.

Similarly, remote management utilities allow a remote device to invoke
local management utilities, typically over an SSH server or WinRM. In
most architectures, the use of remote management utilities requires that
the host and guest machines can communicate with each other, which is
typically achieved through a NAT or host-only interface managed by the
hypervisor. This approach is taken by \textbf{Forensig2}, which
exclusively connects to a running SSH server to carry out user actions.

The use of remote management utilities to automate actions typically
undertaken by users is not uncommon, especially among general
infrastructure as code solutions. For example, the open-source Ansible
framework simplifies the configuration of Windows and Linux devices to
simple, YAML-based files called ``playbooks''. These playbooks typically
contain a sequence of high-level tasks to perform, such as installing
packages, managing local user accounts, running scripts, and more. (This
high-level scripting language is the inspiration for AKF's high-level
scripting language, described in \autoref{declarative-usage}.)

It is worth noting that although this approach does not require
installing new software on the machine, most operating systems perform
some degree of logging when their management utilities are invoked. For
example, the \passthrough{\lstinline!sshd!} daemon logs connections
regardless of which network interface is used, which may make it
difficult to separate or remove logs not related to SSH connections as
part of the scenario itself. Similarly, the invocation of PowerShell
scripts causes event logs to be generated, which can be particularly
noisy if Script Block Logging (which logs the execution and content of
all PowerShell scripts) is active.

AKF inherits the capabilities of VMPOP, allowing users to leverage the
VirtualBox API and Guest Additions to execute arbitrary processes. For
example, users can invoke scripts to automate OS-specific configuration,
such as the use of PowerShell to set registry keys. However, AKF does
not currently implement any agentless OS- or application-specific
functionality through this method; many of the actions and artifacts
previously generated through OS-specific scripts in \textbf{VMPOP} and
other synthesizers can be implemented with greater flexibility through
the agent instead. How this flexibility is achieved, as well as why it
is the preferred method for implementing application-specific
functionality, is described in the following section.

\section{Agent-based artifact
generation}\label{agent-based-artifact-generation}

\textbf{Agent-based artifact creation} involves the use of a dedicated
program on the VM that serves as an interface between the host machine
and the guest machine. This program runs commands natively on the
virtual machine on behalf of the host machine, typically accepting
commands over a dedicated network interface. This allows for greater
flexibility and more complex actions to be taken. In particular, it
allows for existing automation frameworks such as Playwright and
PyAutoGUI to be used in implementing application-specific functionality.
However, this approach often leads to \textbf{Synthesis
pollution\textbar synthesis pollution}; besides the presence of software
that would never exist on a typical user's machine, agents often do not
interact with applications the same way that a human would. In this
case, the synthesizer should document known pollution that can be
ignored for educational and testing purposes.

\subsection{Analysis of the ForTrace
agent}\label{analysis-of-the-fortrace-agent}

Agent-based artifact creation is the approach taken by
\textbf{hystck}/\textbf{ForTrace}, which refers to its agent as an
``interaction manager''. Because ForTrace provides the largest set of
functionality for its agents, and is by far the most mature synthesizer,
AKF's agents borrow heavily from ForTrace's approach. We briefly
describe ForTrace's implementation of agents here, with a more detailed
analysis in \autoref{comparison-of-fortrace-and-akf-agents}.

ForTrace agents are simply an entire copy of the ForTrace codebase
copied over to the virtual machine -- that is, the ``agent'' and
``server'' share the same codebase, but have different entrypoints and
use different parts of the ForTrace library. Communication between the
agent and the server occurs over a dedicated TCP socket using a simple
ASCII-based protocol; to execute commands, the server sends a
space-delimited string containing the application, function, and
arguments to run. Results are similarly passed back by the agent as
structured ASCII messages. These messages are typically sent over a
dedicated ``management'' network interface to exclude them from network
captures for other relevant activities.

Application-specific functionality for agents is organized into
individual modules (files), where each file includes a set of commands
or actions for a single application. These files contain both
server-side code (the logic for \emph{constructing} the ASCII message)
and agent-side code (the logic for \emph{interpreting} the message and
\emph{executing} the command) as a set of classes with a common prefix,
such as `\textless module\_name

There are two major implementation details that should be observed here.
First is that commands are sent through a simple string-based protocol.
This simplicity makes it easy to debug issues that arise as a result of
the protocol itself, but is relatively inflexible. In particular, it is
difficult to send complex Python objects as arguments, since they often
cannot easily be serialized to a string without loss of information. An
example relevant to AKF is passing Playwright browser objects with state
that is difficult to reconstruct using strings alone.

The second implementation detail involves the discovery and execution of
commands indicated by the protocol. As shown previously, ForTrace makes
heavy use of Python's runtime introspection to discover the correct
modules and functions to call based on the contents of a command string.
While this is a valid approach, it is more complex (and difficult to
follow) than the approach taken by AKF.

\subsection{The AKF agent}\label{the-akf-agent}

AKF borrows heavily from the Python agent-based approach of
\textbf{ForTrace}, but improves on its architecture in several respects.
Low-level improvements over ForTrace's agent architecture are described
in more detail in \autoref{comparison-of-fortrace-and-akf-agents}.

Perhaps the most significant change is the use of RPyC for
communication, a library for symmetric remote procedure calls
\cite{TomerfilibaorgRpyc2025}. Although the RPyC protocol is
symmetric, it is often used in typical client-server architectures to
allow clients to manipulate remote Python objects as if they were local
objects, as well as invoke remote (server) functions using local
(client) parameters. By delegating the serialization and deserialization
of complex objects to RPyC, this allows us to perform complex operations
that would have been difficult to implement with the simple string-based
protocol of ForTrace.

In ``new-style'' RPyC, this is achieved by running a \emph{service} on
the device where remote operations should be performed; this service
simply listens on an ephemeral TCP port. Services expose a set of
functions and attributes that may be accessed remotely by an RPyC
client. Application specific-functionality is broken up into individual
RPyC ``subservices'', which are created on demand. The agent's main loop
is itself a ``root'' RPyC service that is responsible for creating and
destroying these subservices upon request; all subservices are known to
the root service at program start, eliminating the need to perform
runtime introspection to find application-specific modules. These
subservices are analogous to the agent-side code of individual ForTrace
modules, interpreting arguments and executing commands on behalf of the
host machine.

Clients are largely decoupled from the server's implementation of
individual services; they do not call or import RPyC service functions
directly. Because the exposed functions (as well as their signatures)
and attributes cannot be inferred from the raw RPyC connection alone,
clients can implement a typed, concrete API to construct RPyC calls.
This allows type checking and autocompletion tools to continue
functioning in development environments. This simultaneously abstracts
the raw RPyC call (and the existence of an RPyC connection) away from
the user, while also providing the signatures of remote functions where
there would otherwise be none. This is analogous to the client-side code
of individual ForTrace modules, constructing messages to the agent to
execute commands.

Additionally, because the service and the API to an RPyC service are
separable, this allows us to break the API (which is used in scenario
scripts) and the agent logic itself into two separate libraries. This
has two advantages -- it may slightly reduce the size of the agent when
installed onto the virtual machine, and it also makes it significantly
easier to build and generate standalone executables using tools like
PyInstaller \cite{PyinstallerPyinstaller2025}. Unlike ForTrace,
whose agent installation process requires a batch script installing
various libraries and Python through the Chocolatey package manager,
AKF's agent requires only that a single executable is copied over and
configured to run on startup (in addition to setting relevant firewall
rules).

From an implementation and usability perspective, this design provides
three major improvements over the ForTrace protocol. First, the routing
of functions is wholly delegated to RPyC. Instead of manually
constructing a message with the function name and its associated
parameters (as strings) over the network, the process of serializing
parameters and routing these parameters to the correct underlying
function call is abstracted away by RPyC.

Second, this allows us to pass and return arbitrarily complex objects
(for which we do not have to write the serialization and deserialization
logic). When passing complex objects from the agent to the server or
vice versa, a reference to the object is sent over the network and
wrapped by a \emph{proxy object}, which behaves like the original object
\cite{TheoryOperationRPyC}. Importantly, it is not necessary to
distinguish between local and remote/proxy objects of the same type when
writing code, which eliminates the extra complexity of using proxies.

Finally, the ability to interact with complex remote objects allows us
to significantly reduce the actual code that is written as part of the
API exposed to the host. For example, there is no need to implement a
wrapper for every method available as part of a Playwright page object;
instead, a reference to the Playwright object \emph{running on the
virtual machine} can be given to the host machine. Instead of writing
individual methods for opening pages, navigating to specific elements,
and so on, we can simply use the methods that already exist in the
Playwright object -- any local calls on the host's proxy object will
lead to remote outcomes on the host, as desired. This, of course, does
not preclude the ability to write convenience methods for complex
actions using that Playwright object.

The list of applications supported by AKF's agent, as well as
implementation-specific details, is described in the table below.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.0376}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2019}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.7606}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Module
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Dependencies
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Details
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Chromium & Playwright \cite{MicrosoftPlaywrightpython2025} & Allows
arbitrary webpages to be visited on Chrome and Edge, as well as perform
complex actions such as completing forms and clicking links based on
HTML selectors \\
\end{longtable}

The generic hypervisor interface is used to support interaction with the
agent. To avoid polluting network captures with agent-related packets,
virtual machines are expected to use a NAT adapter for Internet
communications and a ``maintenance'' host-only adapter for
agent-specific communications. In turn, hypervisor-specific
implementations must expose the ability to discover the IP address of
the host-only adapter. This allows AKF scripts to communicate with the
root RPyC service and any subservices.

\section{Physical artifact
generation}\label{physical-artifact-generation}

\subsection{Overview}\label{overview-1}

\textbf{Physical artifact creation} encompasses any technique in which
virtualization of an operating system is not used to generate artifacts.
This gives the synthesizer the ability to bypass operating system or
driver behavior, which may lead to undesired non-deterministic behavior
for certain actions. This is also referred to as \emph{simulation}, in
contrast with \emph{virtualization}.

For example, a scenario developer may want to guarantee that a
particular deleted file is partially overwritten by another file,
ensuring that the deleted file is recoverable from the slack space of
the newly placed file. However, it is extremely difficult to force the
reuse of the same physical regions from a userspace application.
Operating systems do not expose low-level filesystem functionality to
applications; furthermore, they are still subject to hardware drivers
that regularly rearrange physical space, such as those that engage in
wear leveling.

While it may be possible to predict the outcome of these wear leveling
techniques as demonstrated by Neyaz et al., this is far from the
determinism that may be necessary of research and tool validation
\cite{neyazForensicAnalysisWear2018}. Additionally, background
programs and services may perform file operations that are difficult to
predict. Furthermore, there is also hardware-related non-determinism,
such that caused by faulty hardware or cosmic rays. (It should be noted
that there exists ongoing work in building deterministic platforms, such
as the deterministic hypervisor developed by Antithesis
\cite{pshenichkinYouThinkYou2024}, though outside the scope of this
thesis.) In turn, it is sometimes necessary to bypass the operating
system to reliably place data on a disk.

There are two primary strategies used to achieve physical artifact
creation -- mounting the filesystem and performing direct read/write
operations, and bypassing the filesystem entirely and performing direct
writes to the underlying disk image. Filesystem mounting is leveraged by
several synthesizers, including \textbf{Forensig2} and \textbf{SFX}. For
both of these synthesizers, physical artifact creation is achieved by
simply allowing the user to specify a file to copy from the host machine
to a specific filepath on the disk image.

Other synthesizers, such as \textbf{ForGe} and \textbf{EviPlant},
instead write to the disk image directly. \textbf{ForGe} maintains its
own virtual representation of supported filesystems, using its own data
structures to represent a FAT32/NTFS filesystem. This allows it to
quickly identify and place data in unallocated or slack space. In
contrast, synthesizers such as \textbf{EviPlant} simply allow the user
to provide an offset into the disk image where arbitrary data will be
written to. Both of these techniques involve ``direct writes'', with the
only distinction being filesystem awareness.

It is theoretically possible to construct full forensic datasets through
simulation alone. If a user knows exactly what artifacts are generated
through the execution of an application and \emph{how} these artifacts
are generated, it may be possible to artificially generate these
artifacts as if the application had actually been running. An example
may be editing the history database of a particular browser contained on
a filesystem by mounting it, parsing it, and then adding new entries;
this would make it appear as if the machine was used to browse to these
websites without requiring virtualization.

However, this technique is difficult to implement in practice. For
example, a scenario developer would need to ensure that the artifact is
consistent with other artifacts on disk, since typical Windows
application usage may generate relevant artifacts in prefetch files and
jump lists. Additionally, physical artifact creation does not lend
itself well to generating network captures and volatile memory. Because
no operating system is virtualized, it is naturally the case that there
are no applications making network requests or using volatile memory as
part of execution. Building a network capture or volatile memory dump
from scratch is difficult, at best.

It is important to note that the majority of the artifacts generated by
physical techniques can still be created non-deterministically through
logical means. For example, a user could create files of interest,
delete them, and then write new files to disk through a virtual machine,
possibly leading to the same outcome as simply writing the files
directly to the filesystem without virtualization. However, there is no
strict guarantee in a virtual machine that particular files have
overwritten the space deallocated as a result of deleting the files from
the filesystem. In some cases, this may be acceptable, such as a
scenario in which an instructor simply wants to demonstrate examples of
what might occur when deleting a file.

\subsection{AKF implementation}\label{akf-implementation}

As described in the prior section, there are three techniques for
physical artifact planting implemented by prior synthesizers. These are:

\begin{itemize}
\tightlist
\item
  \textbf{Filesystem mounting}, as done by \textbf{Forensig2} and
  \textbf{SFX}, in which the filesystem is simply mounted to the host
  and edited directly;
\item
  \textbf{Filesystem-independent direct editing}, as done by
  \textbf{EviPlant}, in which edits to specific physical addresses on
  the disk image are made without any parsing or knowledge of the
  underlying filesystem; and
\item
  \textbf{Filesystem-aware direct editing}, as done by \textbf{ForGe}
  and \textbf{EviPlant}, in which filesystem data structures are parsed
  to determine the physical address(es) of the disk image to write to.
  (How EviPlant achieves this is not known.)
\end{itemize}

AKF supports all three to varying degrees, with significant improvements
over prior synthesizers specifically in filesystem-aware editing.

With respect to filesystem mounting, AKF allows users to construct new
ISO files from an existing host directory using the
\passthrough{\lstinline!pycdlib!} library
\cite{lalancetteClalancettePycdlib2025}. These ISOs can be used as
standalone artifacts (for example, presenting it to students as a disk
image of a removable drive), in addition to mounting them as removable
storage on a running virtual machine. Mounting these ISOs, as well as
setting up a shared network folder between the host and guest machines,
are the simplest means through which scenario developers can transfer
files onto a running virtual machine. AKF does not currently support
writing to arbitrary filesystems or disk image formats at rest, since
these require write capabilities not supported by any well-known
library.

AKF also trivially ``supports'' filesystem-independent direct editing.
Python allows users to open files in binary-mode, at which point the
user can call \passthrough{\lstinline!seek()!} on the file pointer to
advance the pointer to a specific offset in the file. The user can then
call \passthrough{\lstinline!write()!} to overwrite an arbitrary number
of bytes at that position, achieving the same outcome as many other
synthesizers that implement range and offset-based disk image editing.

Although blindly writing to arbitrary positions is not always viable for
correctly placing artifacts on a disk image (for example), it is still a
valid approach when the underlying filesystem already been analyzed
ahead of time and the exact offsets and ranges to write are known. This
may be the approach used by \textbf{EviPlant} to construct and leverage
its evidence packages for students; because filesystem analysis is
costly and requires libraries compared to offset-based editing, it is
easier to perform this analysis at construction time and distribute the
offsets to which data must be written to base images.

In many cases, however, the filesystem may not have been analyzed ahead
of time such that the exact offsets and data needed to place a file on
disk are known. This brings us to filesystem-aware direct editing, which
AKF makes significant improvements in.

ForGe implements its physical artifact generation by implementing NTFS
and FAT32 data structures in Python, allowing it to create a fully
virtual representation of these filesystems (with the assistance of a
custom C program). This provides it with the information necessary to
insert data into known slack and unallocated space in an efficient
manner while keeping the filesystem consistent. While extremely powerful
(and implements a valuable feature not found in any other synthesizer to
date), it is inflexible in two specific aspects:

\begin{itemize}
\tightlist
\item
  ForGe does not provide a generic interface for the filesystems it
  supports. Although both the NTFS and FAT32 wrappers provide the same
  methods with the same signatures, this is not strictly enforced by a
  parent class. This means that the common functionality it supports
  across all filesystems is unclear, as is the functionality that must
  be implemented for new filesystems to be compatible with ForGe.
\item
  ForGe lacks a ``frontend'' to support arbitrary disk types, regardless
  of the underlying filesystem. ForGe does not support multi-partition
  disks, nor does it support common non-raw disk formats such as VHD,
  VMDK, and VDI.
\end{itemize}

Read-only libraries addressing these two issues have been in development
since the introduction of ForGe, but have not been integrated into other
synthesizers to achieve the same write capabilities as ForGe. One such
library is \passthrough{\lstinline!libtsk!} (also known as The Sleuth
Kit), the C++ library that powers the open-source digital forensics
platform Autopsy \cite{SleuthkitSleuthkit2025}.
\passthrough{\lstinline!libtsk!} allows users to navigate and analyze
the low-level contents of a variety of filesystems, including
filesystem-specific attributes and the sequence of disk clusters that
form a file. \passthrough{\lstinline!libtsk!} supports a large variety
of filesystems through a filesystem-agnostic interface, including the
FAT family of filesystems, the ext family, NTFS, and a variety of other
filesystems. It also supports a variety of volume systems (partition
schemes for multi-partition disks), such as GPT and MBR, as well as a
small number of filesystem containers and images.

Python bindings for \passthrough{\lstinline!libtsk!} have existed for at
least 15 years, with the most commonly used library being the
automatically generated \passthrough{\lstinline!pytsk!}
\cite{Py4n6Pytsk2025}. However, \passthrough{\lstinline!pytsk!} has
relatively limited Python documentation and adoption, in addition to
inheriting various known issues and limitations in
\passthrough{\lstinline!libtsk!}, such support for niche filesystems and
partitioning formats. Many of these gaps were gradually covered as part
of the \passthrough{\lstinline!libyal!} project, a collection of DFIR
libraries developed by various contributors - most notably Joachim Metz
from Google. The \passthrough{\lstinline!libyal!} project includes
individual libraries for analyzing formats not supported by
\passthrough{\lstinline!libtsk!} such as the QCOW disk image format used
by QEMU, the Apple File System used by modern Apple devices, and more.

This leads to the use of \passthrough{\lstinline!dfvfs!}, or the Digital
Forensics File System, a Python library that leverages
\passthrough{\lstinline!libtsk!} and multiple libraries from the
\passthrough{\lstinline!libyal!} project to provide a generic interface
for analyzing a variety of disk image formats and filesystems
\cite{Log2timelineDfvfs2025}. (The project is derived from Plaso and
Google Rapid Response, two open-source DFIR tools used in a variety of
contexts.) Much like \passthrough{\lstinline!libtsk!}, it exposes a
variety of low-level filesystem concepts that are common across multiple
filesystems, such as the metadata and individual segments that comprise
a file at a known path on the filesystem. This low-level detail, in
combination with its focus on exposing this detail through a consistent
Python interface, makes it extremely powerful in performing
filesystem-aware direct editing.

AKF uses \passthrough{\lstinline!dfvfs!} to locate the clusters of a
file at a known path in a filesystem, which can then be used to identify
the start of slack space within the file's clusters (as well as
unallocated space in the filesystem). By adding the physical offset of
the cluster within the filesystem to the offset of the filesystem's
partition in the disk image, AKF can write to the exact location of a
file's slack space using the offset-based method described earlier. (It
should be noted that this technique does not work on filesystems with
full disk encryption enabled, as is the default on Windows systems
beginning with the Windows 11 24H2 update.)

This achieves feature parity with ForGe, completely delegating
filesystem and image-specific details to \passthrough{\lstinline!dfvfs!}
and allowing for a filesystem-independent method for locating slack
space. This provides a deterministic method for inserting data within
the slack space of a filesystem, simulating the deallocation of a file
and its partial replacement with a known file. However, this does not
fully simulate the process of deleting a file through a running
operating system and having a new file replace the deallocated clusters;
naturally, this does not generate OS-specific artifacts associated with
deleting and creating files, and fails to generate the filesystem
artifacts that \emph{could} exist with the original file (such as the
``deleted'' file's name in the NTFS master file table). Additional
opportunities to leverage \passthrough{\lstinline!dfvfs!} in other
physical artifact generation techniques exist.

\chapter{Output and validation}\label{chapter-five}

This chapter addresses the mechanisms through which AKF generates and
documents its outputs, depicted in the partial architectural diagram
below.

!\textbf{39.5 - Output and validation 2025-02-08 17.13.11.excalidraw}

This chapter addresses the role of the output and validation library in
providing several services to AKF, including a centralized logging
system, the use of CASE objects, and invoking various commands to
generate and export outputs such as disk images, network captures, and
volatile memory dumps. It also describes high-level reporting and
validation functionalities, both those as part of the framework itself
and options available through external tools.

\section{Overview}\label{overview}

Whether generating artifacts through physical or logical means, these
artifacts must ultimately be exported and documented. In most cases,
this involves generating disk images, volatile memory captures, or
network captures; additionally, specific artifacts, such as browser
artifacts, may be selectively copied from a filesystem. Disk, memory,
and network captures will be referred to as ``core outputs'' for
brevity, as well as to distinguish them from ``standalone'' artifacts;
both of these are simply ``outputs''.

In either case, the contents and details of these artifacts, as well as
the means through which they were generated, should be documented in
some format. The term ``ground truth'' describes the contents of a
single specific dataset in full. That is, it ideally provides a
reference for every artifact that can be discovered within a dataset, as
well as every action taken to plant those artifacts. This metadata, in
addition to core outputs and standalone artifacts, comprises a forensic
dataset.

From an educational perspective, the ground truth represents an ``answer
key'' to the dataset; it details every artifact of interest that an
analyst could be expected to discover. For research, it allows for
well-labeled datasets that can be used for tool development, validation,
and testing. Importantly, this should be generated in a manner
independent of the input script used to construct the scenario, allowing
\emph{all} artifacts to be documented, including those not explicitly
declared or deemed important by the scenario creator.

The remainder of this chapter describes the mechanisms through which
outputs are exported from the synthesizer and the mechanisms that allow
for real-time logging and documentation.

\section{Core outputs}\label{core-outputs}

In the same way that artifacts can be generated through logical and
physical means, outputs can also be generated through logical and
physical means. Some of these are analogous to techniques used by
real-world investigators to extract forensically sound evidence that is
valid in a court of law, while others are less suitable in a court
setting but still have valid use cases.

Logical output generation refers to any technique in which software is
used inside a running virtual machine to generate these outputs. This
includes using software such as FTK Imager, booted from a removable
drive, to capture the volatile memory of a device or construct a
\emph{logical} disk image. Similarly, network captures can be done by
simply running Wireshark on the target device and network interface, and
individual artifacts can be copied off of the device by hand and sent to
a network or removable drive.

Physical output generation refers to techniques in which the operating
system is unaware of the technique being used, and therefore leave few
or no traces in the resulting outputs. In practice, this involves using
tools such as hardware write blockers to extract complete disk images
(which also contain standalone artifacts), as well as using network taps
and sniffers (or another traffic mirroring solution) to capture traffic
over a particular interface. Although difficult, it is also possible to
perform a physical extraction of RAM by performing a ``cold boot
attack'', in which the RAM sticks are cooled to low temperatures before
removing them from a running machine, slowing the process of memory
decay as a result of the DRAM cells being unpowered
\cite{yitbarekColdBootAttacks2017}.

AKF directly supports physical output generation for all three core
outputs, and indirectly supports logical output generation for core
outputs and standalone artifacts. Physical output generation for virtual
machines is generally achieved through direct interaction with the
hypervisor itself.

\begin{itemize}
\tightlist
\item
  For disk images, the output can simply be the virtual hard drive used
  by the hypervisor on the host machine (such as VDIs for VirtualBox).
  If an ``actual'' disk image is desired, the command-line tool
  VBoxManage supports converting various virtual drive formats to raw
  disk images using the \passthrough{\lstinline!vboxmanage clonemedium!}
  command, which also expands the compressed virtual drive to the
  ``declared'' size of the drive as seen by the operating system.\\
\item
  For network captures, VirtualBox allows the user to enable network
  tracing over multiple interfaces, dumping network traffic as a .pcap
  file on the host machine. This can be enabled or disabled at any time
  without affecting network connectivity or the state of the virtual
  machine.\\
\item
  For volatile memory dumps, VBoxManage provides the command `vboxmanage
  debugvm \textless machine\_name
\end{itemize}

In most cases, these physical output options are sufficient to generate
suitable datasets. If only specific files are desired, the existing file
transfer utilities provided by AKF can be used to extract standalone
artifacts. Users can also manually perform the logical techniques
described above (such as running Wireshark or installing FTK Imager)
through a variety of means, such as pausing an AKF script until
instructed by the user to continue; during this time, a user can
manually run FTK imager to extract the contents of RAM, as an example.

\section{Metadata and ground
truth}\label{metadata-and-ground-truth}

\subsection{Overview}\label{overview-1}

There exists a gap in the ability of instructors and researchers to
perform bulk searches for specific forensic artifacts in public
datasets. For example, the NIST CFReDS repository
\cite{nationalinstituteofstandardsandtechnologyCFReDSPortal}, one of
the largest listings of forensic datasets, does not have a unified
standard for describing uploaded images. While it is possible for users
to search by keywords and human-applied tags, these are not presented in
a standardized format.

For most datasets, an analyst must read through a PDF answer key (if one
exists) or analyze the image themselves to determine if a particular
artifact is present. Such formats are not immediately machine-readable,
and are therefore difficult to use in bulk searches. Additionally, the
content of human-made reports may be limited to what the author believes
is significant, even if other artifacts of interest are present in the
image. In turn, it may be difficult to quickly determine if a dataset is
useful in demonstrating a particular technique to students, or in
validating a specific feature of a newly-developed tool.

A rigid, well-defined format for ground truth is invaluable to
researchers engaging in tool validation and development. It is easy to
write an automated converter for well-structured data into natural
language; it is likely more difficult to perform the reverse operation.
Perhaps the lack of labeled forensic datasets on major repositories may
be attributable to the lack of a need for one; Grajeda et
al.~demonstrated that few scenarios are shared between researchers to
begin with, so there is rarely a need to label them for general-purpose
usage. AKF has the opportunity to solve this issue by allowing for the
mass production of labeled datasets, adopting a single major standard.

Many forensic analysis tools support exporting case data in both
proprietary and language-agnostic formats. For example, Cellebrite's
UFED supports exporting to UFDR and XML files, Magnet Axiom supports
exporting to XML, and Autopsy supports a variety of formats including
Excel, STIX, and HTML. Each of these vary in structure and format, and
do not necessarily contain equivalent information for the same analyzed
disk image using default settings. This is the primary challenge with
using an existing format, especially a proprietary format that is
subject to vendor changes; certain details may be missing, and may
change at an arbitrary point in time.

There has been extensive work in other fields towards developing a
structured ontology that describes relationships and low-level details.
This includes the EVIDENCE project for criminal justice and the
Structured Threat Information Expression (STIX) format for conveying
cyber threat intelligence
\cite{caseyLeveragingCybOXStandardize2015}. For example, STIX
provides a standard set of objects that allows organizations to describe
observed attacker techniques and associate them with specific pieces of
malware, attack campaigns, or threat actors.

However, there is limited work that aims to document the contents of a
forensic scenario (disk images and related metadata) in a vendor-neutral
manner. Besides their lack of adoption, Casey et al.~found that existing
formats lacked features such as parent-child relationships, user
actions, and non-technical case information such as a chain of custody.
In response, the same authors introduced the Digital Forensic Analysis
eXpression, or DFAX, a language extending CybOX (the predecessor to
STIX) for use in the digital forensics community. DFAX eventually
evolved to become the Cyber-investigation Analysis Standard Expression
(CASE) \cite{caseyAdvancingCoordinatedCyberinvestigations2017},
which we leverage as \emph{fastlabel}'s standard output format. CASE is
perhaps the most comprehensive and actively supported ontology available
for digital forensics; contributors include NIST with support from the
Linux Foundation.

\subsection{CASE and Python
bindings}\label{case-and-python-bindings}

CASE is a vendor-neutral format designed to document both technical and
non-technical information about a digital forensics case
\cite{caseyAdvancingCoordinatedCyberinvestigations2017}. It aims to
cover as many OS-specific and application-specific artifacts as
possible, while still providing the flexibility to describe artifacts
from uncommon applications. In theory, data exported from any major
vendor, such as Cellebrite, Magnet, or FTK, can be converted into a
valid CASE file. CASE is an extension of the Unified Cyber Ontology, or
UCO, which simply provides basic objects that are not specific to
digital forensics (such as applications or users). Consistent with the
CASE project's documentation, CASE and CASE/UCO are one and the same.

CASE is built on the Resource Description Framework (RDF), a model for
describing information using relationships. Two objects are linked using
relationships, each of which is described as a ``triple''. This pattern
allows for directed, labeled graphs to be expressed using RDF. The
ontology of CASE objects is defined using the Terse RDF Triple Language,
or Turtle, which allows these triples to be written in a simple text
format. In many ways, the Turtle definitions can be seen as the class
definitions for CASE objects; instances of these CASE objects can be
expressed in JSON-LD, an extension of JSON for linked data. A collection
of CASE objects is known as a bundle; applications can add instantiated
CASE objects to a bundle as needed.

Because the CASE format itself is language-agnostic, it is necessary to
write language-specific libraries that allow for instantiating CASE
objects. As of writing, the CASE project provides Python bindings for
UCO/CASE version 1.4 \cite{CaseworkCASEMappingPython}, in which each
unique object is represented as a Python class, which can be
instantiated to produce individual objects. However, this library has
several limitations due to its design. For example, CASE objects are
internally represented as dictionary of strings, rather than a set of
instance variables. While this makes it easier to serialize these
objects to JSON-LD dictionaries, it also makes it extremely difficult to
work with these objects after they have been instantiated.

It is also worth noting that each of the objects in the CASE ontology
appear to have been manually translated to their corresponding Python
class definitions. This is slow and time-consuming, especially given the
context that a significant overhaul of UCO/CASE to version 2.0 is
underway, with new object definitions; there appears to be no active
effort to update the 1.4 bindings to 2.0.

\begin{itemize}
\tightlist
\item[$\square$]
  \#task Citation to the 2.0 branch of CASE/UCO, not the original branch
\end{itemize}

AKF leverages (and contributes, though not the direct result of this
thesis) Pydantic-based bindings for CASE. The foundation for this system
will be the Pydantic library for Python, which allows developers to
quickly define classes (referred to as ``Pydantic models'', or simply
``models'') with built-in schema validation and serialization based on
Python type hints \cite{colvinPydantic2024}. More broadly, it allows
us to simplify the declaration of individual objects while providing
runtime type validation and automatic casting.

Examples of CASE-related definitions, as well as a detailed comparison
of AKF's bindings compared to the existing CASE bindings, can be found
in \autoref{case-python-bindings}. One notable
example from this section is the simplification of a CASE object
declaration from 43 lines in the existing CASE bindings to only three
lines in AKF. These simple declarations are largely possible because the
conversion of instance variables to valid JSON-LD keys is deferred until
serialization, rather than converting them to dictionaries immediately
upon instantiation. This design choice allows us to centralize the
serialization logic in a single parent class, which all CASE objects
inherit from. In exchange for slightly increasing the complexity of
converting \passthrough{\lstinline!numberOfLaunches!} to a dictionary
with the correct key name, we can massively simplify the logic for
declaring CASE objects.

A script is provided with AKF's CASE bindings to automatically parse the
RDF files and convert them to valid Pydantic models. It automatically
converts XSD datatypes to their native Python types (or a custom wrapper
type if a native type does not exist), correctly inherits classes, and
automatically generates docstrings and Pydantic fields as applicable.
Additionally, the script also topologically sorts dependencies in the
same file; the parent class of an RDF object may be declared
\emph{after} its child class, which is disallowed in Python. This
greatly simplifies the process of maintaining Python bindings for
UCO/CASE, as well as the overall design of the library for future needs.

\subsection{CASE integration in
AKF}\label{case-integration-in-akf}

With these Python bindings, we can integrate them throughout
artifact-generating libraries in AKF. There are two options for
generating and attaching CASE artifacts to a scenario - during scenario
generation, and after scenario generation.

Various functions and classes throughout the AKF core libraries and
agent API accept an optional CASE bundle when invoked or instantiated.
As users call CASE-compatible functions, these functions can
automatically add CASE objects that correspond to the artifacts
generated as part of their execution. For example, if the agent
subservice API for automating Chromium browser actions is instantiated
with a CASE bundle, navigating to a page using the API will
automatically generate a CASE object describing the page visit and add
it to the bundle. This process can occur entirely within the host,
allowing CASE-related logic to remain out of the agent where desirable.

However, recall that the use of RPyC for agent communication allows AKF
to pass complex objects between the agent and the host machine. This
includes CASE bundles and objects, as well. For example, suppose that a
CASE object must be made for a file downloaded from the internet that
frequently changes in location, size, and content. This can only be
accurately constructed using some form of agent-side analysis as the
action is taking place. Once constructed, the object can be returned to
the host machine to append to a larger CASE bundle.

Some CASE ontology objects, such as those describing Windows prefetch
files, are best constructed at the time disk images and other core
outputs are created. For example, it is possible to create CASE prefetch
objects during the execution of a scenario. However, these objects are
likely to become outdated if their corresponding applications are run
again later in the scenario, thus changing the content of the prefetch
files and making the existing CASE objects inaccurate.

In turn, to make the generation of such objects more efficient and
accurate, certain CASE objects must be discovered ``after the fact'';
that is, they cannot be automatically generated as part of AKF
automation routines. This can be achieved by analyzing core outputs
(such as disk images) using independent tooling. This includes
general-purpose DFIR tools that support CASE objects (such as Autopsy),
as well as tools with an explicit focus on constructing CASE objects
(such as those that depend on the official or AKF Python bindings for
CASE).

However, ``after the fact'' analysis can also be achieved on live
virtual machines using existing AKF design patterns. In particular, it
is possible to write RPyC subservices whose sole purpose is to generate
CASE objects. For example, the agent could be instructed to collect the
contents of prefetch files immediately after a disk image is created,
allowing it to construct CASE prefetch objects that are reflective of
the disk image without the need for independent tooling. CASE-oriented
behavior can also be implemented in existing subservices as well, such
as including a function in the Chromium subservice dedicated to
identifying and creating CASE objects for all Chrome/Edge-related
artifacts.

The flexibility of these two approaches -- largely enabled by their deep
integration into AKF -- makes it possible to construct CASE objects in a
manner that requires little additional effort by scenario developers. In
many cases, scenario developers do not need to be concerned with
constructing their own CASE objects when using high-level APIs, so long
as an AKF library developer has written support for automatic CASE
object construction. This significantly reduces the need for scenario
developers to construct artifact-specific ground truth through manual
analysis of synthesizer-created outputs.

By extension, this means that the detailed documentation of AKF outputs
is innate to many scenarios constructed using AKF. By lowering the
effort required to document an AKF-generated scenario, this improves the
likelihood that any public AKF scenario can be immediately useful (or
determined to be useful) to researchers and educators. This
significantly contributes to AKF's goal of supporting a larger ecosystem
around its images, as the CASE bundles of many scenarios can be queried
in bulk to identify datasets that might be useful for a specific purpose
without having to download the dataset itself. This information can also
be used to identify and analyze broader trends across scenarios, such as
the frequency of a particular artifact appearing in all Windows
datasets.

While this machine-readable reporting significantly improves the ability
of the forensic community to locate useful datasets, it is also verbose
and not suitable as a human-readable summary. Human-readable reporting
is particularly relevant in a classroom setting, where the distribution
of simplified answer keys to graders and students focusing on key
artifacts is preferable to the exhaustive reporting provided by a CASE
bundle. This leads us into the following section, which briefly
addresses the conversion of AKF-generated metadata into human-readable
reports.

\section{Human readable reporting}\label{human-readable-reporting}

\section{Distribution and community
reproducibility}\label{distribution-and-community-reproducibility}

After generating the scenario itself and any metadata that should be
included with the dataset, there is still the challenge of distributing
this information in a manner that makes it suitable for broad reuse. A
key challenge identified by Grajeda et al.~was the difficulty in
reproducing results in the field of digital forensics. While this is
largely attributed to the \emph{availability} of forensic datasets in
general, it can also be attributed to challenges in \emph{reproducible
constructions} of synthetic datasets.

Before addressing the low-level use of AKF as part of \autoref{chapter-six}, we briefly discuss the infrastructure needed to
support community usage of not only the outputs of AKF scenarios, but
synthetic datasets as a whole. Note that for the remainder of this
section, scenarios and datasets are both implied to be synthetic, as the
principles of reproducibility are less applicable to real-world
datasets.

To make results from a synthetic scenario reproducible, there are four
elements that must be distributed with the scenario:

\begin{itemize}
\tightlist
\item
  Any core outputs or individual artifacts generated from the virtual
  machine.
\item
  Any metadata, ground truth, or other reporting that describes the
  scenario.
\item
  The OS-specific ``base image'' used to create the dataset, typically a
  virtual machine with a newly-installed operating system on which all
  synthesizer actions are performed.
\item
  The precise instructions required to build the scenario from the
  provided base image, whether human- or machine-readable instructions.
\end{itemize}

Forensic datasets have long included core outputs and individual
artifacts, well before the development of AKF and other synthesizers;
there is limited value in a forensic scenario without anything to
analyze. Various forms of ground truth have also long been a part of
forensic datasets in various forms; some educational datasets include
PDF answer keys, while some research datasets have been labeled to
include metadata about the dataset.

However, less common are detailed instructions to build the overall
scenario. Manually constructed datasets rarely describe the actions
taken to build a scenario in detail; for example, the educational
M57-Patents scenario built by Woods et al.
\cite{woodsCreatingRealisticCorpora2011} provides an instructor PDF
with a high-level timeline of actions taken in English. This detail is
sufficient for educational purposes, but is too imprecise to guarantee
that researchers following this timeline will construct the image in the
same manner as intended. As described in \autoref{challenges-in-developing-synthetic-datasets},
non-determinism can be acceptable and even desirable in educational
contexts, but is less desirable for tool validation and research.

Even rarer in manually constructed datasets is the inclusion of the base
image before any actions are performed. This may be attributable to both
copyright concerns and a perception that knowledge of the operating
system alone is sufficient to rebuild the base image; while true that
setting up a base image is straightforward, any need for human
interpretation introduces a source of non-determinism that may not be
desired.

Synthesizers significantly improve on the lack of precise instructions;
their machine-readable scripts both document and execute the precise
instructions needed to reconstruct a scenario. However, this is
dependent on the availability of an OS- and synthesizer-specific base
image; again, many synthesizers expect their users to follow a set of
human-readable instructions to prepare a virtual machine for use.

Where copyright issues are not a concern, synthetic images should aim to
include a complete definition of a virtual machine to be used as the
base image. This may be a complete, hypervisor-specific virtual machine
(archiving and compressing the entirety of the associated virtual
machine folder), a hypervisor-independent virtual appliance (in a format
such as OVF), or another infrastructure-as-code solution to define and
build virtual machines, such as Vagrant.

AKF is designed to provide all four of these elements in every scenario
it creates; elements 1, 2, and 4 are inherent to all synthesizers, while
base images can be provided as Vagrantfiles as described in \autoref{setup-and-basic-usage}. This ensures that
results from AKF-generated scenarios are reproducible from both a
dataset creation and dataset usage perspective.

Although not explored as part of this thesis, the inclusion of all four
of these elements as part of a well-structured, standardized
distribution format could be used to build a distribution platform
similar to CFReDS, but with more powerful discovery and querying
functionality. While the contents of the scenario are largely described
by CASE, it may also be possible to perform queries based on the
contents of Vagrantfiles and AKF scripts. For example, a user may want
to search for all images that use the agent-based Chromium artifact
generation described in \autoref{the-akf-agent}, which can be achieved by searching for the inclusion of the
relevant AKF libraries in the scenario's scripts. This, however, does
not address the challenge of storing and distributing scenarios in an
efficient manner to support such a platform; this is partially discussed
in \autoref{distribution}.

With the reproducibility and value of AKF-generated scenarios
established, we now move to a discussion of how to invoke and leverage
the underlying technologies that provide these benefits.

\chapter{Building scenarios}\label{chapter-six}

This chapter addresses the modules responsible for allowing users to
invoke the framework, both through a standard Python script and through
a high-level YAML file. It also addresses the generative AI modules that
can assist a user in building a scenario, as depicted in the partial
architectural diagram below.

!\textbf{39.6 - Building scenarios 2025-02-08 17.23.40.excalidraw}

At this point, we have provided the implementations for automating
artifact generation in a near-deterministic manner with comprehensive
logging and reporting. However, there is still the challenge of exposing
this functionality in a user-friendly manner. At a high level, there are
currently two ways to define an input to a synthesizer:

\begin{itemize}
\tightlist
\item
  An \textbf{imperative} format, in which the synthesizer is provided
  instructions in an imperative language and the developer must provide
  the exact instructions for the synthesizer to take through some
  exposed API.
\item
  A \textbf{declarative} format, in which the synthesizer is provided a
  file that describes the desired elements of the result, and it is up
  to the synthesizer to execute the instructions necessary to achieve
  the result.
\end{itemize}

Even if the \emph{process} of placing artifacts or performing actions
can be simplified through imperative and declarative inputs, the
challenge of deciding what actions to perform still remains. It is still
largely the responsibility of instructors to provide the actual
background noise to populate the images with. Although the high-level
languages provided by many of these frameworks make it easy to place
files at desired locations or visit websites that are part of a
scenario, these must all be defined and created ahead of time.

This chapter addresses the challenges of creating background noise and
providing simple APIs for complex GUI-driven applications. More
precisely, we address two questions -- how do we invoke AKF's automation
systems, and how does AKF assist a user in building a scenario? Here, we
explore AKF's imperative and declarative syntaxes, as well as the
viability of using large language models (LLMs) to assist in building
individual files and complete scenario descriptions.

\section{Scripting background}\label{scripting-background}

We begin by analyzing how synthesizers accept instructions for execution
-- more precisely, how do users define the sequence of operations that
the synthesizer should take to form the image?

For many of the frameworks created in the last decade, users define
scenarios by using a Python library to interact with the framework,
setting up the virtualized environment and performing high-level actions
on the environment. This abstracts away the underlying calls to the
virtualized environment from the scenario developer. This code-based
approach represents an \emph{imperative} strategy to scenario creation,
where the user describes how the image should be created by describing
the exact order and methodology by which actions should be taken.

It is worth noting that, like many automation frameworks such as
Playwright \cite{MicrosoftPlaywrightpython2025}, the language used
to interact with the synthesizer's API does not need to match the
language used to implement the synthesizer itself (although this is
often the case). For example, Playwright itself is implemented in
TypeScript, and therefore began with a Node.JS API. Today, Playwright
provides APIs in Python, Java, and C\#.

In contrast, custom scenario formats provided by \emph{D-FET}
\cite{williamCloudbasedDigitalForensics2011}, \emph{SFX}
\cite{russellForensicImageDescription2012}, and Yannikos
\cite{yannikosDataCorporaDigital2014} follow a \emph{declarative}
strategy. Here, a custom high-level language describes what the final
state of the image should be, representing a \emph{declarative} strategy
to scenario creation. Instead of importing libraries and writing code,
users state the desired elements of the forensic dataset, allowing the
synthesizer to decide \emph{how} to achieve the desired state. The
specifics of state management and execution are delegated to the
synthesizer.

Consider the following declarative \emph{SFX} code taken from
\cite{russellForensicImageDescription2012}. Here, a Windows 7
partition is created, after which a user called ``Gordon'' uses Firefox
to browse the internet. (It should be noted that this simple scenario
will be reused throughout this chapter.)

\begin{lstlisting}[language=XML]
<disk
    <partition index="p1" hidden="0" size="48M" type="ntfs"
        <base os="windows7x64"/
        <user username = "Gordon"
             <browserhistory browser="firefox"
                 <url link="[http://bbc.co.uk"](http://bbc.co.uk") time="13:14:00 1 Jan 2013"/
             </browserhistory
        </user
    </partition
</disk
\end{lstlisting}

The same might be partially expressed in \emph{ForTrace} as the
following, excluding additional overhead for ground truth generation:

\begin{lstlisting}[language=Python]
import logging

from fortrace.core.vmm import Vmm
from fortrace.utility.logger_helper import create_logger
from fortrace.core.vmm import GuestListener

logger = logging.getLogger(__name__)

if __name__ == "__main__":
    logger = create_logger('fortraceManager', logging.DEBUG)
    macsInUse = []
    guests = []
    
    guestListener = GuestListener(guests, logger)
    virtual_machine_monitor1 = Vmm(macsInUse, guests, logger)
    # boottime expressed as "%Y-%m-%d %H:%M:%S"
    guest = virtual_machine_monitor1.create_guest(guest_name="w-guest01", platform="windows", boottime="2013-01-01 13:14:00")

    browser_obj = guest.application("webBrowserFirefox", {'webBrowser': "firefox"})
    browser_obj.open(url="[http://bbc.co.uk")](http://bbc.co.uk"))
    while browser_obj.is_busy:
        time.sleep(2)
    browser_obj.close()
\end{lstlisting}

Observe that although these two code blocks have the same expressive
power (i.e., they achieve the same overall outcomes), there is a clear
difference in the complexity and length between the two. It is
significantly easier to read and write the declarative XML in the first
code block, as it abstracts away the need to instantiate various
synthesizer objects and call specific methods. By extension, this also
allows for a common declarative syntax to be used across multiple
synthesizers, since low-level details do not need to be exposed as part
of the declarative syntax.

The primary benefit of an imperative approach to generation is its
flexibility; on a Python-based synthesizer, one can simply import
another library to extend the functionality of the base scenario
definition. This flexibility naturally comes at the expense of a greater
learning curve; while true that many digital forensic specialists are
likely to have programming experience, it is far easier to learn a
restricted declarative specification (like XML) than an entire
programming language, which may entail additional setup (such as an IDE,
dependencies, and so on).

When accessibility is preferred over functionality, declarative syntaxes
are often more powerful than imperative syntaxes. Certain scenario
developers, such as classroom instructors, may not need the low-level
control provided by an imperative syntax or a full programming language
such as Python. It also takes time to learn about the functionality
exposed by the synthesizer's library, not to mention learning the
programming language itself. These are the primary motivators behind
supporting well-defined declarative syntaxes.

Of course, low-level control is still important, especially when
external libraries must be used to implement functionality not
inherently exposed by a synthesizer. For this reason, some synthesizers
support both declarative and imperative scripts to generate scenarios;
for example, the Python-based \emph{hystck} and \emph{ForTrace}
frameworks allow users to write YAML scripts to execute actions. This
highlights the fact that both declarative and imperative approaches can
be used simultaneously; in particular, it demonstrates that
declarative-to-imperative translators can be written to support
arbitrary declarative languages, such as those of both SFX and ForTrace.
(While not explored in this thesis, it is also worth noting the
GUI-based interfaces provided by \textbf{Yannikos et al.} and
\textbf{ForGe} for building scenarios.)

AKF supports both an imperative syntax (through its Python API) as well
as a declarative syntax. Unlike prior synthesizers, AKF's declarative
syntax supports both execution and declarative-to-imperative
translation, allowing users to quickly create and modify imperative
scripts from high-level declarative descriptions.

\section{Setup and basic usage}\label{setup-and-basic-usage}

Like many of its predecessors, AKF implements its functionality and
exposes its API in the same language, Python 3. There are numerous
advantages to a Python-based API; besides the relatively low difficulty
of setting up and using Python, its rich ecosystem allows scenarios to
be extended through the use of other libraries from the Python
ecosystem. For example, if a user wanted to conditionally execute
certain parts of a scenario by testing if a particular remote service is
currently online, a user could use the \emph{Requests} library
\cite{Requests31Documentation} to issue an HTTP request out-of-band
before performing the same action in a virtualized environment.

Users must have two foundational technologies for AKF to operate -- an
installation of Python 3.11 or later and a supported hypervisor. AKF
currently only supports VirtualBox, though QEMU/libvirt has also been
used in prior synthesizers. AKF uses
\passthrough{\lstinline!pyproject.toml!} to define Python library
dependencies, which can be installed into a virtual environment using a
package manager such as \emph{pip} or \emph{uv}.

At this point, a virtual machine must be prepared for use with AKF. As
with prior synthesizers, it is possible to manually configure a machine
by downloading a supported operating system and creating a new virtual
machine from scratch. The manual process, which is similar to that of
other synthesizers, is as follows:

\begin{itemize}
\tightlist
\item
  Download an ISO or pre-prepared virtual machine from a distributor
  with the desired operating system.
\item
  If necessary, create a new VM with that operating system image and
  install it.
\item
  Configure the virtual machine with the desired host resources,
  including two network interfaces - one connected to the NAT adapter
  for general internet usage, and one connected to the host-only adapter
  for agent communications.
\item
  Create an administrative user with known credentials. Configure the
  operating system as desired to reduce friction with the synthesizer
  (disabling UAC prompts, enabling auto-logon, etc.)
\item
  Build and copy the OS-specific AKF agent to the virtual machine,
  configuring it as a startup application. Add firewall rules to ensure
  that the host and agent are able to communicate.
\end{itemize}

After following this process, the virtual machine can now be cloned and
used as needed in AKF scripts. Although relatively straightforward, this
process is still time-consuming, especially when adapting this process
for new operating systems. While a prepared AKF virtual machine can
theoretically be distributed (in a virtual appliance format such as
OVF), this may run into legal issues if software on the underlying
operating system is copyrighted.

As a result, AKF makes use of modern infrastructure-as-code solutions to
vastly simplify the setup of new virtual machines. Vagrant, developed by
HashiCorp, is a tool for rapidly building development environments
\cite{HashicorpVagrant2025}. It allows users to define and build
virtual machines on a variety of virtualization platforms, such as
VirtualBox and VMWare. Virtual machines are built using a ``box'' as a
base image, which is then configured according to a Vagrantfile
describing hypervisor-specific configuration options as well as
instructions to provision the machine with applications. (A similar
approach of distributing the ``differences'' of base images is used to
reduce the size of distributed forensic datasets by \textbf{EviPlant},
as described in \autoref{distribution}.)

The AKF Windows agent includes a Vagrantfile for creating a new Windows
11 virtual machine with the agent installed and configured, which can
easily be adapted for other platforms and hypervisors. The Vagrantfiles
used to generate a dataset should be included with the dataset itself to
maximize reproducibility, as described in \textbf{39.5 - Output and
validation\#5.5 - Community distribution and reproducibility}. A robust
ecosystem of Vagrant boxes for varying Linux distributions and Windows
versions exists, many of which can be pulled from the Vagrant public
registry \cite{hashicorpHashiCorpCloudPlatform}. When combined with
the flexibility of Vagrant over multiple virtualization platforms, this
can significantly improve the reproducibility and usability of AKF
across many platforms. It should also be noted that Vagrant can be used
to configure and build larger environments with multiple machines. For
organizations that are able to express corporate environments as
Vagrantfiles, this could allow AKF to perform artifact generation at
scale, allowing for incident response scenarios reflecting real-world
networks.

Following setup, developers can build scenarios by using the AKF core
libraries (\passthrough{\lstinline!akflib!}) and the API of the
platform-specific agent installed onto the virtual machine. This
reflects typical imperative usage, in which environment setup, artifact
generation, and output generation is handled explicitly through a script
executed through the Python interpreter.

A simple AKF script achieving the same outcomes as the ForTrace and SFX
scripts above follows:

\section{Declarative usage}\label{declarative-usage}

\subsection{Existing declarative
syntaxes}\label{existing-declarative-syntaxes}

As described previously, declarative inputs are well-structured files
with a fixed set of available actions -- effectively forming an API --
where each entry in the file specifies an artifact (or set of artifacts)
to be placed in the dataset, along with any configurable options that
are available for that artifact.

Declarative formats can be used in one of two ways. The first is
\emph{execution}, in which the elements of the declarative script are
directly interpreted to generate imperative API calls. This is
characteristic of synthesizers that only support declarative script
inputs, exposing no low-level APIs. This takes advantage of the
high-level nature of declarative scripts; a declarative script can
remain the same even if the libraries that execute it change, so long as
the \emph{interpreter} is updated accordingly. The second is
\emph{translation}, in which the declarative script is used to generate
an equivalent imperative script adhering to a particular synthesizer's
API. This allows the declarative script to be used as a ``base'', after
which an experienced scenario developer can make additional
modifications to the generated imperative script as needed. Such scripts
are subject to changing dependencies and deprecated functions, but can
be regenerated so long as the \emph{translator} is updated accordingly.

It is important to note that a declarative syntax, which may be more
``rigid'' in structure, does not preclude the use of randomness. One
notable example of this is the discrete-time Markov chains used by
Yannikos et al.~to express scenarios in a probabilistic manner, with
each state of the Markov chain representing a particular action (sending
an email, deleting a file, etc.) taken by the synthesizer
\cite{yannikosDataCorporaDigital2014}. These chains are evaluated at
runtime to generate multiple unique datasets from a single description.

The challenges of defining a suitable declarative syntax for a
particular synthesizer is not unlike the challenges faced in general
programming language design. There are several key factors to the
success of imperative programming languages that extend to declarative
syntaxes, some of which are derived from \cite{finkel1996advanced}
and described as follows:

\begin{itemize}
\tightlist
\item
  The language should be \textbf{simple}, using as few basic concepts as
  possible. This makes code easier to read and write, an important
  aspect for users with limited programming experience.\\
\item
  The language should be \textbf{modular}, such that the role and
  interfaces of individual program units is clear.
\item
  The language should be \textbf{predictable}, such that users can apply
  their existing knowledge of a synthesizer to easily implement or add
  new features to a scenario.\\
\item
  The language should \textbf{abstract} as much as possible away from
  the user, such that the minimum information needed to fulfill artifact
  generation is exposed to the user.
\end{itemize}

Perhaps the most important factor, however, is an awareness of the
\textbf{purpose} of the declarative syntax. The purpose of a synthesizer
is to make it easier to generate forensic artifacts and datasets. The
declarative language should reflect this, focusing on making actions and
artifacts as easy to declare and customize as possible.

In designing the AKF declarative syntax, the declarative syntaxes of
both prior syntaxes and unrelated technologies were evaluated. An
analysis of some of these syntaxes, with examples, is described briefly
in \autoref{historical-declarative-syntaxes}.
However, two syntaxes in particular contributed the most to the AKF
declarative syntax: those of ForTrace and Ansible.

ForTrace uses YAML to express scenarios in a declarative manner.
ForTrace influenced both AKF declarative syntax and the libraries
leveraging the syntax significantly, in large part because it was the
sole synthesizer identified with both imperative and declarative support
that was open source.

Below is a simple example of a ForTrace declarative scenario:

\begin{lstlisting}
name: haystack-example
description: A example action suite to generate a haystack (traffic)
author: MPSE Group
seed: 1234
collections:
  c-http-0:
    type: http
    urls: ./generator/friendly_urls.txt
settings:
  host_nfs_path:
  guest_nfs_path:
applications:
hay:
  h-http-0:
    application: http
    amount: 3
    collection: c-http-0
needles:
  n-http-0:
    application: http
    file: [https://dasec.h-da.de/](https://dasec.h-da.de/)
    amount: 1
dumps:
  d-dump-0:
    dump-type: mem
    dump-path: /home/fortrace/gendump.file
\end{lstlisting}

At a high level, ForTrace scenarios contain five distinct elements:

\begin{itemize}
\tightlist
\item
  Metadata about the scenario, such as the name, description, and author
  of the scenario.
\item
  ``Collections'' of data that can be reused throughout the scenario in
  supported application types, such as a newline-separated list of URLs.
\item
  Configuration options, which may be applied to individual applications
  or the entire scenario.
\item
  The actual artifacts to create as part of the
  \passthrough{\lstinline!hay!} and \passthrough{\lstinline!needles!}
  sections, where \passthrough{\lstinline!hay!} includes artifacts that
  should be considered background noise, and
  \passthrough{\lstinline!needles!} includes artifacts that should be
  considered significant. Each artifact contains a unique ID, an
  application type (the \passthrough{\lstinline!application!} key), and
  arguments that are specific to the application responsible for
  generating the artifact, such as a the URLs for web browsing.\\
\item
  Any core outputs that should be created as part of the scenario.
\end{itemize}

This file is passed into a ``generator'', which parses the contents of
the YAML file to prepare various internal data structures, initialize
the virtual machine, and then execute the actions specified in the
\passthrough{\lstinline!hay!} and \passthrough{\lstinline!needles!}
sections in a random order according to the
\passthrough{\lstinline!seed!} key. Depending on the value of the
\passthrough{\lstinline!application!} key, the data for that action is
passed to an application-specific handler that interacts with the
running virtual machine using existing ForTrace libraries. Once all the
actions have been executed, the generator creates any requested outputs
(such as volatile memory dumps) and shuts down the virtual machine.

This analysis provided insight into the design decisions and
functionality required to execute actions from the high-level
descriptions of a scenario. In particular, it demonstrates the need for
actions or artifacts to be defined in a consistent, flexible manner such
that program state and other data can be passed to application-specific
libraries as needed. It also demonstrates the need for various levels of
configuration, including scenario-wide configuration,
application-specific configuration, and action/artifact-specific
configuration. ForTrace implements this in a somewhat inflexible manner;
in fact, nearly all declarative language support is contained in a
single file, with a hardcoded ``router'' handling each unique
\passthrough{\lstinline!application!} type. This makes it difficult to
add support for new applications without significant effort,
particularly because the generator must be aware of every possible
action/artifact type ahead of time.

With these priorities and issues from ForTrace identified, are there
ideas from other technologies that can be used to address them? That is,
are there other technologies designed to execute a large set of complex
actions, using a simple but flexible and configurable syntax, and how
does it work? Ansible, the second major inspiration for the AKF
declarative syntax, precisely fills this need.

Ansible is an open-source automation framework that is often used to
remotely configure Windows and Linux machines at scale, allowing
organizations to interact with many machines at once without the need to
install orchestration software on these machines in advance. To achieve
this, users write \emph{playbooks}, which are simple YAML files that
contain one or more \emph{plays}. Each play is simply a set of
\emph{tasks} that is run on multiple machines at once, and each task
depends on a \emph{module} that is designed to achieve a single,
specific outcome.

The following is a simple Ansible playbook with a single play, derived
from the official Ansible documentation for playbooks:

\begin{lstlisting}

- name: Update web servers
  hosts: webservers
  remote_user: root
  tasks:
  - name: Ensure apache is at the latest version
    ansible.builtin.yum:
      name: httpd
      state: latest
  - name: Write the apache config file
    ansible.builtin.template:
      src: /srv/httpd.j2
      dest: /etc/httpd.conf
\end{lstlisting}

This play uses the \passthrough{\lstinline!yum!} package manager to
install Apache, and then copies a local file to the remote host.
\passthrough{\lstinline!ansible.builtin.yum!} and
\passthrough{\lstinline!ansible.builtin.template!} are both modules in
the \passthrough{\lstinline!ansible.builtin!} collection included with
all default Ansible installations, which accept parameters passed as a
YAML dictionary.

Although Ansible contains many features that contribute to its
flexibility, the two important concepts of Ansible that are relevant to
AKF are \emph{roles} and \emph{modules}. Roles are a collection of
Ansible resources that typically achieve some ``larger'' reproducible
goal, typically by running multiple tasks and leveraging variables,
configuration options, and files included as part of the role. Roles can
include \emph{modules}, which are standalone imperative scripts
(typically in Python) that accept arguments, execute code based on those
arguments, and return data using well-structured interfaces. These
modules, as shown above, can be called and executed from playbooks or
executed independently on the command line.

These concepts are extremely relevant to synthesizers, as they must
support individual application-specific actions and group these actions
together in a flexible, well-defined manner. As described in
\autoref{chapter-four}, each of the RPyC subservices of AKF
agents expose a group of application-specific automation methods. The
functionality of each of these groups must be re-exposed in a
declarative manner, which can be achieved by adapting the concept of
Ansible roles and modules to AKF.

Together, the ForTrace and Ansible syntaxes provide three concepts that
are reflected in the AKF syntax, described further in the following
section:

\begin{itemize}
\tightlist
\item
  The overall structure and information of the data contained in the
  YAML file
\item
  An action syntax that allows us to declare individual actions,
  referring to those actions by name, and pass arguments directly to
  that action for translation \emph{or} execution
\item
  A modular architecture that allows us to define the supported
  arguments of each action and expose them to the declarative
  interpreter, while also being as decoupled from the standard
  imperative library as much as possible
\end{itemize}

\subsection{The AKF declarative
syntax}\label{the-akf-declarative-syntax}

The AKF declarative syntax is very similar to the Ansible playbook
syntax. Declarative scripts are comprised of metadata, global
configuration, libraries to import, and individual tasks to execute as
part of the scenario. Each task refers to a single \emph{module} by name
using a qualified Python import path, accepting a dictionary of
arguments in addition to global configuration overrides.

Similar to Ansible, individual modules are implemented as well-defined
subclasses of \passthrough{\lstinline!AKFModule!}, an abstract base
class that serves as the root of all AKF declarative modules. Each
module must define Pydantic models that specify the arguments accepted
by the module. The arguments declared in the YAML file are then passed
to these module-specific Pydantic models for validation, after which the
\passthrough{\lstinline!AKFModule!} must perform one of two tasks:

\begin{itemize}
\tightlist
\item
  \textbf{Execution}: When instructed to perform actions directly from
  the argument model, the \passthrough{\lstinline!AKFModule!} should
  import AKF core libraries and agent APIs to carry out the required
  actions.
\item
  \textbf{Translation:} When instructed to generate imperative code from
  the argument model, the \passthrough{\lstinline!AKFModule!} should
  generate the equivalent code that \emph{would} perform related actions
  if executed through a standard Python interpreter with the necessary
  libraries installed.
\end{itemize}

The ability of AKF to both \emph{execute} and \emph{translate}
declarative scripts provides significant flexibility to scenario
developers. To the best of the author's knowledge, prior synthesizers
have only supported direct execution from declarative scripts, which
limits the opportunities to use declarative scripts as a ``starting
point'' for writing more complex imperative scripts.

In both cases, modules can read and modify a global state dictionary
that allows otherwise independent modules to cooperate with each other.
For example, if an action should be executed in a context manager,
causing the indentation level of the code to increase, successive
modules generating code within the context manager can increase the
indentation level of generated code, as well. Additionally, this allows
for ``outputs'', such as CASE bundles, to be passed and gradually
constructed across modules. This design allows for context-aware code
generation and action execution.

An example of a declarative AKF scenario, carrying out the same actions
as the SFX, ForTrace, and imperative AKF script above, is shown below:

\begin{lstlisting}
name: sample scenario
description: sample scenario
author: lgactna
seed: "0"
libraries:
  - akflib.actions
actions:
  - name: Run the sample action
    module: akflib.actions.sample.SampleModule
    args:
      arg1: "value1"
      arg2: "value2"
\end{lstlisting}

AKF declarative scripts, which are simply large YAML dictionaries,
contain three distinct elements. The first is a set of high-level
metadata keys associated with the scenario. The second is a set of
scenario-wide configuration; in particular, it lists the libraries
containing the necessary modules to execute or translate this imperative
script. Finally, scripts list out a sequence of actions, which are
typically a \passthrough{\lstinline!module!} specified by name and a
dictionary of \passthrough{\lstinline!args!}.

The execution flow of the declarative interpreter itself is relatively
straightforward. Given a path to a YAML script, the interpreter will
load the necessary libraries and configuration keys defined in the file
and instantiate resources accordingly. This may include setting global
configuration variables, locating and starting a virtual machine by
name, setting the \passthrough{\lstinline!random!} seed, and so on.
Then, the interpreter simply runs each module under the
\passthrough{\lstinline!actions!} key with the provided arguments and
configuration in order, continuing until all
\passthrough{\lstinline!actions!} have been completed.

Modules are located and executed using Python's dynamic import system.
(For efficiency and safety, all modules in the script are located at the
start of script execution.) These modules can be located in any library
so long as they can be found through Python's import system. For
example, both \passthrough{\lstinline!akflib!} and the AKF Windows agent
contain their own declarative module libraries, leveraging functionality
specific to each code repository.

This design allows for declarative modules to be written independently
of the libraries they depend on, reducing the ``impact'' of supporting
declarative features on the core imperative libraries. In fact, this
independence allows for the AKF module system to be used in
general-purpose scripting, similar to Ansible; it is not tightly bound
to the creation of forensic scenarios and artifacts.

While the AKF imperative and declarative scripts provide users with
significant flexibility in \emph{using} AKF, there still remains the
challenge of building artifacts and scenarios to generate through these
execution options. The remainder of this chapter addresses this
challenge.

\section{Using generative AI for individual
artifacts}\label{using-generative-ai-for-individual-artifacts}

Users of synthesizers must still perform a significant amount of work
when generating individual artifacts. More precisely -- while AKF and
other synthesizers can streamline the process of placing artifacts on a
dataset, users must still provide the actual artifacts themselves. For
example:

\begin{itemize}
\tightlist
\item
  If a user wants to place 100 photos on the drive to simulate real
  usage, the user needs to create and provide 100 realistic images.
\item
  If a user wants to simulate an email or other online conversation, the
  user needs to provide the entirety of the conversation to simulate.
\item
  If a user wants to generate ``proprietary'' documents to emulate some
  form of corporate sabotage, the user would need to create and provide
  a variety of Microsoft Office, PDF, or other files in these formats.
\end{itemize}

This is particularly relevant when adding background noise that is often
present in real-world datasets -- the gigabytes of documents created as
part of a user's benign activity over . The net result is that although
creating forensic datasets can be accomplished with the work presented
thus far, creating realistic images that are more reflective of
real-world scenarios that a forensic analyst might encounter still
requires extensive work. While true that images should often be small
enough in a classroom setting to allow the student to explore a single
specific technique, real-world scenarios encountered by analysts are
typically not limited by time or size. An analyst might have to deal
with a drive used over the course of a decade to store many photographs
and send many messages. Such scenarios are valuable training material
for courses that encapsulate a long period of forensic study, allowing a
student to apply many different techniques in reconstructing a
large-scale scenario.

With recent advancements in generative AI, popularized by services such
as Midjourney and ChatGPT, it is now significantly easier to generate
realistic images and text content from short, high-level descriptions.
Additionally, various services exist for creating realistic audio and
video files that emulate a particular person's voice or facial
movements; these can be used to generate additional scenario content of
interest, especially if the scenario is based on a real-world event.

It holds that generative AI can be used to quickly populate forensic
datasets with realistic conversations and images consistent with an
arbitrary scenario. For example, a corporate espionage case could be
built by providing a large language model such as ChatGPT with prompts
to describe complex machinery in both a technical writing and a
conversational style. Simultaneously, similar prompts can be passed into
an image synthesizer such as Midjourney to produce related images. The
images and text produced can then be used to create documents describing
an unreleased product of high value, providing a pipeline through which
significant artifacts can be planted onto a forensic image.

This idea can be extended further by training models on specific
datasets; for example, if an instructor wished to create a fictional
scenario in which a user frequently interacts with users of a particular
online community, a large language model could be trained on available
conversations to provide a degree of realism to the scenario. However,
as mentioned before, this faces the challenges of ownership, privacy,
and legality behind works derived from publicly available information
that was (likely) not published with the expectation of its usage in an
AI model.

It is important to note that the inclusion of generative AI into
synthesizers does not necessarily require deep integration with the
framework itself. Many existing frameworks could be extended to use
documents, images, or other data sourced from generative AI instead of
user-defined files without the need to change the architecture of the
framework. However, as advancements in AI continue, it may make sense to
directly integrate AI-driven actions into synthesizers. For example,
there may come a time in which synthesizers can be provided natural
language prompts (such as ``Open Firefox and browse to news-related
websites'') that directly lead to the generation of relevant artifacts,
without the need to explicitly program the process of browsing to a
website in advance.

\chapter{Evaluation and observations}\label{chapter-seven}

\section{Context and scenario}\label{context-and-scenario}

\section{Student analysis}\label{student-analysis}

\chapter{Future work}\label{chapter-eight}

AKF was built with the expectation that it would be easy to maintain,
develop, and extend. Indeed, there are several use-cases that AKF does
not fulfill as of writing, primarily due to time constraints. Future
work related to AKF can be divided into two groups -- tasks that
integrate cutting-edge advancements to implement new features, and tasks
that extend or improve existing functionality. \autoref{open-ended-automation-with-ai} describes the application of recent AI
developments towards addressing tasks relevant to forensic dataset
development; the remaining sections focus on extending existing concepts
in AKF.

\section{Open-ended automation with
AI}\label{open-ended-automation-with-ai}

During the development of AKF, OpenAI announced the release of Operator,
an ``agent'' capable of automating tasks on webpages using natural
language prompts \cite{openaiIntroducingOperator2025}. Rather than
using a browser automation framework like Playwright or Selenium, it
leverages its own browser to interact with webpages. This allows users
to provide Operator with a high-level goal, which it can then convert to
concrete webpage actions to achieve that goal.

The example provided by OpenAI involves Operator searching Allrecipes
for a clam linguine recipe, then ordering the ingredients for the
linguine through Instacart. Various sensitive actions, such as inputting
payment information or logging into a service, are delegated to the user
to complete. Operator is also capable of identifying ambiguity in a task
and prompting the user, such as asking the user which store to use for
ordering Instacart items. This demonstrates multiple notable features
that are relevant to forensic synthesizers; in particular, it
demonstrates the ability to both \emph{interpret} and \emph{interact}
with arbitrary GUIs, as well as the ability to convert human prompts
into a sequence of automated actions that may change as the agent
discovers new information or encounters unexpected issues.

It is powered by what OpenAI calls its ``Computer-Using Agent'', or CUA,
which is trained to interact with a virtual machine by accepting natural
language and a screenshot of a virtual monitor
\cite{openaiComputerUsingAgent2025}. It leverages OpenAI's GPT-4o
model, following a three-step process in which it analyzes screenshots
of the virtual desktop, conducts reasoning to determine the necessary
steps to achieve a task (using its own prior context), and executes
actions on the virtual machine. OpenAI notes that CUA can reliably
perform simple tasks that a human would normally take, such as
navigating to specific categories of websites or repeating UI
interactions. However, it struggles with UIs that it has not encountered
before and tends to be inefficient or hallucinate on more complex
tasking.

The challenge of automating open-ended tasks through AI is not new.
Multiple benchmarks (mentioned in OpenAI's articles), including
WebVoyager, WebArena, and OSWorld, were developed in early 2024 to
provide examples of typical webpage and OS interaction tasks done by
humans
\cite{zhouWebArenaRealisticWeb2024,heWebVoyagerBuildingEndtoEnd2024,xieOSWorldBenchmarkingMultimodal2024}.
CUA is stated to achieve state-of-the-art results in these benchmarks,
and therefore represents the current ability of AI to address open-ended
tasking. Although CUA has clear limitations and falls well behind human
performance on these benchmarks, Operator demonstrates significant
progress in the ability of AI models to replicate actions that are often
performed by real users. Even in its current state, CUA is likely able
to automate the ``simple'' tasking that is characteristic of generating
background noise for forensic datasets, such as browsing news sites,
interacting with social media platforms, and more.

In the near future, it is likely that works similar to CUA or Operator
will be capable of fully automating human actions as part of a larger
scenario with a high degree of reliability and accuracy. Not only does
this accelerate the process of building forensic datasets, it also
improves the variety of applications that can be used as part of a
scenario due to the flexibility of these models. For educators whose
focus is to build scenarios with investigative and analytic value to
students, this may be exactly what is needed to streamline scenario
development. What does this mean for synthesizers?

First, the verbosity of writing a script and passing it to a synthesizer
is still valuable, especially in contexts where the non-determinism and
opacity of an AI model may not be acceptable. Non-determinism is often
acceptable in educational contexts, so long as the actions taken by the
model are logged and can be verified after the fact. However, the
development of datasets for research and tool validation may require
that actions are taken in a specific manner every time the dataset is
generated. Several works have described the non-deterministic nature of
LLMs in multiple distinct tasks
\cite{astekinExploratoryStudyHow2024,songGoodBadGreedy2024,ouyangEmpiricalStudyNonDeterminism2025},
which negatively impacts the reproducibility of results -- an important
quality of forensic datasets as described by Grajeda et al.
\cite{grajedaAvailabilityDatasetsDigital2017}.

Second, these developments are not \emph{incompatible} with
synthesizers, and should instead be seen as an option to complement
them. The capabilities provided by Operator could likely be built into
AKF as part of its internal library or an OS-specific agent, providing
users with access to both verbose imperative/declarative scripts as well
as simpler natural language prompts when automating actions and building
scenarios. Additionally, these agents cannot (currently) act as a
substitute for physical artifact generation, in which the underlying
filesystem or disk image must be edited to fulfill a certain task.

\section{Alternative platform
support}\label{alternative-platform-support}

Perhaps the most impactful example is implementing support for other
desktop environments, such as Mac and Linux. At a high level, this
requires implementing all three artifact generation methods described in
\autoref{chapter-four}.

Consistent with AKF's focus on using existing automation frameworks
through agents to implement application-specific functionality, much of
the effort for supporting other platforms requires writing and deploying
a new OS-specific agent. It is likely that much of the same code and
overall design can be shared between the Windows agent and other
platforms. This is largely possible through the portability of Python
and the lack of OS-specific assumptions made by RPyC. Certain automation
frameworks may have significant cross-platform support (such as
Playwright and \passthrough{\lstinline!pywinauto!}, which supports
Windows, Mac, and Linux), though implementing functionality for other
applications may require more effort.

In general, implementing logical agentless generation through VirtualBox
is likely straightforward, in large part because Oracle supports
VirtualBox Guest Additions on MacOS and a large variety of Linux
distributions. Similarly, the physical artifact generation implemented
in AKF for FAT32 and NTFS (the most common filesystems for Windows) may
be extensible to other disks, such as ext4 (the default for modern Linux
distributions).

The challenge here lies in accounting for artifacts or mechanisms unique
to other operating systems. Some features, such as the use of
PowerShell/WinRM to carry out actions on Windows, are analogous to
Bash/SSH on Unix-based systems and can be adapted accordingly. However,
some concepts are truly unique to an operating system, such as
performing modifications to the operating system through registry keys,
and may require more effort to adapt the same outcomes to other
platforms.

\section{Additional interface
implementations}\label{additional-interface-implementations}

Another example is extending the various concrete implementations of AKF
interfaces to other technologies that may have better support for
specific host and guest platforms. AKF currently provides a
VirtualBox-based implementation of the hypervisor-agnostic interface
provided as part of \passthrough{\lstinline!akflib!}, largely depending
on the VirtualBox Guest Additions software to function. Indeed, this is
the approach taken by virtually all prior synthesizers that require
virtualization -- existing works in implementing synthesizer
functionality in VirtualBox have contributed significantly to its
adoption in these synthesizers, as well as AKF.

However, prior synthesizers have also considered KVM/Qemu and VMWare as
hypervisors; in particular, Forensig2 leveraged Qemu as its
virtualization platform \cite{mochForensicImageGenerator2009}. There
are several motivations for using other synthesizers, including support
on different host platforms, performance, and available functionality.
For example, Hwang et al.~compared the performance and low-level
features of multiple hypervisors, including Hyper-V, KVM/Qemu, and
VSphere, finding significant variability between hypervisors when
running workloads and applications
\cite{hwangComponentbasedPerformanceComparison2013}. It may be the
case that certain scenarios or host machines are better suited for other
hypervisors, or even a multi-hypervisor environment. It is worth noting
that VMWare has similar guest-specific software to VirtualBox, with
multiple Python libraries for interacting with guests on VSphere such as
\passthrough{\lstinline!pyvmomi!} \cite{VmwarePyvmomi2025}.

Similarly, rather than create new implementations of existing
interfaces, it may also be worth building new interfaces entirely. One
particular example is the implementation of agents in languages other
than Python. This may cover needs for specific tasks (such as actions
that can be automated using a library for which no Python equivalent
exists) or specific deployment restrictions (such as avoiding specific
forms of synthesis pollution). In general, the implementation for an
agent can be written in any language, so long as a Python API exists;
the burden would be on the author to handle any communication or
discrepancies across disparate languages, such as writing a TCP-based
protocol for issuing commands.

\section{Distribution}\label{distribution}

One notable aspect of prior synthesizers not implemented as part of AKF
are various improvements to the distribution of forensic datasets,
particularly in environments with limited bandwidth or limited storage
space. AKF largely addresses challenges in the \emph{creation} of
forensic datasets, rather than their \emph{distribution}. This section
largely focuses on prior efforts to reduce the size of forensic
datasets, as well as how they could be integrated into AKF in the
future.

The size of forensic datasets can be trivially reduced in various ways.
For example, a full forensic dataset (with various core outputs) can be
compressed using a standard algorithm such as LZMA. Specific core
outputs, such as disk images, can be stored in a dynamically expandable
image format such as VDI or VHD. Such formats only use as much space as
is currently used on the disk image itself, even if the size reported to
the operating system is much larger. Such reductions are ``lossless'',
in the sense that they largely reflect what a forensic analyst would
encounter using real hardware.

However, there are more aggressive improvements that can be made with
respect to the distribution process. These typically come at one of two
expenses -- either irrelevant data is outright removed, or additional
effort must be taken by the end user to finish ``constructing'' the
forensic scenario for use after downloading relevant files (which are
significantly smaller than the finished forensic dataset itself). The
first is a tradeoff between realism and image size; the second is a
tradeoff between immediate usability and additional processing time.

The first method describes the process of simply removing data that is
not deemed to be relevant for educational or research purposes. For
example, if an instructor is simply interested in demonstrating the
recovery and analysis of SQLite databases in Chrome's AppData directory,
there is no need to include files from most other directories on the
filesystem. One option for achieving this is by using a logical disk
image format, such as the .AD1 format, which are collections of files
(much like a ZIP archive) as opposed to a collection of physical disk
units.

A more notable approach to removing irrelevant information is described
by Russell et al.~as part of their work developing \textbf{SFX}
\cite{russellForensicImageDescription2012}. Their approach,
described as ``partition squeezing'', is based on the idea that the
contents of files irrelevant to an educational scenario can be truncated
to the size of a single filesystem cluster or block. This preserves the
file and directory structure of the overall filesystem (thus preserving
realism), while also removing data that is unlikely to be useful to
students performing analysis. Thus, the educational value of the image
remains the same, even if some data is missing. Russell et al.~note that
additional space can be saved by replacing irrelevant files,
particularly those deep within the filesystem that are highly unlikely
to be analyzed in detail by students, with hard links to other files.
This eliminates the original contents of the file entirely while
preserving most of its attributes.

The use of logical images and partition squeezing, of course, is not
suitable in all cases. In particular, logical images generally do not
include slack or unallocated space. The partition squeezing approach
destroys data that may be required for certain forensic analysis tools
to function, as well as data that might be of interest to users of the
dataset in other research or educational contexts.

AKF does not provide any functionality for directly generating logical
images or performing partition squeezing. However, generating logical
images is relatively straightforward using a publicly available tool
such as FTK Imager, so long as the scenario developer is aware of
relevant directories that should be included -- a process that might
occur with AKF's reporting features in mind. Partition squeezing might
be achieved by leveraging the filesystem-aware functionality of
libraries such as \passthrough{\lstinline!libtsk!} and
\passthrough{\lstinline!dfvfs!}, which is used as part of AFK's physical
planting techniques described in \autoref{akf-implementation}. Such functionality could be implemented either in
AKF or as part of a wholly independent tool.

The second method describes a variety of techniques in which end users
must take additional actions after downloading relevant materials before
a forensic dataset is ready for use. One example described by Scanlon et
al.~is the use of ``evidence packages'', in which users are provided
with a base image of a single operating system that can be reused to
build multiple forensic datasets through \textbf{EviPlant}
\cite{scanlonEviPlantEfficientDigital2017}. After a scenario
developer has finished creating artifacts (whether manually or through a
synthesizer), they can use EviPlant's ``diffing'' tool to determine and
extract differences between the base image and the developer's current
disk state. These differences can be distributed to users, who can then
use EviPlant's ``injection'' tool on the previously-distributed base
image to recreate the final disk image.

This is described more broadly as ``differential imaging'', in which
only differences from some initial file are transmitted to users. These
differences are often significantly smaller than the disk images they
are designed to build, and therefore are much easier to distribute than
complete disk images. In exchange, each user must spend additional
processing time before the image can be used for analysis. EviPlant's
injection tool operates through a combination of logical and physical
planting, which requires time software, and resources that are not
required when distributing complete disk images. This is an acceptable
cost particularly when conducting remote classes, where not all students
may have high-speed internet. Again, although AKF does not implement
differential imaging, this functionality can likely be implemented as
part of an independent tool or module.

\section{Mobile synthesis}\label{mobile-synthesis}

Finally, while well outside the scope of this thesis, there is also the
challenge of building a synthesizer for non-desktop platforms,
particularly mobile devices. For many people, mobile devices are the
primary means through which we interact with the digital world, causing
them to play a unique role in investigations
\cite{chernyshevMobileForensicsAdvances2017}. They can contain data
from many facets of our lives, including text messages, location
history, images, application logs, and other information that can
provide insight into an individual's actions during a period of interest
\cite{sutiknoCapabilitiesCellebriteUniversal2024}. This information
can be combined with other investigative methods, such as desktop and
conventional forensics, to form a better understanding of a larger
scenario.

Naturally, this means that there is a need for datasets in digital
forensics, as well. Grajeda et al.~identify a small number of existing
mobile dataset collections, including those containing Android malware,
Android application files, and smartphone disk images
\cite{grajedaAvailabilityDatasetsDigital2017}. However, these
datasets are far less common than their desktop counterparts. When
compared to the hundreds of desktop disk images hosted on Digital
Corpora and CFReDS, there are only around 25 mobile disk images on the
same platforms. A mobile-specific survey conducted by Gonalves et
al.~in 2022 found not only a low availability of mobile images, but also
lack the recency and variety of data that would be present in a modern
investigation \cite{goncalvesRevisitingDatasetGap2022}.

There has been limited work in developing forensic synthesizers for
mobile platforms. Two notable examples are FADE
\cite{ceballosdelgadoFADEForensicImage2022}, developed by Delgado et
al.~in 2022, and an unnamed framework developed by Demmel et al.~in 2024
\cite{demmelDataSynthesisGoing2024}. FADE operates largely through
physical artifact generation, which it achieves by extracting and
mounting partitions from an emulated, rooted Android device using the
Android Debug Bridge. It then modifies application-specific database
files to create artifacts such as phone call entries and text messages.

In contrast, Demmel et al.~generate data using a logical agentless
approach, using a tool called AndroidViewClient (AVC) to send keypresses
and touch gestures. Unlike VMPOP, which uses hardcoded mouse movements
to click on GUI-based elements, this framework leverages AVC to dump all
interactable UI elements and directly ``touch'' these elements once the
desired element has been found. This allows it to implement more
application-specific functionality, such as using WhatsApp and opening
Google Chrome. There has also been broader work in automating actions as
part of testing pipelines for Android applications
\cite{janickiObstaclesOpportunitiesDeploying2012,nagowahNovelApproachAutomation2012,linares-vasquezHowDevelopersTest2017},
though these largely address actions at the application scope, rather
than automating actions across the entire device. Of note is the lack of
iOS-relevant synthesizer functionality, perhaps due to the greater
difficulty of working on a closed-source operating system with fewer
options for exposing internal functionality.

It is possible that \emph{some} of AKF's architecture could be adapted
to support mobile dataset generation, primarily by interacting with
Android emulators and existing developer tools. However, the isolation
around individual Android applications suggests that it may be difficult
to use a logical agent-based approach to automating application
activity; a Python agent running as an application is unlikely to have
full filesystem access on a non-rooted device. However, application
isolation also suggests that it may be easier to build and use physical
approaches to generating mobile artifacts, since these artifacts will
largely be constrained to the application's allocated directory. More
research is needed to determine viable options through which Android
datasets can be constructed. This is especially true for iOS, for which
there are significantly fewer resources.

\chapter{Conclusion}\label{chapter-nine}

Public forensic datasets are invaluable to advancing research and
education throughout the field of digital forensics. However,
high-quality datasets are few in number and may not fit specific needs,
motivating the development of new datasets. Constructing these datasets
by hand is time-consuming and prone to errors, yet continues to be the
primary method through which new datasets are made. In turn, there is a
need for a synthesizer that is capable of fulfilling the needs of
scenario developers while simultaneously promoting its usage throughout
the forensic community.

AKF introduces a modern approach to forensic synthesis through a modular
architecture with a focus on sustained development and community
adoption. It provides significant improvements in artifact generation,
logging and validation, and the overall construction of new scenarios.
It improves on artifact generation by integrating numerous technologies
not leveraged by prior synthesizers, greatly simplifying implementation
of the overall architecture without compromising the breadth of features
available through the framework. By using a centralized logging
architecture and the CASE standard, AKF is able to generate detailed,
queryable metadata of the datasets it generates, allowing users to
quickly identify artifacts of interest in a dataset. AKF also exposes a
simple declarative syntax for generating artifacts, allowing users to
develop scenarios without needing to write code. Finally, it provides a
demonstration of using LLMs to further streamline scenario development,
both when constructing individual artifacts and when building a complete
scenario.

Although there are still limitations in what AKF is able to accomplish,
there are numerous opportunities to leverage existing and emerging
technologies to extend the framework. These contributions have been made
in the hope that it will advance research and education throughout
digital forensics, allowing organizations to fill known gaps in public
datasets.

% == End thesis content


%glossary & acronym lists
\printglossary[type=\acronymtype]
\printglossary

\nocite{*}
\bibliography{lloyd_gonzales_thesis}

\appendix

\chapter{Architectural diagrams}\label{appendix-a}



\chapter{Code samples}\label{appendix-b}

\section{Code repositories}\label{code-repositories}

The complete implementation of AKF is available on GitHub at the
following locations:

\begin{itemize}
\tightlist
\item
  Core libraries (\passthrough{\lstinline!akflib!}):
  \url{https://github.com/lgactna/akflib}
\item
  The AKF agent for Windows:
  \url{https://github.com/lgactna/akf-windows}
\end{itemize}

Additionally, a standalone library for CASE/UCO 2.0 is provided at
\url{https://github.com/lgactna/CASE-pydantic,} which is used throughout
AKF.

\section{Comparison of ForTrace and AKF
agents}\label{comparison-of-fortrace-and-akf-agents}

Invoking commands in ForTrace entails issuing a simple string to the
agent running on the VM. For example, invoking a command to add a new
user to the VM may look like the following:

\begin{lstlisting}[language=Python]

# From Demo.py

guest = virtual_machine_monitor1.create_guest(guest_name=imagename, platform="windows")

# Wait for the VM to connect to the VMM

guest.waitTillAgentIsConnected()

# create userManagement object

userManagement_obj = guest.application("userManagement", {})

# Add different users

logger.info("Adding user1")
try:
    userManagement_obj.addUser("user1", "password")
    while userManagement_obj.is_busy is True:
        time.sleep(1)
except Exception as e:
    print("An error occured: ")
    print(e)
time.sleep(5)
\end{lstlisting}

The agent's main loop, which receives and interprets these commands, is
extremely simple:

\begin{lstlisting}[language=Python]
def main():
    # create logger
    logger = create_logger('guestAgent', logging.INFO)

    logger.info("create Agent")
    a = Agent(operating_system=platform.system().lower(), logger=logger)
    logger.info("connect to fortrace controller: %s:%i" % (fortrace_CONTROLLER_IP, fortrace_CONTROLLER_PORT))
    a.connect(fortrace_CONTROLLER_IP, fortrace_CONTROLLER_PORT)

    # let all network interfaces come up
    time.sleep(15)

    # inform fortrace controller about network configuration
    a.register()

    # wait for commands
    while 1:
        time.sleep(1)
        a.receiveCommands()
\end{lstlisting}

When the \passthrough{\lstinline!Agent!} is first instantiated, it forms
a permanent TCP socket out to the configured port and IP address where
it expects the server (the VMM) to be issuing commands.
\passthrough{\lstinline!Agent.receiveCommands()!} receives every message
available by reading the socket, parsing it, and then invoking
\passthrough{\lstinline!Agent.do\_command()!}, which converts the
command message into a specific Python function call with arguments.

The server, or ``virtual machine monitor'' (VMM), issues commands over
TCP to a running instance of the agent on the VM. Each module under
\passthrough{\lstinline!fortrace.application!} can be thought of as a
coherent group of commands associated with a particular \emph{real}
application (like Firefox or Thunderbird). Recall earlier how we found
our \passthrough{\lstinline!userManagement!} application and invoked
\passthrough{\lstinline!addUser!}:

\begin{lstlisting}[language=Python]
userManagement_obj = guest.application("userManagement", {})
userManagement_obj.addUser("user1", "password")
\end{lstlisting}

At a high level, the \passthrough{\lstinline!application()!} call
attempts to import
\passthrough{\lstinline!fortrace.application.\{application\_name\}!}.
Although not enforced by a higher-level interface, each of these modules
contains subclasses of the following four classes (defined in
\passthrough{\lstinline!fortrace.application.application!}) at minimum,
with possibly more helper classes for OS-specific functionality or other
modularity as needed:

\begin{itemize}
\tightlist
\item
  \passthrough{\lstinline!ApplicationVmmSide!}: Contains one function
  for each command implemented in
  \passthrough{\lstinline!ApplicationGuestSide!}, building a message
  that will be interpreted and acted upon by the corresponding
  \passthrough{\lstinline!ApplicationGuestSideCommands!} class.
\item
  \passthrough{\lstinline!ApplicationVmmSideCommands!}: Accepts and
  interprets module-specific messages returned by the agent, which may
  be used to update the remote state as tracked by the host.
\item
  \passthrough{\lstinline!ApplicationGuestSide!}: Implements the actual
  application-specific functionality for the agent, providing one
  function for each separated command by this module.
\item
  \passthrough{\lstinline!ApplicationGuestSideCommands!}: Interprets
  commands and arguments, calling the respective function in the
  corresponding \passthrough{\lstinline!ApplicationGuestSide!} subclass.
  This allows the actual dispatch of commands to be delegated to this
  class, which is free to choose how actions are performed (threading,
  multiprocessing, etc.) as well as any module-wide state it may need to
  maintain.
\end{itemize}

This naming convention is intentional, as individual modules dictate the
name of the module in camelcase. For example,
\passthrough{\lstinline!fortrace.application.userManagement!} contains
\passthrough{\lstinline!UserManagementVmmSide!},
\passthrough{\lstinline!UserManagementVmmSideCommands!}, and so on.
Discovering and getting handles to these classes is performed through
string manipulation of the relevant application's module name, as shown
in the example below:

\begin{lstlisting}[language=Python]

# Agent.do_command()

# load class moduleGuestSide and moduleGuestSideCommands
name = "fortrace." + package + "." + module
self.logger.debug("module to load: " + name)
mod = __import__(name, fromlist=[''])
self.logger.debug("module '" + module + "' will be loaded via __import__")
class_commands = getattr(mod, module[0].upper() + module[1:] + 'GuestSideCommands')
\end{lstlisting}

In the example above,
\passthrough{\lstinline!UserManagementGuestSide.addUser()!} contains the
literal code to execute on the guest when
\passthrough{\lstinline!addUser()!} is called, such as adding the
registry keys needed for a user to be created. On the other hand,
\passthrough{\lstinline!UserManagementVmmSide.addUser()!} contains the
code to send a message over TCP that the agent will understand,
eventually leading to the execution of the agent's version of
\passthrough{\lstinline!addUser()!}.

More precisely, the \passthrough{\lstinline!ApplicationVmmSide!}
subclass effectively serves as the API for calling the associated
functions in the \passthrough{\lstinline!ApplicationGuestSide!} class
running on the virtual machine. This subclass, along with the complete
agent-side code, is stored in a single file. For example, the API and
code for opening a Firefox browser window is as follows:

\begin{lstlisting}[language=Python]
class WebBrowserFirefoxVmmSide(ApplicationVmmSide):
    def open(self, url):
        """Sends a command to open a webBrowserFirefox on the associated guest.

        @param url: Website to open.
        """
        try:
            self.logger.info("function: WebBrowserFirefoxVmmSide::open")
            self.url = url
            self.window_id = self.guest_obj.current_window_id
            self.guest_obj.send(
                "application " + "webBrowserFirefox " + str(self.window_id) + " open " + self.webBrowserFirefox + " " + self.url)

            self.guest_obj.current_window_id += 1
        except Exception as e:
            raise Exception("error WebBrowserFirefoxVmmSide::open: " + str(e))

# Example usage:

browser = WebBrowserFirefoxVmmSide()
browser.open("google.com")
\end{lstlisting}

Calling the \passthrough{\lstinline!open()!} command from the host
causes a message of the following form to be sent to the agent:

\begin{lstlisting}
application webBrowserFirefox <window id
\end{lstlisting}

Upon receiving this message, the agent's main loop will search for the
\passthrough{\lstinline!webBrowserFirefox!} module and import its
corresponding \passthrough{\lstinline!ApplicationGuestSideCommands!}
subclass. After any state management, the subclass will then search for
a function called \passthrough{\lstinline!open!} in its corresponding
\passthrough{\lstinline!ApplicationGuestSide!} class, as implemented
below:

\begin{lstlisting}[language=Python]
class WebBrowserFirefoxGuestSide(ApplicationGuestSide):
    def open(self, args):
        # docstring omitted
        try:
            arguments = args.split(" ")
            web_browser = arguments[0]
            url = arguments[1]

            if len(arguments) 
                self.timeout = arguments[2]
            else:
                self.timeout = 30

            self.logger.info(self.module_name + "GuestSide::open")
            self.last_driven_url = url
            self.logger.debug("URL to call: " + url)

            self.logger.info("open url: " + url)

            self.helper.run_firefox()  # start ff session

            retval = self.helper.navigate_to_url(url)  # browse to the specified url
            if not retval:
                self.logger.warning("could not open url")

            self.agent_object.send("application " + self.module_name + " " + str(self.window_id) + " opened")

            self.agent_object.send("application " + self.module_name + " " + str(self.window_id) + " ready")
            self.window_is_crushed = False
        except Exception as e:
            # Some logging/teardown...
            self.window_is_crushed = True
            self.agent_object.send("application " + self.module_name + " " + str(self.window_id) + " error")
\end{lstlisting}

As described in \autoref{the-akf-agent}, this is achieved in AKF using RPyC services, which are analogous
to the \passthrough{\lstinline!ApplicationGuestSideCommands!} and
\passthrough{\lstinline!ApplicationGuestSide!} classes of individual
ForTrace modules. In addition to providing the RPyC services themselves,
agents also implement an API to call these services using typed
functions.

For example, suppose that we wanted to implement functionality similar
to the ForTrace code above, which allows users to navigate to a webpage.
An equivalent RPyC service and its corresponding API may be implemented
as follows:

\begin{lstlisting}[language=Python]

# Server-side code

class ChromiumService(AKFService):
    def exposed_set_browser(
        self, browser_type: Literal["msedge", "chrome"], profile: str = "Default"
    ) -
        # Various setup code...
        self.browser = chromium.launch_persistent_context(
            headless=False,
            user_data_dir=profile_path,
            channel=browser_type,
            args=[f"--profile-directory={profile}"],
        )

        return self.browser

# Client-side API

class ChromiumServiceAPI(WindowsServiceAPI):
    def __init__(self, host: str, port: int) -
        """
        Initialize the API with an RPyC connection to the service.
        """
        self.rpyc_conn = rpyc.connect(
            host,
            port,
            config={"sync_request_timeout": None},
        )

    def set_browser(
        self, browser_type: Literal["msedge", "chrome"], profile: str = "Default"
    ) -
        self.browser = self.rpyc_conn.root.set_browser(browser_type, profile)
        return self.browser
\end{lstlisting}

Observe that implementing support for a particular service on the
client-side API is as simple as calling an untyped function
\passthrough{\lstinline!rpyc\_conn.root.set\_browser()!}. By wrapping it
around a typed function,
\passthrough{\lstinline!ChromiumServiceAPI.set\_browser()!}, users
regain the ability to use code completion and static linting tools.

\section{CASE Python bindings}\label{case-python-bindings}

As described in \autoref{case-and-python-bindings}, CASE is defined using Turtle, allowing objects to be
written in a human-readable text format. For example, the following set
of triples describes an object called
\passthrough{\lstinline!ApplicationFacet!} with two properties,
\passthrough{\lstinline!numberOfLaunches!} and
\passthrough{\lstinline!applicationIdentifier!}:

\begin{lstlisting}
observable:ApplicationFacet
    a
        owl:Class ,
        sh:NodeShape
        ;
    rdfs:subClassOf core:Facet ;
    rdfs:label "ApplicationFacet"@en ;
    rdfs:comment "An application facet is a grouping of characteristics unique to a particular software program designed for end users."@en ;
    sh:property
        [
            sh:datatype xsd:integer ;
            sh:maxCount "1"^^xsd:integer ;
            sh:nodeKind sh:Literal ;
            sh:path observable:numberOfLaunches ;
        ] ,
        [
            sh:datatype xsd:string ;
            sh:maxCount "1"^^xsd:integer ;
            sh:nodeKind sh:Literal ;
            sh:path observable:applicationIdentifier ;
        ] ,
        ...
        ;
    sh:targetClass observable:ApplicationFacet ;
    .
\end{lstlisting}

An instance of a \passthrough{\lstinline!Application!} object may thus
be represented in the JSON-LD format using the
\passthrough{\lstinline!ApplicationFacet!} as follows (including
attributes omitted above):

\begin{lstlisting}
{
    "@id": "kb:dcec8d09-a8bc-4b7c-93ab-16c7b363d48b",
    "@type": "uco-observable:Application",
    "uco-core:hasFacet": [
        {
            "@id": "kb:68004de9-1139-405f-aea7-2c05f3a84709",
            "@type": "uco-observable:ApplicationFacet",
            "uco-observable:numberOfLaunches": 12,
            "uco-observable:applicationIdentifier": "test"
        },
    ]
}
\end{lstlisting}

The CASE project provides Python bindings, though it stores all object
attributes as dictionaries after instantiation. This can be seen in the
example of an \passthrough{\lstinline!ApplicationFacet!} below.
Intuitive usage suggests that the
\passthrough{\lstinline!application\_identifier!} attribute is
accessible through the \passthrough{\lstinline!facet!} object, but it
must instead be accessed as a dictionary key with a non-intuitive name.

\begin{lstlisting}[language=Python]
facet = ApplicationFacet(application_identifier = "test", number_of_launches=3)

# These attributes do not exist

facet.application_identifier
facet.number_of_launches

# The number of launches must be accessed as a dictionary key, which does

# not return a simple integer, but instead returns a dictionary
app_facet['uco-observable:numberOfLaunches']

# -
\end{lstlisting}

AKF's Pydantic-based bindings greatly simplify the declaration of
individual CASE objects while allowing for normal attribute-based
access. For example, AKF's declaration of
\passthrough{\lstinline!ApplicationFacet!} can be seen below:

\begin{lstlisting}[language=Python]
from typing import Optional

from uco import core

class ApplicationFacet(core.Facet):
    numberOfLaunches: Optional[int] = None
    applicationIdentifier: Optional[str] = None
\end{lstlisting}

This definition is only three lines, which is 40 lines shorter than the
declaration of \passthrough{\lstinline!ApplicationFacet!} provided by
the CASE project's existing Python bindings.

A simple CASE bundle, representing the complete contents of a forensic
scenario, can be constructed and exported to JSON-LD as shown below:

\begin{lstlisting}[language=Python]
from caselib import case, uco

bundle = uco.core.Bundle(
    description="An Example Case File",
    specVersion="UCO/CASE 2.0",
    tag="Sample artifacts",
)

url_object = uco.observable.ObservableObject()
url_facet = uco.observable.URLFacet(fullValue="www.docker.com/howto")
url_object.hasFacet.append(url_facet)
bundle.object.append(url_object)

with open("example.jsonld", "wt+") as f:
    data = bundle.model_dump(serialize_as_any=True)
    f.write(json.dumps(data, indent=2))
\end{lstlisting}

\section{Historical declarative
syntaxes}\label{historical-declarative-syntaxes}

While designing the declarative syntax for AKF, the syntaxes of D-FET,
SFX, and the work of Yannikos et al.~were reviewed. For completeness,
brief examples of scenarios in each of these synthesizers are included
here. Notably, none of these works provide details into how the parser
of their declarative languages are implemented, nor the architecture of
the code used to carry out the actions described in the examples below.
This may be attributed to the fact that the scenario creation enabled by
these declarative syntaxes, rather than the syntaxes themselves, was the
primary contribution of these works.

D-FET uses a custom language that does not depend on any existing
text-based languages \cite{williamCloudbasedDigitalForensics2011}:

\begin{lstlisting}
INSTANCE LOAD [Image=WINDOWS2003] 
MOUNT INSTANCE [Disk=STANDARDDISK] AS [Partition="c"] 
ACTIVITY LOAD [Number=12] [Type=JPEG IMAGES; Class=DRUGS] 
    INTO [Folder=USER FOLDER] 
    AT [Period=1 MINUTE] [Interval=INTERVAL] 
    FOR [User=Fred]
\end{lstlisting}

The scenario above will create an instance based on WINDOWS2003 from the
Host Forensics Image library (which contains predefined instances and
are created from OS installation discs). It will then load 12 .jpg
images from the ``DRUGS'' class of images, using the ``STANDARDDISK''
disk image. This creates a host with \emph{predefined}, but not
\emph{timed} activity -- the machine simply begins in this state, rather
than simulating a user doing this over some period of time.

To make activity that can be placed on a timeline, \emph{events} can be
added, such as logging in as a user, deleting files, and then logging
out.

\begin{lstlisting}
INSTANCE LOAD [Image=WINDOWS2003] 
MOUNT INSTANCE [Disk=STANDARDDISK] AS [Partition="c"] 
ACTIVITY LOAD [Number=12] [Type=JPEG IMAGES; Class=DRUGS] 
    INTO [Folder=USER FOLDER] 
    AT [Period=1 MINUTE] [Interval=INTERVAL] 
    FOR [User=Fred]
ACTIVITY EVENT [Event=LOGIN; User=Fred] 
ACTIVITY EVENT [Event=DELETEFILE; User=Fred; File=JPEF IMAGES] 
ACTIVITY EVENT [Event=LOGOUT; User=Fred ]
\end{lstlisting}

SFX uses an XML-based language with tags and attributes that are easily
readable \cite{russellForensicImageDescription2012}:

\begin{lstlisting}[language=XML]
<disk size="512M" alignment="cylinder" diskid="0x12345678"
    <partition index="p1" hidden="0" size="48M" type="vfat"
        <expand archive="part1.zip" /
        <copy from="fake.dat" to="/fake01.dat" /
        <copy from="fake.dat" to="/fake02.dat" /
        <delete from="/Thomas.jpg" /
    </partition
    <partition index="p2" hidden="0" size="48M" type="ntfs"
        <base os="windows7x64" /
        <copy from="fake.dat" to="/fake03.dat" /
        <copy from="fake.dat" to="/fake04.dat" /
        <user username="Gordon"
            <browserhistory browser="firefox"
                <url link="[http://bbc.co.uk"](http://bbc.co.uk") time="13:14:00 1 Jan 2013" /
            </browserhistory
        </user
    </partition
    <partition index="p3" hidden="1" size="64M" type="ntfs"
        <expand archive="part3.zip" /
        <copy from="fake.dat" to="/fake11.dat" /
        <copy from="fake.dat" to="/fake12.dat" /
        <delete from="/docs/image.exe" /
        <slackspace offset="20" file="/tomas.gif" message="This is a secret message" /
    </partition
    <partition index="s1" hidden="0" size="144M" type="ext3"
        <base os="fedora15x64" /
        <expand archive="part2.tar" /
        <copy from="fake.dat" to="/tmp/fake01.dat" /
        <copy from="fake.dat" to="/tmp/fake02.dat" /
    </partition
</disk
\end{lstlisting}

Here, a user is able to define multiple partitions on a single disk,
each with a distinct filesystem that may or may not contain an
underlying filesystem. SFX allows users to generate artifacts in
multiple ways, with each unique application- or OS-specific feature
using a distinct XML element name. Artifact generation features include
copying files to the guest machine in bulk, inserting files in the slack
space of an existing file, and using browser artifacts.

Finally, the work of Yannikos et al.~is particularly notable because it
appears to be fully GUI-based, expecting users to visually construct
Markov chains to define scenarios
\cite{yannikosDataCorporaDigital2014}. An example scenario from
their publication can be seen below:

!\textbf{Pasted image 20250207182912.png}

Although details are relatively limited, each node appears to be a
distinct action that can be automated. Individual nodes accept
parameters that can be used to configure how their associated artifacts
are created. Each ``box'' encompasses a complete Markov chain, with
transitions from one box to another occurring after an unknown condition
is fulfilled. Finally, the diamonds outside the boxes, known as
extensions, are additional libraries that provide additional
functionality during the execution of the overall scenario.



\end{document}

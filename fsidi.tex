% \documentclass[preprint,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
\documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% Fixing DOIs in the bibtex file, which otherwise don't play well with elsarticle.bst
\usepackage{doi}

\journal{Forensic Science International: Digital Investigation}

\begin{document}

\begin{frontmatter}

\title{AKF: A modern synthesis framework for building datasets in digital forensics}

\author[unr]{Lloyd Gonzales}
\author[unr]{Nancy LaTourrette}
\author[unr]{Bill Doherty}
%% Author affiliation
\affiliation[unr]{organization={Department of Computer Science and Engineering, University of Nevada, Reno},%Department and Organization
            addressline={1664 North Virginia Street}, 
            city={Reno},
            postcode={89557}, 
            state={Nevada},
            country={USA}}

%% Abstract
\begin{abstract}
%% Text of abstract
The forensic community depends on datasets containing disk images,
network captures, and other forensic artifacts for education and
research. However, real-world datasets often contain sensitive
information that may be difficult to remove, limiting their use and
distribution. This often leads to the manual development of new datasets
to cover gaps in available datasets. While viable, this approach is
time-consuming and rarely produces datasets that are fully reflective of
real-world conditions. In turn, there is ongoing research into forensic
synthesizers, which automate the process of creating unique, synthetic
datasets that can be publicly distributed without legal and other
logistical concerns.

This work introduces the automated kinetic framework, or AKF, a modular
synthesizer for creating and interacting with virtualized environments
to simulate human activity. AKF makes significant improvements to the
approaches and implementations of prior synthesizers used to generate
forensic artifacts. AKF also improves the process of documenting these
datasets, leveraging the CASE standard to provide human- and
machine-readable reporting to expose dataset features in multiple
formats. Finally, AKF several options for simplifying the process of
using these features to build and document datasets, including a custom
scripting language and generative AI workflows. These contributions are
intended to improve the speed at which synthetic datasets can be created
and ensure the long-term usefulness of AKF-generated datasets and the
framework as a whole.
\end{abstract}

%% Graphical abstract
% \begin{graphicalabstract}
%\includegraphics{grabs}
% \end{graphicalabstract}

%% Research highlights
\begin{highlights}
\item Research highlight 1
\item Research highlight 2
\end{highlights}
    

%% Keywords
\begin{keyword}
%% keywords here, in the form: keyword \sep keyword

%% PACS codes here, in the form: \PACS code \sep code

%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)

\end{keyword}

\end{frontmatter}

%% Beginning of main text
\section{1. Introduction}\label{introduction}

The investigation of cybercrime and other computer-related incidents
requires the collection of forensic datasets, which can broadly be
described as a collection of forensic artifacts. These artifacts are
usually contained in some specific medium, such as a disk image or
volatile memory capture. The analysis of these artifacts is central to
the field of digital forensics, and must be done in such a way that the
resulting evidence and conclusions are valid in a court of law.

From an education and training perspective, these datasets are critical
in introducing new specialists to the field. Datasets in an educational
setting typically cover a range of techniques and tools, allowing
students to practice applying theoretical concepts learned through
lectures and recitations
\cite{adelsteinAutomaticallyCreatingRealistic2005}. Besides the
specific technical skills covered by these scenarios, these datasets aim
to develop the analytical skills needed for students to adapt to
developments in tools and technology
\cite{cooperStandardsDigitalForensics2010}. In other words, students
should be familiar with common tools and patterns in digital forensics,
providing a foundation on which more niche techniques can be learned
\cite{lawrenceFrameworkDesignWebbased2009}.

Similarly, from a research perspective, these datasets serve two
purposes. The first is in improving specific processes in the analytic
step of a forensic investigation. This includes the development of
analysis techniques for niche platforms, direct improvements to existing
techniques, or novel methodologies for performing forensic
investigations for a particular platform. The second is in upholding the
quality of the investigation process as new technologies and tools to
analyze datasets are developed. One notable example is the Computer
Forensics Tool Testing program maintained by the National Institute of
Standards and Technology, which provides a standard methodology and test
corpora for evaluating specific forensic tool capabilities
\cite{nationalinstituteofstandardsandtechnologyComputerForensicsTool2017}.

However, it is challenging to provide hands-on labs that realistically
and comprehensively separate the ideas learned in theoretical courses
\cite{adelsteinAutomaticallyCreatingRealistic2005,guptaDigitalForensicsLab2022,lawrenceFrameworkDesignWebbased2009}
for multiple reasons. Similarly, researchers often have very specific
needs that are not covered by existing datasets. In many cases, this can
be attributed to privacy and legal concerns that limit the distribution
and usage of real-world datasets that would otherwise be suitable for
use. As a result, many researchers and educators alike develop their own
datasets by hand, often with a very narrow scope and limited
reproducibility
\cite{garfinkelBringingScienceDigital2009,grajedaAvailabilityDatasetsDigital2017}.
This is a time-consuming process that responds slowly to changes in
technology in software, and limits the ability of other researchers to
reproduce and validate advancements in the field.

There is a clear need for a more streamlined, reproducible method of
developing scenarios for research and education -- one that is able to
provide the variability and complex content needed for datasets to be
useful in a broad variety of use cases. One such option is the use of
\textbf{forensic synthesizers}, which aim to automate part or all of
dataset creation by programmatically creating forensic artifacts. Over
the past 15 years, there has been significant research into the
development of these synthesizers, each of which produces artifacts
through distinct approaches.

Our work, the \textbf{automated kinetic framework}, or \textbf{AKF},
directly improves upon the foundations provided by prior synthesizers.
It achieves this by adding to the unique contributions of these past
synthesizers through modern techniques and technologies. In doing so, it
aims to provide the foundation of a larger forensic dataset ecosystem --
not only vastly reducing the time spent developing new datasets for
research and education, but also improving the documentation and
discoverability of these datasets. Additionally, through AKF's focus on
long-term viability through its modular architecture, educators and
researchers will be able to rapidly develop relevant datasets, even as
new developments and advancements in technology occur. Our improvements
are intended to address several challenges observed in prior
synthesizers

This paper begins with an analysis of related work in \autoref{related-work}, covering existing dataset sources and the contributions
of prior synthesizers. In \autoref{artifact-generation}, we
introduce AKF's improvements to creating forensic artifacts by directly
modifying disk images and interacting with virtualized environments. In
\autoref{artifact-documentation}, we then describe how AKF documents
these generated datasets with minimal user overhead, providing both
machine- and human-readable options for identifying artifacts of
interest. Finally, in \textbf{\#5. Dataset construction}, we explore how
AKF exposes the ability to generate and document artifacts in an
accessible manner, allowing users of all levels of experience to benefit
from AKF's design principles. We conclude with an evaluation of AKF in
\textbf{\#6 - Results?}, as well a summary of our work and the future
opportunities that exist in \autoref{conclusion-and-future-work}.

\section{2. Related work}\label{related-work}

We begin with a literature review of two topics: publicly available
forensic datasets and the contributions of prior synthesizers. Before we
discuss \emph{how} to automate the creation of forensic artifacts, we
must first cover \emph{what} to automate by reviewing existing datasets
and their identified gaps in \autoref{existing-forensic-corpora}.
This will provide the necessary context for both the need for
synthesizers, as well as the specific gaps that these synthesizers have
gradually filled as discussed in \autoref{analysis-of-prior-synthesizers}.

\subsection{Existing forensic
corpora}\label{existing-forensic-corpora}

Forensic datasets have been available for public use (sometimes by
request) since the early days of the digital forensics field, though
their sourcing and qualities have changed significantly over time. This
topic was explored in far greater detail by Grajeda et al., who
performed a survey of over 700 research articles to identify the various
datasets used throughout the field
\cite{grajedaAvailabilityDatasetsDigital2017}. However, it is still
important to highlight specific datasets relevant to the development of
synthesizers.

Early datasets were primarily derived from real sources, whether made
available to the public or otherwise. The earliest collections of real
datasets include the used hard drives collected by Garfinkel from 1998
to 2006 and the Enron email corpus obtained during the federal
investigation of Enron \cite{garfinkelForensicCorporaChallenge2007}.
Other early real datasets identified by Grajeda et al.~include the
public Apache mailing archive, the Reuters news corpora, and various
facial recognition collections such as the MORPH corpus
\cite{ricanekMORPHLongitudinalImage2006}, all of which were made in
the early to mid-2000s
\cite{yannikosDataCorporaDigital2014,grajedaAvailabilityDatasetsDigital2017}.

A variety of synthetic datasets were also constructed during this early
period. These datasets include the network captures obtained from
simulated attacks conducted by the MIT Lincoln Laboratory from 1998 to
2000 \cite{garfinkelForensicCorporaChallenge2007}, as well as
standalone datasets used for tool validation as part of the early CFTT
program developed by NIST. Other synthetic datasets during this period
were generated as part of challenges, such as those produced for DFRWS
conferences \cite{woodsCreatingRealisticCorpora2011}.

The variety of forensic datasets increased considerably towards the late
2000s, which can be credited to both the overall growth of the field
(including the broader field of incident response) and computing as a
whole. Various notable datasets described by Grajeda et al.~include
malware samples discovered ``in the wild,'' natural language collections
from multiple languages, and file-specific datasets such as collections
of Microsoft Office files. It was also during this time that non-disk
datasets, such as volatile memory dumps and network captures, became
more prevalent. Although not explored by this thesis in detail, mobile
datasets -- such as smartphone disk images, mobile malware and
applications, and SIM card images -- also grew more prevalent.

Many of these datasets were not maintained as part of a larger
collection with the explicit intent of providing them for digital
forensics research. This began to change towards the late 2000s; for
example, Garfinkel's collection of disk images eventually evolved into
the Real Data Corpus, growing to 30 terabytes by 2013
\cite{garfinkelBringingScienceDigital2009,yannikosDataCorporaDigital2014}.
The collection included disk images, flash drive images, and a variety
of optical discs sourced from real-world usage, requiring institutional
review board approval to use. This collection would eventually be part
of the Digital Corpora platform, which includes a set of purely
synthetic datasets. Separately, NIST began developing the CFTT and
CFReDS projects, both of which provide forensic datasets for various
purposes. In 2021, Xu et al.~compiled and published a repository of
educational datasets that explicitly focuses on ease of use, realism,
and breadth \cite{xuDesigningSharedDigital2022}. All corpora
mentioned in this paragraph are currently actively maintained.

There are other datasets that are relatively unique and are often
maintained as part of a larger niche collection. Collections of malware
samples have grown significantly, in part because of the modern threat
intelligence ecosystem supported by platforms such as VirusTotal. There
exist datasets focused on the dark web, including datasets for services
operating on the Tor network and ``black market'' sites on which illegal
goods are bought and sold. Finally, there are also network captures from
various novel sources, such as iterations of the Collegiate Cyber
Defense Competition and the networking infrastructures of university IT
departments \cite{grajedaAvailabilityDatasetsDigital2017}.

These existing datasets span a wide range of technologies -- including
different versions of the same technology -- that require distinct
methodologies to analyze effectively. However, the field continually
requires new datasets that reflect current advancements in technology,
such as updated operating systems or new applications. For example,
instant messaging applications have changed considerably over the
history of the field, ranging from MSN Messenger in the early 2000s to
Skype, Discord, Telegram, Signal, Slack, and more. Artifacts from these
applications must be handled differently, even if the value of the
underlying service is essentially the same. Similarly, the strategies
for analyzing Windows artifacts have changed significantly from version
to version, as new registry keys become relevant in analyzing
applications while others become unused.

This gradual ``aging'' of datasets, in which their relevance degrades
over time, contributes to the continuous need for new datasets. Indeed,
the need for new, novel datasets is one of the reasons identified by
Grajeda et al.~for the manual development of new datasets by research
authors; it was often the case that a modern dataset simply did not
exist for their needs. The other motivation is that the dataset used was
never made public, either due to the author not having the resources to
distribute the dataset themselves or because of legal and privacy
concerns.

How are these datasets relevant to the development of synthesizers? The
datasets throughout this section have demonstrated that many datasets
have proved to be relevant in digital forensics research, even if not
immediately evident. In turn, any effort to streamline the development
of forensic datasets should be able to cover as many use cases as
possible. Synthesizers must fulfill two requirements to achieve this:

\begin{itemize}
\item
  A synthesizer should be able to generate artifacts present in existing
  datasets, provided that the underlying technologies are still
  available.
\item
  A synthesizer should be able to account for developments in operating
  systems, applications, or other technologies without requiring
  significant changes to the underlying architecture.
\end{itemize}

Indeed, the synthesizers described in the following section have
explicitly addressed these two concerns. For example, many of these
synthesizers have focused on implementing features that reflect the
qualities of real-world datasets. These features include the ability to
execute malware samples in a virtualized environment, send emails to
arbitrary email servers, and insert data on removable drives -- all of
which generate forensic artifacts present in previously used datasets.
Similarly, modern synthesizers are capable of generating disk images,
network captures, and volatile memory dumps, in addition to extracting
specific artifacts such as application-specific files. The gradual
progression in the ability of synthesizers to generate artifacts is
described in \textbf{39.2 - Literature review\#2.2 - Analysis of
existing synthesizers}, which explores specific contributions made by
each synthesizer.

Of note is the generation of datasets that are derived from human
interactions, such as public email distribution lists, photographs of
human faces, and transcripts of conversations. While prior synthesizers
have not explored this in depth, it is briefly addressed as part of the
generative AI work done as part of AKF in \textbf{39.6 - Building
scenarios}.

The second issue, in which synthesizers must be extensible in such a way
that they can support new applications, has been approached in several
ways. This is described in greater detail in \textbf{39.3 - Architecture
and design} and has been a significant design consideration in the
development of most synthesizers.

\subsection{Analysis of prior
synthesizers}\label{analysis-of-prior-synthesizers}

Numerous frameworks have been built over the last two decades that aim
to significantly reduce the effort involved in creating synthetic images
from scratch by automating various application- and OS-specific actions
according to provided instructions. (This is a subset of broader
automation efforts throughout digital forensics, as described by
Michelet et al. \cite{micheletAutomationDigitalForensics2023}.) The
functionality and availability of the frameworks described throughout
the literature have varied considerably over time. However, the goal of
these frameworks has largely remained consistent: they all aim to
provide a rapid method for instructors to develop forensic labs for
students.

\textbf{Forensig2}, described by Moch and Freiling in 2009 and revisited
in 2012, appears to be the first detailed description of a forensic
synthesizer
\cite{mochForensicImageGenerator2009,mochEvaluatingForensicImage2012}.
Users define scenarios through a Python 2 library that provides
abstractions around various virtual machine operations, such as
formatting disks, creating partitions, and copying files from the host
to the virtual machine. Actions are performed live through a Qemu-based
VM, eventually producing a ground truth report and (effectively) an
image to be analyzed by students. The source code for Forensig2 is not
currently maintained and appears to be unavailable.

The vast majority of later synthesizers are similar to Forensig2 in that
they are all implemented in Python, providing users with a Python
library to define and generate artifacts. Notable exceptions include the
\textbf{Digital Forensic Evaluation Test (D-FET)} platform
\cite{williamCloudbasedDigitalForensics2011}, the \textbf{Summarized
Forensic XML (SFX)} language
\cite{russellForensicImageDescription2012}, and the work described
by \textbf{Yannikos et al.} \cite{yannikosDataCorporaDigital2014}.
Each of these synthesizers use custom languages for creating datasets,
providing abstractions around synthesizer functionality.

A brief summary of the notable aspects of the remaining Python-based
frameworks -- many of which were identified by Göbel et al.~-- are
described below:

\begin{itemize}
\item
  \textbf{ForGeOSI} \cite{maxfraggMaxfraggForGeOSI2023} introduced
  the use of the VirtualBox SDK to automate various operations, forming
  the basis for much of the work done as part of VMPOP.
\item
  \textbf{ForGe} \cite{vistiAutomaticCreationComputer2015} is
  specifically designed to generate NTFS and FAT32 images, focusing on
  placing data directly onto disk images without a virtual machine by
  maintaining and serializing custom data structures for supported
  filesystems.
\item
  \textbf{EviPlant} \cite{scanlonEviPlantEfficientDigital2017}
  encompasses a novel method for efficiently distributing generated disk
  images, which is achieved by generating and distributing differential
  ``evidence packages'' to apply to a base image file. An OS-specific
  injection tool creates relevant artifacts according to the evidence
  package. Since the base image must only be downloaded once, each
  reconstructed disk image is significantly smaller than if distributed
  as standalone images.
\item
  \textbf{VMPOP} \cite{parkTREDEVMPOPCultivating2018} provides an
  architecture for elaborate VirtualBox control (such as attaching USB
  devices and starting video captures) in addition to various
  OS-specific commands. Provided routines for Windows include creating
  restore points, installing programs, mapping network drives, setting
  registry values, and more. Although the architecture as a whole is
  platform-independent, the provided implementations operate with
  VirtualBox and Windows. \textbf{TraceGen}
  \cite{duTraceGenUserActivity2021} is similar in that it makes use
  of the VirtualBox API for various operations but is more like hystck
  in its use of a Python-based agent to carry out application-specific
  actions.
\item
  \textbf{hystck} \cite{gobelNovelApproachGenerating2020} provides
  routines for automating OS- and application-specific commands through
  both YAML configuration files (passed through an intermediate
  interpreter script) and/or Python scripts (executed normally). The
  framework produces network captures and disk images; similar to
  EviPlant, it supports ``differential'' images that can be distributed
  and applied to ``template'' images.
\item
  \textbf{ForTrace} \cite{gobelForTraceHolisticForensic2022} is the
  most recently developed synthesizer, which directly builds upon hystck
  by providing volatile memory captures (alongside disk and network
  captures) in addition to various other new features (such as the
  ability to execute PowerShell scripts to create Windows artifacts),
  with a focus on a modular architecture. A variant focusing on Android
  artifact generation was developed in 2024
  \cite{demmelDataSynthesisGoing2024}.
\end{itemize}

Clearly, significant work has been done to streamline the process of
developing images, although the availability and functionality of each
framework vary greatly. Notably, with the exception of ForTrace and
VMPOP, none of these synthesizers are direct extensions of prior works,
implying that the framework authors' needs could only be met by
developing new frameworks from scratch. This is not directly stated in
any of these works, with the exception of hystck
\cite{gobelNovelApproachGenerating2020}. However, it can be
reasonably concluded that the limited maturity, availability, and
maintenance of prior works contributed to the independent development of
most frameworks.

There are several possible motivations for developing entirely new
codebases instead of extending existing synthesizer. One reason could be
that several synthesizers are not open source and thus cannot easily be
extended, as is the case with Forensig2
\cite{mochForensicImageGenerator2009} and TraceGen
\cite{duTraceGenUserActivity2021}. Another reason is that the focus
of certain synthesizers results in an architecture that is simply
incompatible with the goals of newer works. For example, ForGe's
architecture \cite{vistiAutomaticCreationComputer2015} focuses
largely on direct filesystem manipulation to generate forensic artifacts
and is unsuitable for a synthesizer requiring virtualization. Similarly,
synthesizers that exclusively leverage agentless artifact generation as
described in \autoref{artifact-generation}, such as VMPOP
\cite{parkTREDEVMPOPCultivating2018}, require significant
architectural changes to support agent-based artifact generation.

However, perhaps the largest motivation for constructing new
synthesizers is the lack of ongoing support for virtually all
synthesizers. It appears that no synthesizer has gained significant
traction within the broader forensic community, possibly with the
exception of ForTrace; for the synthesizers that \emph{are} open source,
none are under active development and maintenance. Additionally, the
forensic datasets generated by these synthesizers have not seen
significant adoption in either education or research; many instructors
continue to use the human-generated datasets available on public
platforms.

The inflexibility of prior synthesizers, combined with the overall lack
of support and success of synthesizer-based datasets, contributes to the
lack of shared codebases. However, this is not to say that the
individual contributions of each prior synthesizer cannot be merged into
a single project that resolves many of the architectural barriers that
have limited the adoption and extension of existing synthesizers. We now
move to a discussion of the three main areas in which AKF makes
significant Improvements: artifact generation, artifact documentation,
and dataset creation.

\section{3. Artifact generation}\label{artifact-generation}

Each of the synthesizers described in \textbf{39.2 - Literature
review\#2.2 - Analysis of existing synthesizers} takes one of three
approaches to artifact generation, as partly described by Scanlon et al.
\cite{scanlonEviPlantEfficientDigital2017}:

\begin{itemize}
\item
  \textbf{Physical}: No virtualization of software or hardware ever
  occurs; data is written directly to the target medium, such as a disk
  image or virtual hard drive.
\item
  \textbf{Agentless logical}: The synthesizer interacts with a live VM
  to generate artifacts. Interaction is achieved without the need for
  custom software to be installed on the VM; instead, actions are
  achieved using the hypervisor itself or a remote management tool
  native to the virtualized operating system.
\item
  \textbf{Agent-based logical}: The synthesizer interacts with a
  dedicated client, or agent, on a live VM to carry out actions. The VM
  must have the agent installed before any interaction can occur.
\end{itemize}

These three approaches are not mutually exclusive within a single
synthesizer, though many prior synthesizers have supported only one
approach to generate artifacts. \autoref{tbl:prior-techniques} below
denotes the approaches used by each of the synthesizers previously
discussed. Where source code is unavailable, a best effort was made to
identify the approach used by a particular synthesizer based on its
published paper, if one exists; otherwise, the entire row contains
question marks.


\begin{table}[t]
\centering
\begin{tabular}{l l l l}
  \textbf{Synthesizer} & \textbf{Physical} & \textbf{Agentless} & \textbf{Agent-based} \\ \hline
  \textbf{FALCON} \cite{adelsteinAutomaticallyCreatingRealistic2005},
  2005 & ? & ? & ? \\
  \textbf{CYDEST} \cite{bruecknerAutomatedComputerForensics2008}, 2008
  & ? & ? & ? \\
  \textbf{Forensig2}
  \cite{mochForensicImageGenerator2009,mochEvaluatingForensicImage2012},
  2009 & Yes (filesystem mounting; filesystem-independent editing) & Yes
  (over SSH only) & No \\
  \textbf{D-FET} \cite{williamCloudbasedDigitalForensics2011}, 2011 &
  Yes (filesystem mounting; filesystem-independent editing) & No & No \\
  \textbf{SFX} \cite{russellForensicImageDescription2012}, 2012 & Yes
  (filesystem mounting) & No & No \\
  \textbf{Yannikos et al.} \cite{yannikosDataCorporaDigital2014}, 2014
  & ? & ? & ? \\
  \textbf{ForGeOSI} \cite{maxfraggMaxfraggForGeOSI2023}, 2014 & No &
  Yes (hypervisor interfaces) & No \\
  \textbf{ForGe} \cite{vistiAutomaticCreationComputer2015}, 2015 & Yes
  (filesystem-aware editing) & No & No \\
  \textbf{ForGen} \cite{jjk422Jjk422ForGen2019}, 2016 & No & No &
  No \\
  \textbf{EviPlant} \cite{scanlonEviPlantEfficientDigital2017}, 2017 &
  Yes (filesystem-independent editing) & No & Yes (unknown mechanism) \\
  \textbf{VMPOP} \cite{parkTREDEVMPOPCultivating2018}, 2018 & No & Yes
  (hypervisor interfaces) & No \\
  \textbf{hystck} \cite{gobelNovelApproachGenerating2020}, 2020 & No &
  No & Yes (Python agent) \\
  \textbf{TraceGen} \cite{duTraceGenUserActivity2021}, 2021 & No & No
  & Yes (unknown mechanism) \\
  \textbf{ForTrace} \cite{gobelForTraceHolisticForensic2022}, 2022 &
  No & No & Yes (Python agent) \\
\end{tabular}
\caption{Summary of artifact generation techniques in prior synthesizers}\label{tbl:prior-techniques}
\end{table}


There are advantages and disadvantages to each approach, in addition to
requiring distinct implementation techniques for each. Although AKF
makes improvements in all three techniques, we only discuss physical and
agentless generation, as they are where AKF makes the largest
improvements. More specifically, this section addresses the
functionality of the action automation library
(\passthrough{\lstinline!akflib!}) to generate artifacts by either
interacting with a live virtual machine or by directly editing disk
images stored on the host.

\subsection{Physical generation}\label{physical-generation}

Physical artifact creation encompasses any technique in which the
virtualization of an operating system is not used to generate artifacts.
This allows the synthesizer to bypass the operating system or related
software that could lead to undesirable non-deterministic behavior. This
is sometimes called \emph{simulating} the creation of artifacts rather
than \emph{virtualizing} their creation.

For example, a scenario developer may want to guarantee that a
particular deleted file is partially overwritten by another file,
ensuring that the deleted file is recoverable from the slack space of
the newly placed file. However, it is extremely difficult to force the
reuse of the same physical disk clusters from a userspace application.
Operating systems rarely expose low-level filesystem functionality to
applications; furthermore, operating systems are still subject to
hardware drivers that regularly rearrange physical space, such as those
that engage in wear leveling. In turn, it is sometimes necessary to
bypass the operating system to reliably place data on a disk.

There are three primary techniques for physical artifact planting
implemented by prior synthesizers. These are:

\begin{itemize}
\item
  \textbf{Filesystem mounting}, as done by Forensig2
  \cite{mochForensicImageGenerator2009} and SFX
  \cite{russellForensicImageDescription2012}, in which the
  filesystem is mounted to the host and edited directly.
\item
  \textbf{Filesystem-independent direct editing}, as done by EviPlant
  \cite{scanlonEviPlantEfficientDigital2017}, in which edits to
  specific physical addresses on the disk image are made without any
  parsing or knowledge of the underlying filesystem.
\item
  \textbf{Filesystem-aware direct editing}, as done by ForGe
  \cite{vistiAutomaticCreationComputer2015} and EviPlant
  \cite{scanlonEviPlantEfficientDigital2017}, in which filesystem
  data structures are parsed to determine the physical address(es) of
  the disk image to write to. (It is unclear how EviPlant achieves this,
  as its source code is not available.)
\end{itemize}

AKF supports all three to varying degrees, as implemented in the opaque
submodules indicated in \autoref{fig:action-physical}.

\begin{figure}[htbp]
\centering
\includegraphics[width=1\linewidth]{action-physical.png}
\caption{Abridged submodule diagram for physical artifact
creation}\label{fig:action-physical}
\end{figure}

However, AKF makes the largest improvements in filesystem-aware direct
editing over prior synthesizers. ForGe
\cite{vistiAutomaticCreationComputer2015} implements its physical
artifact generation by implementing NTFS and FAT32 data structures in
Python, allowing it to create a fully virtual representation of these
filesystems (with the assistance of a custom C program). This
virtualized filesystem provides ForGe with the information necessary to
efficiently insert data into known slack and unallocated space while
maintaining filesystem consistency. While extremely powerful (and
implements a valuable feature not found in any other synthesizer to
date), it is inflexible in two specific aspects:

\begin{itemize}
\item
  ForGe does not provide a generic interface for the filesystems it
  supports. Although the NTFS and FAT32 wrappers provide the same
  methods with the same signatures, this is not strictly enforced by a
  parent class. The lack of a generic interface means that the
  functionality supported across all filesystems is unclear, as is the
  functionality that must be implemented for new filesystems to be
  compatible with ForGe.
\item
  ForGe lacks a ``frontend'' to support arbitrary disk types, regardless
  of the underlying filesystem. ForGe does not support multi-partition
  disks or common non-raw disk formats such as VHD, VMDK, or VDI.
\end{itemize}

Read-only libraries addressing these two issues have been in development
since the introduction of ForGe but have not been integrated into other
synthesizers to achieve the same write capabilities as ForGe. Two such
libraries are \passthrough{\lstinline!libtsk!} (also known as The Sleuth
Kit), the C++ library that powers the open-source digital forensics
software Autopsy \cite{SleuthkitSleuthkit2025}, and
\passthrough{\lstinline!libyal!}, a collection of libraries for
analyzing formats not supported by \passthrough{\lstinline!libtsk!}.
These libraries eventually supported the development of
\passthrough{\lstinline!dfvfs!}, or the Digital Forensics Virtual File
System, a Python library that leverages \passthrough{\lstinline!libtsk!}
and multiple libraries from the \passthrough{\lstinline!libyal!} project
to provide a generic interface for analyzing a variety of disk image
formats and filesystems \cite{Log2timelineDfvfs2025}.

AKF uses \passthrough{\lstinline!dfvfs!} to locate the clusters of a
file at a known path in a filesystem, which can then be used to identify
the start of slack space within the file's clusters (as well as
unallocated space in the filesystem). By adding the physical offset of
the cluster within the filesystem to the offset of the filesystem's
partition in the disk image, AKF can write to the exact location of a
file's slack space using the offset-based method described earlier. This
achieves feature parity with ForGe by completely delegating filesystem-
and image-specific details to \passthrough{\lstinline!dfvfs!}, enabling
a filesystem-independent method for locating slack space.

More generally, this technique provides a deterministic method for
inserting data within the slack space of a filesystem, simulating the
deallocation of a file and its partial replacement with a known file.
However, this does not fully simulate the process of deleting a file
through a running operating system and having a new file replace the
deallocated clusters; naturally, this does not generate OS-specific
artifacts associated with deleting and creating files and fails to
generate the filesystem artifacts that could exist with the original
file (such as the ``deleted'' file's name in an NTFS master file table).
Future work in the field could address this gap by combining
\passthrough{\lstinline!dfvfs!} with additional technologies to improve
the accuracy of physical techniques.

\subsection{Agent-based generation}\label{agent-based-generation}

\textbf{Agent-based artifact creation} involves the use of a dedicated
executable on the VM that serves as an interface between the host
machine and the guest machine. This program runs commands natively on
the virtual machine on behalf of the host machine, accepting commands
over a dedicated network interface. This allows for greater flexibility
and more complex actions to be taken when compared to agentless
approaches. In particular, it allows application-specific functionality
to be implemented using existing automation frameworks such as Selenium
\cite{SeleniumHQSelenium2025}, Playwright
\cite{MicrosoftPlaywrightpython2025}, and PyAutoGUI
\cite{sweigartAsweigartPyautogui2025}.

Relevant AKF submodules for agent-based generation are depicted as
opaque elements in \autoref{fig:action-agent}.

\begin{figure}[htbp]
\centering
\includegraphics[width=1\linewidth]{action-agent.png}
\caption{Abridged submodule diagram for agent-based artifact
creation}\label{fig:action-agent}
\end{figure}

Agent-based artifact creation is the approach taken by hystck/ForTrace
\cite{gobelNovelApproachGenerating2020,gobelForTraceHolisticForensic2022},
which refers to its agent as an ``interaction manager.'' Because
ForTrace provides extensive agent functionality and is by far the most
mature synthesizer, AKF's agents borrow heavily from ForTrace's
approach. However, AKF improves upon ForTrace by solving two specific
issues:

\begin{itemize}
\item
  Commands in ForTrace are sent through a simple string-based protocol.
  In particular, it is challenging to send complex Python objects as
  arguments or return values since these objects often cannot be easily
  serialized to a string without loss of information. An example
  relevant to AKF is passing Playwright browser objects, which contain
  an internal state that is difficult to extract and reconstruct using
  strings alone.
\item
  ForTrace depends on Python's runtime introspection to discover the
  correct modules and functions to call based on the contents of a
  command string, importing these modules during runtime. While this is
  a valid approach, it is more complex (and difficult to follow) than
  the approach taken by AKF.
\end{itemize}

AKF resolves these issues through the use of RPyC, a library for
symmetric remote procedure calls, for agent communication
\cite{TomerfilibaorgRpyc2025}. Although the RPyC protocol is
symmetric, it is often used in typical client-server architectures to
allow clients to manipulate remote Python objects as if they were local
objects, as well as invoke remote (server) functions using local
(client) parameters. Delegating the serialization and deserialization of
complex objects to RPyC allows us to perform complex operations that
would have been difficult to implement with the simple string-based
protocol of ForTrace.

In ``new-style'' RPyC, this is achieved by running a \emph{service} on
the device where remote operations should be performed. Services expose
a set of functions and attributes that may be accessed remotely by an
RPyC client, listening on a specified TCP port for requests to access
these exposed elements. Clients access these functions and attributes by
name as if they were local objects; arguments passed to functions are
serialized and deserialized in the background, as are the results of
function calls and attribute accesses.

AKF's application-specific functionality is divided into individual RPyC
``subservices'' created on demand. These subservices implement
automation support for a specific application or group of actions, and
are analogous to the agent-side code of individual ForTrace modules. The
agent's main loop is itself a ``root'' RPyC service that is responsible
for creating and destroying these subservices upon request; all
subservices are known to the root service at initialization, eliminating
the need to perform runtime introspection to find application-specific
modules.

From an implementation and usability perspective, this design provides
three significant improvements over the ForTrace protocol. First, the
routing of functions is wholly delegated to RPyC. Instead of manually
constructing a message with the function name and its associated
parameters (as strings) over the network, the process of serializing
parameters and routing them to the correct function call is abstracted
away by RPyC.

Second, this allows us to pass and return arbitrarily complex objects
(for which we do not have to manually write the serialization and
deserialization logic). When passing complex objects from the agent to
the server or vice versa, a reference to the object is sent over the
network and wrapped by a \emph{proxy object}, which behaves like the
original object \cite{TheoryOperationRPyC}. Importantly, it is
usually not necessary to distinguish between local and remote/proxy
objects of the same type when writing code, which eliminates the extra
complexity of using proxies.

Finally, the ability to interact with complex remote objects allows us
to significantly reduce the actual code written as part of the API
exposed to the host. For example, there is no need to implement a
wrapper for every method available as part of a Playwright page object;
instead, a reference to the Playwright object \emph{running on the
virtual machine} can be given to the host machine. Instead of writing
individual methods for opening pages, navigating to specific elements,
and so on, we can simply use the methods that already exist in the
Playwright object -- any local calls on the host's proxy object will
lead to remote outcomes on the host, as desired. This, of course, does
not preclude the ability to write convenience methods for more complex
actions requiring the Playwright object.

Together, these three features significantly simplify the process of not
only implementing and managing support for individual applications, but
also the actual use of the agent. Simplified application-specific
support makes it easier for the community to extend AKF. Furthermore,
this modular approach to application-specific support makes it possible
to add or remove functionality from the agent with minimal effort. Since
not all automation frameworks will be available on every platform or
operating system, these ``subservices'' allow us to ensure support for
as many different platforms as possible with minimal changes to the
underlying codebase.

The list of subservices supported by the AKF Windows agent is described
in \autoref{tbl:akf-applications} below. Although only three subservices
are implemented, each subservice is an example of a distinct design
pattern that could be easily adapted to implement other
application-specific functionality. (Support for specific applications,
such as Thunderbird or Firefox, has already been explored and
implemented in prior works.)


\begin{table}[t]
\centering
\begin{tabular}{l l l}
  \textbf{Subservice} & \textbf{Dependencies} & \textbf{Features} \\ \hline
  \passthrough{\lstinline!autogui!} & PyAutoGUI
  \cite{sweigartAsweigartPyautogui2025a} & Hypervisor-independent
  mouse and keyboard control, as well as other PyAutoGUI features \\
  \passthrough{\lstinline!artifacts!} & Windows-Prefetch-Parser
  \cite{wittPoorBillionaireWindowsPrefetchParser2025} & Collection of
  Windows artifacts and conversion to corresponding CASE objects \\
  \passthrough{\lstinline!chromium!} & Playwright
  \cite{MicrosoftPlaywrightpython2025} & Automated webpage browsing;
  also allows for performing complex actions such as completing forms and
  clicking links based on HTML selectors \\
\end{tabular}
\caption{Implemented subservices for the AKF Windows agent}\label{tbl:akf-applications}
\end{table}


A generic hypervisor interface is used to support agent discovery and
communication. To avoid polluting network captures with agent-related
packets, virtual machines are expected to use a NAT adapter for Internet
communications and a ``maintenance'' host-only adapter for
agent-specific communications. In turn, hypervisor-specific
implementations must expose the ability to discover the IP address of
the host-only adapter. This allows AKF scripts to communicate with the
root RPyC service and any subservices over the host-only adapter,
concealing them from network captures on the NAT adapter.

\section{4. Artifact documentation}\label{artifact-documentation}

There exists a gap in the ability of instructors and researchers to
perform bulk searches for specific forensic artifacts in public
datasets. For example, the NIST CFReDS repository
\cite{nationalinstituteofstandardsandtechnologyCFReDSPortal}, one of
the largest listings of forensic datasets, does not have a unified
standard for describing uploaded images. Although users can search by
keywords and human-applied tags, metadata is not available in a
standardized format that can be programmatically queried.

For many datasets, an instructor or researcher must read through a PDF
answer key (if one exists) or analyze the image themselves to determine
if a particular artifact is present. Answer keys are not inherently
machine-readable and are not suited for identifying specific artifact
types in bulk. Additionally, the content of human-made reports may be
limited to what the author believes is significant, even if other
artifacts of interest are present in the image. In turn, it may be
difficult to quickly determine if a dataset is useful in demonstrating a
particular technique to students or validating a specific feature of a
newly developed tool.

This section describes not only our approach to providing machine- and
human-readable reporting for AKF-generated datasets, but also AKF's role
in providing a foundation for reproducible research.

\subsection{CASE bundles}\label{case-bundles}

A rigid, well-defined format for ground truth is invaluable to
researchers engaging in tool validation and development. We identified
CASE, developed by Casey et al., as the best format to fit these needs
\cite{caseyAdvancingCoordinatedCyberinvestigations2017}. CASE is a
vendor-neutral format designed to document both technical and
non-technical information about a digital forensics case. It aims to
provide as many definitions for OS-specific and application-specific
artifacts as possible while still providing the flexibility to describe
artifacts from uncommon applications. These definitions are written in
the Terse RDF Triple Language, or Turtle, which expresses object
attributes and types in a plain text format. Instances of these objects
are expressed in a CASE ``bundle'', which is typically serialized to a
format like JSON-LD.

Because the CASE format itself is language-agnostic, it is necessary to
write language-specific libraries that allow for instantiating CASE
objects. At the time of writing, the CASE project provides official
Python bindings for CASE version 1.4
\cite{CaseworkCASEMappingPython}. Each unique object type is
represented as a Python class, which can be instantiated to produce
individual objects. However, this library has several limitations,
particularly the need to manually maintain these definitions due to the
instantiation and serialization logic contained in each class.

AKF contributes and leverages its own bindings for CASE. Its foundation
is the Pydantic library for Python, which allows developers to easily
define classes with typed attributes based on Python type hints
\cite{colvinPydantic2024}. This allows us to vastly simplify the
declaration of individual CASE objects while providing runtime type
validation and automatic casting. More importantly, this simplicity
allows us to automatically generate our Python bindings directly from
the Turtle definitions. This is particularly relevant when considering
the active development of CASE version 2.0, which has significant
differences from version 1.4.

Various functions and classes throughout the AKF core libraries and
agent API accept an optional CASE bundle when invoked or instantiated.
As CASE-compatible functions are called, they can automatically add CASE
objects corresponding to the artifacts generated through their
execution, as depicted in \textbf{FIGURE HERE}. For example, if the
agent subservice API for automating Chromium browser actions is provided
with a CASE bundle, navigating to a page using the API could
automatically generate a CASE object describing the page visit and add
it to the bundle. This process can occur entirely within the host,
allowing CASE-related logic to remain out of the agent where needed.

Other functions are wholly dedicated to the creation of CASE objects
based on runtime analysis. For example, it is possible to create Windows
prefetch objects during artifact generation. However, these objects are
likely to become outdated if their corresponding applications are
launched later in the scenario, thus changing the content of the
prefetch files and making the existing CASE objects inaccurate. In turn,
the \passthrough{\lstinline!artifacts!} subservice mentioned in
\autoref{agent-based-generation} can collect Windows prefetch
files immediately before a disk image is created, allowing it to
construct CASE prefetch objects that reflect the disk image without
requiring separate tooling. CASE-oriented functionality can also be
implemented in existing subservices; for example, the
\passthrough{\lstinline!chromium!} subservice can create CASE objects
for Chrome and Edge browser history after all browser automation actions
have been performed.

The flexibility of these two approaches -- enabled by CASE's deep
integration into AKF -- makes it possible to construct CASE objects in a
manner that requires little additional effort by scenario developers. In
most cases, scenario developers do not need to be concerned with
instantiating their own CASE objects when using high-level APIs so long
as an AKF library developer has written support for automatic CASE
object construction. This significantly reduces the need for scenario
developers to construct ground truth information by manually analyzing
synthesizer-created outputs.

By extension, this means that the detailed documentation of AKF outputs
is innate to many scenarios constructed using AKF. Lowering the effort
required to document an AKF-generated scenario improves the likelihood
that any public AKF scenario can be immediately valuable (or determined
to be valuable) to researchers and educators. This significantly
contributes to AKF's goal of supporting an ecosystem around its images;
the CASE bundles of many scenarios can be queried in bulk to identify
datasets that might be useful for a specific purpose without having to
download the dataset itself. This information can also be used to
identify and analyze broader trends across scenarios, such as the
frequency of a particular artifact appearing in all Windows datasets.

While this machine-readable reporting significantly improves the ability
of the forensic community to locate useful datasets, it is verbose and
unsuitable as a human-readable summary. Human-readable reporting is
particularly relevant in a classroom setting, where the distribution of
simplified answer keys to graders and students focusing on key artifacts
is preferable to the exhaustive reporting provided by a CASE bundle.
This leads us to \autoref{pdf-reporting}, which briefly addresses
the conversion of AKF-generated metadata into human-readable reports.

\subsection{PDF reporting}\label{pdf-reporting}

As alluded to at the beginning of this section, converting a rigid,
well-defined format to a human-readable format is often easier than
performing the reverse operation. Indeed, this is the approach taken by
AKF, which does not create human-readable reports as an immediate output
of dataset generation. (AKF generates human-readable log files during
artifact generation, but these are unstructured and are created
primarily for debugging rather than analysis.) Instead, AKF supports a
simple yet flexible system for generating human-readable PDF reports
from existing CASE bundles after generating a dataset.

AKF implements human-readable reporting through a set of ``renderers,''
which focus on analyzing specific artifacts found in a CASE bundle and
generating human-readable content. Each renderer accepts a complete CASE
bundle and extracts CASE objects of supported types; the renderer then
uses the information contained in these objects to generate a Markdown
document, which can include formatted text, tables, images, and other
elements that may be useful to a human. The results of each renderer are
combined to form a larger Markdown document (or documents) with multiple
sections, one for each renderer. The combined document can be converted
to a PDF using Pandoc \cite{macfarlanePandoc2025}, a general-purpose
tool for converting between documents of various types. Users can modify
the generated Markdown documents before running Pandoc, if desired.

A single CASE bundle can be passed through as many or as few renderers
as needed to generate a suitable report for a dataset, as depicted in
\textbf{FIGURE HERE}. So long as the original CASE bundle is available,
users can reanalyze datasets with arbitrary renderers; this means that
dataset reports can be regenerated with as much detail as a user needs
for a specific use case. Furthermore, if new renderers are developed for
artifacts that are present in older datasets, the human-readable report
can be regenerated to include these artifacts.

This modular, ``evergreen'' approach to reporting allows these reports
to be interpreted as a focused snapshot of what a dataset contains.
Importantly, this can be done without compromising the dataset itself;
the CASE bundle remains the single, comprehensive source of truth.
Contrast this with human-written PDF reports, which may contain human
biases and are rarely maintained in older datasets.

\autoref{fig:scenario-report} shows a page taken from a sample report.
The information contained in this report was derived from collected
\passthrough{\lstinline!WindowsPrefetch!} and
\passthrough{\lstinline!URLHistory!} CASE objects, which were then
passed through their associated renderers. Each renderer generated
Markdown documents containing the information in the report, which were
then combined and converted to a PDF with Pandoc. (Note that
\autoref{fig:scenario-report} uses the Eisvogel template
\cite{waglerWandmalfarbePandoclatextemplate2025} for Pandoc,
significantly improving the appearance and readability of generated
documents. Eisvogel is not distributed with AKF but can be manually
installed alongside Pandoc; \passthrough{\lstinline!akflib!} will use
Eisvogel if it is detected.)

\begin{figure}[htbp]
\centering
\includegraphics[width=1\linewidth]{human-reporting.png}
\caption{Sample PDF report generated by AKF
renderers}\label{fig:scenario-report}
\end{figure}

Of course, other options exist for generating human-readable (and
machine-readable) reporting from an AKF-generated dataset. For example,
the analysis features provided by tools such as Plaso
\cite{Log2timelinePlaso2025} and those developed by Eric Zimmerman
\cite{zimmermanEricZimmermansTools} can be used to supplement the
existing reporting and validation features of AKF. These tools can be
directly integrated into AKF workflows in the future, although this is
not currently the case.

After generating the scenario itself and any metadata and reporting that
should be included with the dataset, the challenge of distributing this
information remains. More precisely, how do we make our dataset as
accessible, reusable, and discoverable as possible? \#\# 4.3 -
Reproduciblity

A key challenge identified by Grajeda et al.~was the difficulty in
reproducing results in the field of digital forensics. While this is
primarily attributed to the \emph{availability} of forensic datasets in
general, it can also be attributed to challenges in the
\emph{reproducibility} of creating synthetic datasets.

Before addressing the low-level use of AKF as part of \textbf{39.6 -
Building scenarios}, we will briefly discuss the infrastructure needed
to support community usage of the outputs of AKF scenarios and synthetic
datasets as a whole. Note that for the remainder of this section,
scenarios and datasets are both implied to be synthetic, as the
principles of reproducibility are less applicable to real-world
datasets.

There are four elements that must be distributed with a scenario to make
a dataset (and its results) reproducible:

\begin{itemize}
\item
  Any core outputs or individual artifacts generated from the virtual
  machine.
\item
  Any metadata, ground truth, or other reporting that describes the
  scenario.
\item
  The OS-specific ``base image'' used to create the dataset, typically a
  virtual machine with a newly installed operating system on which all
  synthesizer actions are performed.
\item
  The precise instructions required to build the scenario from the
  provided base image, whether human- or machine-readable instructions.
\end{itemize}

Forensic datasets have long included core outputs and individual
artifacts well before the development of AKF and other synthesizers;
there is limited value in a forensic scenario without anything to
analyze. Various forms of ground truth have also long been a part of
forensic datasets in multiple forms; some educational datasets include
PDF answer keys, while some research datasets have been labeled in a
structured format to include metadata about the dataset.

However, less common are detailed instructions to build the overall
scenario. Manually constructed datasets rarely describe the actions
taken to create a scenario in detail; for example, the educational
M57-Patents scenario built by Woods et al.
\cite{woodsCreatingRealisticCorpora2011} provides an instructor PDF
with a high-level timeline of actions taken in English. This detail is
sufficient for educational purposes but is too imprecise to guarantee
that others following this timeline will construct the disk image in the
same manner as intended. As described in \textbf{39.1 -
Introduction\#1.3.4 - Challenges in developing synthetic datasets},
non-determinism can be acceptable and even desirable in educational
contexts but is less desirable for tool validation and research.

Even rarer in manually constructed datasets is the inclusion of a base
image representing the machine's state before any actions are performed.
This may be attributable to both copyright concerns and a perception
that knowledge of the operating system alone is sufficient to rebuild
the base image; while it is true that setting up a virtual machine is
straightforward, any need for human interpretation introduces a source
of non-determinism that could be eliminated.

Synthesizers significantly improve on the lack of precise instructions;
their machine-readable scripts both document and execute the exact
instructions needed to reconstruct a scenario. However, this depends on
the availability of an OS- and synthesizer-specific base image; many
synthesizers expect their users to follow a set of human-readable
instructions to prepare a virtual machine specifically for use with that
synthesizer.

Where copyright issues are not a concern, synthetic images should aim to
include a complete definition of a virtual machine to be used as the
base image. A base image may be a full, hypervisor-specific virtual
machine (archiving and compressing the entirety of the associated
virtual machine folder), a hypervisor-independent virtual appliance (in
a format such as OVF), or another infrastructure-as-code solution to
define and build virtual machines, such as Vagrant.

AKF is designed to provide all four of these elements in every scenario
it creates; elements 1, 2, and 4 are inherent to AKF's design, while
base images can be provided as Vagrantfiles as described in \textbf{39.6
- Building scenarios\#6.2 - Setup and basic usage}. This ensures the
reproducibility of both the creation of AKF datasets and any results
derived from them.

Although not explored as part of this thesis, the inclusion of all four
of these elements as part of a well-structured, standardized
distribution format could be used to build a distribution platform
similar to CFReDS but with more powerful discovery and querying
functionality. While the contents of the scenario are primarily
described by CASE, it may also be possible to perform queries based on
the contents of Vagrantfiles and AKF scripts. For example, a user may
want to search for all images that use the agent-based Chromium artifact
generation described in \textbf{39.4 - Action automation\#4.3.2 - AKF
implementation}, which can be achieved by searching for the inclusion of
the relevant AKF libraries in the scenario's scripts. However, this does
not address the challenge of storing and distributing scenarios
efficiently to support such a platform; this is discussed in
\textbf{39.8 - Future work\#8.4 - Distribution}.

With the reproducibility and value of AKF-generated scenarios
established, we now discuss how to invoke and leverage the underlying
technologies that provide these benefits. \# 5. Dataset construction

\subsection{Setup and usage}\label{setup-and-usage}

\subsection{The AKF scripting
language}\label{the-akf-scripting-language}

\subsection{Generative AI
workflows}\label{generative-ai-workflows}

\section{6 - Results?}\label{results}

Maybe just a simple set of scenarios that demonstrate it actually works?
idk

I think the simplest demonstration of everything is a \emph{VERY} simple
ransomware scenario in which a user visits a bunch of pages (which is
automated), perhaps downloading some files in between, and eventually
lands on a google drive page of some sort where they download and run a
ransomware

for the sake of realism, the ransomware is manually executed by opening
Explorer and clicking on it, just to demonstrate the pyautogui part of
the agent (?)

\section{7 - Conclusion and future
work}\label{conclusion-and-future-work}

Public forensic datasets are invaluable to advancing research and
education throughout digital forensics. However, high-quality datasets
are presently few in number and may not fit specific needs, motivating
the development of new datasets. Constructing these datasets by hand is
time-consuming and prone to errors, yet it continues to be the primary
method through which new datasets are made. In turn, there is a need for
synthesizers, which allow users to create datasets using high-level
scripting languages that can automate many common actions. Although
prior synthesizers have addressed the creation of specific forensic
artifacts, there are still opportunities to improve their flexibility
and usability while promoting their usage throughout the forensic
community.

AKF, the \emph{automated kinetic framework}, introduces a modern
approach to forensic synthesis through a modular architecture focusing
on sustained development and community adoption. It provides significant
advancements in artifact generation, logging and validation, and the
overall construction of new scenarios. It improves artifact generation
by integrating numerous technologies not leveraged by prior
synthesizers, greatly simplifying the implementation of the overall
architecture without compromising the breadth of features available
through the framework. By using a centralized logging architecture and
the CASE ontology, AKF is able to generate detailed, queryable metadata
of the datasets it generates, allowing users to quickly identify
artifacts of interest in a dataset. AKF also exposes a simple
declarative syntax for generating artifacts, allowing users to develop
scenarios without writing code using the underlying Python
libraries.~Finally, we provide a demonstration of using generative AI
and AKF to further streamline scenario development, both when
constructing individual artifacts and when building a complete scenario.

Of course, there are still limitations to what AKF can accomplish.
Numerous opportunities exist to leverage existing and emerging
technologies to extend the framework. For example, recent advancements
in AI have significantly lowered the barrier of automating open-ended
tasks. One notable example is OpenAI's Operator, an ``agent'' capable of
automating tasks on webpages using only natural language prompts. The
ability to automate GUI-based applications based on high-level prompts
is particularly valuable for synthesizers, as this can dramatically
reduce the time needed to implement artifact generation for new
applications or operating systems. Such functionality is complementary
to ``conventional'' synthesizers, which depend on well-defined scripts
as opposed to natural language. For artifacts that do not require
deterministic, verifiable creation -- such as ``background noise'' --
AI-based generation may be preferrable to the verbose scripts used to
generate datasets today.

There are also several practical limitations in our current
implementation of AKF. For example, AKF currently only supports
VirtualBox machines running Windows as its target environment. Support
for other hypervisors can be easily achieved through new implementations
of the hypervisor-agnostic interface and minor modifications to the
AKF-provided Vagrantfile. Similarly, consistent with AKF's focus on
using existing automation frameworks through agents to implement
application-specific functionality, much of the effort for supporting
other platforms requires writing and deploying a new OS-specific agent.
Much of the code and overall design can be inherited from the Windows
agent, enabled through the portability of Python, the lack of
OS-specific assumptions made by RPyC, and the cross-platform support of
many automation frameworks.

AKF does not address two notable topics that are prevalent in the field
of dataset creation. The first topic is the distribution of generated
datasets, which is particularly important in environments with limited
bandwidth or storage space. Several synthesizers have explored this
topic; for example, SFX \cite{russellForensicImageDescription2012}
reduces dataset sizes by truncating irrelevant information in a process
called ``partition squeezing'', while EviPlant
\cite{scanlonEviPlantEfficientDigital2017} uses a form of
differential imaging in which only differences from some base image are
distributed to users over the network. Such approaches could be
implemented as part of AKF in the future.

The second topic is the application of synthesizers to the field of
mobile datasets, which are comparable in importance to desktop datasets.
There has been limited work in developing forensic synthesizers for
mobile platforms. Two notable examples are FADE
\cite{ceballosdelgadoFADEForensicImage2022}, developed by Delgado et
al.~in 2022, and a branch of ForTrace developed by Demmel et al.~in 2024
\cite{demmelDataSynthesisGoing2024}. It may be possible to apply
some of the approaches used as part of AKF in mobile dataset creation,
but more research is needed to determine viable options for streamlining
the construction of both Android and iOS datasets.

There is no doubt that continuing advancements in technology will
improve the process of constructing complex datasets for digital
forensics. Our contributions have been made in the hope that they will
not only form the basis of future developments in dataset synthesis, but
also advance research and education throughout digital forensics.

%% Beginning of appendices
\appendix

%% Using bibtex to generate items, expecting article_bib.bib
\bibliographystyle{elsarticle-num} 
\bibliography{article_bib}

\end{document}
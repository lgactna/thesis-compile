This chapter addresses the mechanisms through which AKF generates and
documents its outputs, depicted in the partial architectural diagram
below.

!\textbf{39.5 - Output and validation 2025-02-08 17.13.11.excalidraw}

This chapter addresses the role of the output and validation library in
providing several services to AKF, including a centralized logging
system, the use of CASE objects, and invoking various commands to
generate and export outputs such as disk images, network captures, and
volatile memory dumps. It also describes high-level reporting and
validation functionalities, both those as part of the framework itself
and options available through external tools.

\subsection{5.1 - Overview}\label{overview}

Whether generating artifacts through physical or logical means, these
artifacts must ultimately be exported and documented. In most cases,
this involves generating disk images, volatile memory captures, or
network captures; additionally, specific artifacts, such as browser
artifacts, may be selectively copied from a filesystem.

In either case, the contents and details of these artifacts, as well as
the means through which they were generated, should be documented in
some format. The term ``ground truth'' describes the contents of a
single specific dataset in full. That is, it ideally provides a
reference for every artifact that can be discovered within a dataset, as
well as every action taken to plant those artifacts.

From an educational perspective, the ground truth represents an ``answer
key'' to the dataset; it details every artifact of interest that an
analyst could be expected to discover. For research, it allows for
well-labeled datasets that can be used for tool development, validation,
and testing. Importantly, this should be generated in a manner
independent of the input script used to construct the scenario, allowing
\emph{all} artifacts to be documented, including those not explicitly
declared or deemed important by the scenario creator.

The remainder of this chapter describes the mechanisms through which
outputs are exported from the synthesizer and the mechanisms that allow
for real-time logging and documentation.

\subsection{5.2 - Core outputs}\label{core-outputs}

\subsubsection{5.2.1 - Extracting datasets}\label{extracting-datasets}

\subsubsection{5.2.2 - Distribution and
storage}\label{distribution-and-storage}

\subsection{5.3 - Metadata and ground
truth}\label{metadata-and-ground-truth}

\subsubsection{5.3.1 - Overview}\label{overview-1}

There exists a gap in the ability of instructors and researchers to
perform bulk searches for specific forensic artifacts in public
datasets. For example, the NIST CFReDS repository
\cite{nationalinstituteofstandardsandtechnologyCFReDSPortal}, one of
the largest listings of forensic datasets, does not have a unified
standard for describing uploaded images. While it is possible for users
to search by keywords and human-applied tags, these are not presented in
a standardized format.

For most datasets, an analyst must read through a PDF answer key (if one
exists) or analyze the image themselves to determine if a particular
artifact is present. Such formats are not immediately machine-readable,
and are therefore difficult to use in bulk searches. Additionally, the
content of human-made reports may be limited to what the author believes
is significant, even if other artifacts of interest are present in the
image. In turn, it may be difficult to quickly determine if a dataset is
useful in demonstrating a particular technique to students, or in
validating a specific feature of a newly-developed tool.

A rigid, well-defined format for ground truth is invaluable to
researchers engaging in tool validation and development. It is easy to
write an automated converter for well-structured data into natural
language; it is likely more difficult to perform the reverse operation.
Perhaps the lack of labeled forensic datasets on major repositories may
be attributable to the lack of a need for one; Grajeda et
al.~demonstrated that few scenarios are shared between researchers to
begin with, so there is rarely a need to label them for general-purpose
usage. AKF has the opportunity to solve this issue by allowing for the
mass production of labeled datasets, adopting a single major standard.

Many forensic analysis tools support exporting case data in both
proprietary and language-agnostic formats. For example, Cellebrite's
UFED supports exporting to UFDR and XML files, Magnet Axiom supports
exporting to XML, and Autopsy supports a variety of formats including
Excel, STIX, and HTML. Each of these vary in structure and format, and
do not necessarily contain equivalent information for the same analyzed
disk image using default settings. This is the primary challenge with
using an existing format, especially a proprietary format that is
subject to vendor changes; certain details may be missing, and may
change at an arbitrary point in time.

There has been extensive work in other fields towards developing a
structured ontology that describes relationships and low-level details.
This includes the EVIDENCE project for criminal justice and the
Structured Threat Information Expression (STIX) format for conveying
cyber threat intelligence
\cite{caseyLeveragingCybOXStandardize2015}. For example, STIX
provides a standard set of objects that allows organizations to describe
observed attacker techniques and associate them with specific pieces of
malware, attack campaigns, or threat actors.

However, there is limited work that aims to document the contents of a
forensic scenario (disk images and related metadata) in a vendor-neutral
manner. Besides their lack of adoption, Casey et al.~found that existing
formats lacked features such as parent-child relationships, user
actions, and non-technical case information such as a chain of custody.
In response, the same authors introduced the Digital Forensic Analysis
eXpression, or DFAX, a language extending CybOX (the predecessor to
STIX) for use in the digital forensics community. DFAX eventually
evolved to become the Cyber-investigation Analysis Standard Expression
(CASE) \cite{caseyAdvancingCoordinatedCyberinvestigations2017},
which we leverage as \emph{fastlabel}'s standard output format. CASE is
perhaps the most comprehensive and actively supported ontology available
for digital forensics; contributors include NIST with support from the
Linux Foundation.

\subsubsection{5.3.2 - CASE and Python
bindings}\label{case-and-python-bindings}

CASE is a vendor-neutral format that aims to document both technical and
non-technical information about a digital forensics case
\cite{caseyAdvancingCoordinatedCyberinvestigations2017}. It aims to
cover as many OS-specific and application-specific artifacts as
possible, while still providing the flexibility to describe artifacts
from uncommon applications. In theory, data exported from any major
vendor, such as Cellebrite, Magnet, or FTK, can be converted into a
valid CASE file. CASE is an extension of the Unified Cyber Ontology, or
UCO, which simply provides basic objects that are not specific to
digital forensics (such as applications or users). Consistent with
existing documentation, CASE and CASE/UCO refer to the same format.

CASE is built on the Resource Description Framework (RDF), a model for
describing information using relationships. Two objects are linked using
relationships, each of which is described as a ``triple''. This pattern
allows for directed, labeled graphs to be expressed using RDF. The
ontology of CASE objects is defined using the Terse RDF Triple Language,
or Turtle, which allows these triples to be written in a simple text
format. For example, the following set of triples describes an object
called \texttt{ApplicationFacet} with two properties,
\texttt{numberOfLaunches} and \texttt{applicationIdentifier}:

REPLACE CODEBLOCK HERE

An instance of a \texttt{Application} object may thus be represented in
the JSON-LD format using the \texttt{ApplicationFacet} as follows
(including attributes omitted above):

REPLACE CODEBLOCK HERE

Because the CASE format itself is language-agnostic, it is necessary to
write a library for each language that leverages CASE. As of writing,
the CASE project provides Python bindings for UCO/CASE 1.4
\cite{CaseworkCASEMappingPython}, in which each unique object is
represented as a Python class, which can be instantiated to produce
individual objects. However, this library has several limitations due to
its design. For example, CASE objects are internally represented as
dictionary of strings, rather than a set of instance variables. While
this makes it easier to serialize these objects to JSON-LD, it also
makes it extremely difficult to work with objects after they have been
instantiated:

REPLACE CODEBLOCK HERE

Additionally, it appears that each of the objects have been manually
translated from the Turtle definitions to their corresponding Python
class. This is slow and time-consuming, especially given the context
that a significant overhaul of UCO/CASE to version 2.0 is underway, with
new object definitions; there appears to be no active effort to update
the 1.3 bindings to 2.0.

AKF leverages (and contributes, as part of a project independent of this
thesis) Pydantic-based bindings for CASE. The foundation for this system
will be the Pydantic library for Python, which allows developers to
quickly define classes (referred to as ``Pydantic models'', or simply
``models'') with built-in schema validation and serialization based on
Python type hints \cite{colvinPydantic2024}. More broadly, it allows
us to simplify the declaration of individual objects while providing
runtime type validation and automatic casting.

For example, we can write the same object above as follows:

REPLACE CODEBLOCK HERE

This declaration is only three lines, 40 lines shorter than the existing
declaration of \texttt{ApplicationFacet} - a 93\% reduction in code
written. This is because the conversion of instance variables to valid
JSON-LD keys is deferred until serialization. This design choice allows
us to centralize the serialization logic in a single parent class that
all CASE objects inherit from. In exchange for slightly increasing the
complexity of converting \texttt{numberOfLaunches} to a dictionary with
the correct key name, we can massively simplify the logic for declaring
CASE objects.

A simple CASE bundle, representing the complete contents of a forensic
scenario, can be observed below:

REPLACE CODEBLOCK HERE

A script is provided with the Pydantic-based CASE bindings to
automatically parse the RDF files and convert them to valid Pydantic
models. It automatically converts XSD datatypes to their native Python
types (or a custom wrapper type if a native type does not exist),
correctly inherits classes, and automatically generates docstrings and
Pydantic fields as applicable. Additionally, the script also
topologically sorts dependencies in the same file; the parent class of
an RDF object may be declared \emph{after} its child class, which is
disallowed in Python. This greatly simplifies the process of maintaining
Python bindings for UCO/CASE, as well as the overall design of the
library for future needs.

\subsubsection{5.3.3 - CASE integration in
AKF}\label{case-integration-in-akf}

With these Python bindings, we can integrate them throughout
artifact-generating libraries in AKF.

(How does one actually generate CASE entries? the answer is that various
things accept CASE bundles, and they can optionally attach extra data to
the bundle as needed\ldots)

\subsection{5.4 - Human readable
reporting}\label{human-readable-reporting}

as previously mentioned, it's easier to go from rigid and well-defined
to human reports than it is to go the other way around.

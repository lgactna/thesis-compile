This chapter addresses the mechanisms through which AKF generates and
documents its outputs, depicted in the partial architectural diagram
below.

!\textbf{39.5 - Output and validation 2025-02-08 17.13.11.excalidraw}

This chapter addresses the role of the output and validation library in
providing several services to AKF, including a centralized logging
system, the use of CASE objects, and invoking various commands to
generate and export outputs such as disk images, network captures, and
volatile memory dumps. It also describes high-level reporting and
validation functionalities, both those as part of the framework itself
and options available through external tools.

\section{Overview}\label{overview}

Whether generating artifacts through physical or logical means, these
artifacts must ultimately be exported and documented. In most cases,
this involves generating disk images, volatile memory captures, or
network captures; additionally, specific artifacts, such as browser
artifacts, may be selectively copied from a filesystem. Disk, memory,
and network captures will be referred to as ``core outputs'' for
brevity, as well as to distinguish them from ``standalone'' artifacts;
both of these are simply ``outputs''.

In either case, the contents and details of these artifacts, as well as
the means through which they were generated, should be documented in
some format. The term ``ground truth'' describes the contents of a
single specific dataset in full. That is, it ideally provides a
reference for every artifact that can be discovered within a dataset, as
well as every action taken to plant those artifacts. This metadata, in
addition to core outputs and standalone artifacts, comprises a forensic
dataset.

From an educational perspective, the ground truth represents an ``answer
key'' to the dataset; it details every artifact of interest that an
analyst could be expected to discover. For research, it allows for
well-labeled datasets that can be used for tool development, validation,
and testing. Importantly, this should be generated in a manner
independent of the input script used to construct the scenario, allowing
\emph{all} artifacts to be documented, including those not explicitly
declared or deemed important by the scenario creator.

The remainder of this chapter describes the mechanisms through which
outputs are exported from the synthesizer and the mechanisms that allow
for real-time logging and documentation.

\section{Compiling datasets}\label{compiling-datasets}

\subsection{Generating outputs}\label{generating-outputs}

In the same way that artifacts can be generated through logical and
physical means, outputs can also be generated through logical and
physical means. Some of these are analogous to techniques used by
real-world investigators to extract forensically sound evidence that is
valid in a court of law, while others are less suitable in a court
setting but still have valid use cases.

Logical output generation refers to any technique in which software is
used inside a running virtual machine to generate these outputs. This
includes using software such as FTK Imager, booted from a removable
drive, to capture the volatile memory of a device or construct a
\emph{logical} disk image. Similarly, network captures can be done by
simply running Wireshark on the target device and network interface, and
individual artifacts can be copied off of the device by hand and sent to
a network or removable drive.

Physical output generation refers to techniques in which the operating
system is unaware of the technique being used, and therefore leave few
or no traces in the resulting outputs. In practice, this involves using
tools such as hardware write blockers to extract complete disk images
(which also contain standalone artifacts), as well as using network taps
and sniffers (or another traffic mirroring solution) to capture traffic
over a particular interface. Although difficult, it is also possible to
perform a physical extraction of RAM by performing a ``cold boot
attack'', in which the RAM sticks are cooled to low temperatures before
removing them from a running machine, slowing the process of memory
decay as a result of the DRAM cells being unpowered
\cite{yitbarekColdBootAttacks2017}.

AKF directly supports physical output generation for all three core
outputs, and indirectly supports logical output generation for core
outputs and standalone artifacts. Physical output generation for virtual
machines is generally achieved through direct interaction with the
hypervisor itself.

\begin{itemize}
\tightlist
\item
  For disk images, the output can simply be the virtual hard drive used
  by the hypervisor on the host machine (such as VDIs for VirtualBox).
  If an ``actual'' disk image is desired, the command-line tool
  VBoxManage supports converting various virtual drive formats to raw
  disk images using the \passthrough{\lstinline!vboxmanage clonemedium!}
  command, which also expands the compressed virtual drive to the
  ``declared'' size of the drive as seen by the operating system.\\
\item
  For network captures, VirtualBox allows the user to enable network
  tracing over multiple interfaces, dumping network traffic as a .pcap
  file on the host machine. This can be enabled or disabled at any time
  without affecting network connectivity or the state of the virtual
  machine.\\
\item
  For volatile memory dumps, VBoxManage provides the command `vboxmanage
  debugvm \textless machine\_name
\end{itemize}

In most cases, these physical output options are sufficient to generate
suitable datasets. If only specific files are desired, the existing file
transfer utilities provided by AKF can be used to extract standalone
artifacts. Users can also manually perform the logical techniques
described above (such as running Wireshark or installing FTK Imager)
through a variety of means, such as pausing an AKF script until
instructed by the user to continue; during this time, a user can
manually run FTK imager to extract the contents of RAM, as an example.

\subsection{Distribution and
storage}\label{distribution-and-storage}

\section{Metadata and ground
truth}\label{metadata-and-ground-truth}

\subsection{Overview}\label{overview-1}

There exists a gap in the ability of instructors and researchers to
perform bulk searches for specific forensic artifacts in public
datasets. For example, the NIST CFReDS repository
\cite{nationalinstituteofstandardsandtechnologyCFReDSPortal}, one of
the largest listings of forensic datasets, does not have a unified
standard for describing uploaded images. While it is possible for users
to search by keywords and human-applied tags, these are not presented in
a standardized format.

For most datasets, an analyst must read through a PDF answer key (if one
exists) or analyze the image themselves to determine if a particular
artifact is present. Such formats are not immediately machine-readable,
and are therefore difficult to use in bulk searches. Additionally, the
content of human-made reports may be limited to what the author believes
is significant, even if other artifacts of interest are present in the
image. In turn, it may be difficult to quickly determine if a dataset is
useful in demonstrating a particular technique to students, or in
validating a specific feature of a newly-developed tool.

A rigid, well-defined format for ground truth is invaluable to
researchers engaging in tool validation and development. It is easy to
write an automated converter for well-structured data into natural
language; it is likely more difficult to perform the reverse operation.
Perhaps the lack of labeled forensic datasets on major repositories may
be attributable to the lack of a need for one; Grajeda et
al.~demonstrated that few scenarios are shared between researchers to
begin with, so there is rarely a need to label them for general-purpose
usage. AKF has the opportunity to solve this issue by allowing for the
mass production of labeled datasets, adopting a single major standard.

Many forensic analysis tools support exporting case data in both
proprietary and language-agnostic formats. For example, Cellebrite's
UFED supports exporting to UFDR and XML files, Magnet Axiom supports
exporting to XML, and Autopsy supports a variety of formats including
Excel, STIX, and HTML. Each of these vary in structure and format, and
do not necessarily contain equivalent information for the same analyzed
disk image using default settings. This is the primary challenge with
using an existing format, especially a proprietary format that is
subject to vendor changes; certain details may be missing, and may
change at an arbitrary point in time.

There has been extensive work in other fields towards developing a
structured ontology that describes relationships and low-level details.
This includes the EVIDENCE project for criminal justice and the
Structured Threat Information Expression (STIX) format for conveying
cyber threat intelligence
\cite{caseyLeveragingCybOXStandardize2015}. For example, STIX
provides a standard set of objects that allows organizations to describe
observed attacker techniques and associate them with specific pieces of
malware, attack campaigns, or threat actors.

However, there is limited work that aims to document the contents of a
forensic scenario (disk images and related metadata) in a vendor-neutral
manner. Besides their lack of adoption, Casey et al.~found that existing
formats lacked features such as parent-child relationships, user
actions, and non-technical case information such as a chain of custody.
In response, the same authors introduced the Digital Forensic Analysis
eXpression, or DFAX, a language extending CybOX (the predecessor to
STIX) for use in the digital forensics community. DFAX eventually
evolved to become the Cyber-investigation Analysis Standard Expression
(CASE) \cite{caseyAdvancingCoordinatedCyberinvestigations2017},
which we leverage as \emph{fastlabel}'s standard output format. CASE is
perhaps the most comprehensive and actively supported ontology available
for digital forensics; contributors include NIST with support from the
Linux Foundation.

\subsection{CASE and Python
bindings}\label{case-and-python-bindings}

CASE is a vendor-neutral format that aims to document both technical and
non-technical information about a digital forensics case
\cite{caseyAdvancingCoordinatedCyberinvestigations2017}. It aims to
cover as many OS-specific and application-specific artifacts as
possible, while still providing the flexibility to describe artifacts
from uncommon applications. In theory, data exported from any major
vendor, such as Cellebrite, Magnet, or FTK, can be converted into a
valid CASE file. CASE is an extension of the Unified Cyber Ontology, or
UCO, which simply provides basic objects that are not specific to
digital forensics (such as applications or users). Consistent with the
CASE project's documentation, CASE and CASE/UCO are one and the same.

CASE is built on the Resource Description Framework (RDF), a model for
describing information using relationships. Two objects are linked using
relationships, each of which is described as a ``triple''. This pattern
allows for directed, labeled graphs to be expressed using RDF. The
ontology of CASE objects is defined using the Terse RDF Triple Language,
or Turtle, which allows these triples to be written in a simple text
format.

Because the CASE format itself is language-agnostic, it is necessary to
write language-specific libraries that allow for instantiating CASE
objects. As of writing, the CASE project provides Python bindings for
UCO/CASE version 1.4 \cite{CaseworkCASEMappingPython}, in which each
unique object is represented as a Python class, which can be
instantiated to produce individual objects. However, this library has
several limitations due to its design. For example, CASE objects are
internally represented as dictionary of strings, rather than a set of
instance variables. While this makes it easier to serialize these
objects to JSON-LD dictionaries, it also makes it extremely difficult to
work with these objects after they have been instantiated.

It is also worth noting that each of the objects in the CASE ontology
appear to have been manually translated to their corresponding Python
class definitions. This is slow and time-consuming, especially given the
context that a significant overhaul of UCO/CASE to version 2.0 is
underway, with new object definitions; there appears to be no active
effort to update the 1.4 bindings to 2.0.

AKF leverages (and contributes, though not the direct result of this
thesis) Pydantic-based bindings for CASE. The foundation for this system
will be the Pydantic library for Python, which allows developers to
quickly define classes (referred to as ``Pydantic models'', or simply
``models'') with built-in schema validation and serialization based on
Python type hints \cite{colvinPydantic2024}. More broadly, it allows
us to simplify the declaration of individual objects while providing
runtime type validation and automatic casting.

Examples of CASE-related definitions, as well as a detailed comparison
of AKF's bindings compared to the existing CASE bindings, can be found
in \autoref{case-python-bindings}. One notable
example from this section is the simplification of a CASE object
declaration from 43 lines in the existing CASE bindings to only three
lines in AKF. These simple declarations are largely possible because the
conversion of instance variables to valid JSON-LD keys is deferred until
serialization, rather than converting them to dictionaries immediately
upon instantiation. This design choice allows us to centralize the
serialization logic in a single parent class, which all CASE objects
inherit from. In exchange for slightly increasing the complexity of
converting \passthrough{\lstinline!numberOfLaunches!} to a dictionary
with the correct key name, we can massively simplify the logic for
declaring CASE objects.

A script is provided with AKF's CASE bindings to automatically parse the
RDF files and convert them to valid Pydantic models. It automatically
converts XSD datatypes to their native Python types (or a custom wrapper
type if a native type does not exist), correctly inherits classes, and
automatically generates docstrings and Pydantic fields as applicable.
Additionally, the script also topologically sorts dependencies in the
same file; the parent class of an RDF object may be declared
\emph{after} its child class, which is disallowed in Python. This
greatly simplifies the process of maintaining Python bindings for
UCO/CASE, as well as the overall design of the library for future needs.

\subsection{CASE integration in
AKF}\label{case-integration-in-akf}

With these Python bindings, we can integrate them throughout
artifact-generating libraries in AKF.

(How does one actually generate CASE entries? the answer is that various
things accept CASE bundles, and they can optionally attach extra data to
the bundle as needed\ldots)

\section{Human readable reporting}\label{human-readable-reporting}

as previously mentioned, it's easier to go from rigid and well-defined
to human reports than it is to go the other way around.

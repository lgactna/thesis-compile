We begin with a literature review of two topics: publicly available
forensic datasets and the contributions of prior synthesizers. More
precisely, we explore the datasets that currently exist for digital
forensics in greater detail, as well as gaps identified in these
datasets by various authors. This will provide context for both the need
for synthesizers and the gaps that these synthesizers have gradually
filled.

\section{Existing forensic
corpora}\label{existing-forensic-corpora}

Forensic datasets have been available for public use (sometimes by
request) since the early days of the digital forensics field, though
their sourcing and qualities have changed significantly over time. This
topic was explored in far greater detail by Grajeda et al., who
performed a survey of over 700 research articles to identify the various
datasets used throughout the field. However, it is still important to
highlight specific datasets relevant to the development of synthesizers.

Early datasets were primarily derived from real sources, whether made
available to the public or otherwise. The earliest collections of real
datasets include the used hard drives collected by Garfinkel from 1998
to 2006 and the Enron email corpus obtained during the federal
investigation of Enron \cite{garfinkelForensicCorporaChallenge2007}.
Other early real datasets identified by Grajeda et al.~include the
public Apache mailing archive, the Reuters news corpora, and various
facial recognition collections such as the MORPH corpus
\cite{ricanekMORPHLongitudinalImage2006}, all of which were made in
the early to mid-2000s
\cite{yannikosDataCorporaDigital2014,grajedaAvailabilityDatasetsDigital2017}.

A variety of synthetic datasets were also constructed during this early
period. These datasets include the network captures obtained from
simulated attacks conducted by the MIT Lincoln Laboratory from 1998 to
2000 \cite{garfinkelForensicCorporaChallenge2007}, as well as
standalone datasets used for tool validation as part of the early CFTT
program developed by NIST. Other synthetic datasets during this period
were generated as part of challenges, such as those produced for DFRWS
conferences \cite{woodsCreatingRealisticCorpora2011}.

The variety of forensic datasets increased considerably towards the late
2000s, which can be credited to both the overall growth of the field
(including the broader field of incident response) and computing as a
whole. Various notable datasets described by Grajeda et al.~include
malware samples discovered ``in the wild,'' natural language collections
from multiple languages, and file-specific datasets such as collections
of Microsoft Office files. It was also during this time that non-disk
datasets, such as volatile memory dumps and network captures, became
more prevalent. Although not explored by this thesis in detail, mobile
datasets -- such as smartphone disk images, mobile malware and
applications, and SIM card images -- also grew more prevalent.

Many of these datasets were not maintained as part of a larger
collection with the explicit intent of providing them for digital
forensics research. This began to change towards the late 2000s; for
example, Garfinkel's collection of disk images would eventually evolve
into the Real Data Corpus, growing to 30 terabytes by 2013
\cite{garfinkelBringingScienceDigital2009,yannikosDataCorporaDigital2014}.
The collection included disk images, flash drive images, and a variety
of optical discs sourced from real-world usage, requiring institutional
review board approval to use. This collection would eventually be part
of the Digital Corpora platform, which includes a set of purely
synthetic datasets. Separately, NIST began developing the CFTT and
CFReDS projects, both of which provide forensic datasets for various
purposes. In 2021, Xu et al.~compiled and published a repository of
educational datasets that explicitly focuses on ease of use, realism,
and breadth \cite{xuDesigningSharedDigital2022}. All corpora
mentioned in this paragraph are currently actively maintained.

There are other datasets that are relatively unique and are often
maintained as part of a larger niche collection. Collections of malware
samples have grown significantly, in part because of the modern threat
intelligence ecosystem supported by platforms such as VirusTotal. There
exist datasets focusing on the dark web, such as those operating on the
Tor network, including ``black market'' sites on which illegal goods are
bought and sold. Finally, there are also network captures from various
novel sources, such as iterations of the Collegiate Cyber Defense
Competition and the networking infrastructure of university IT
departments \cite{grajedaAvailabilityDatasetsDigital2017}.

These datasets span a wide range of technologies -- including different
versions of the same technology -- that require distinct methodologies
to analyze effectively. However, the field faces the challenge of making
datasets available that reflect current advancements in technology, such
as new operating system versions or new applications. For example,
instant messaging applications have changed considerably over the
history of the field, ranging from MSN Messenger in the early 2000s to
Skype, Discord, Telegram, Signal, Slack, and more. Artifacts from these
applications must be handled differently, even if the value of the
underlying service is essentially the same. Similarly, the strategies
for analyzing Windows artifacts have changed significantly from version
to version, as new registry keys become relevant in analyzing
applications while others become unused.

This gradual ``aging'' of datasets, in which their relevance degrades
over time, contributes to the continuous need for new datasets. Indeed,
the need for new, novel datasets is one of the reasons identified by
Grajeda et al.~for the manual development of new datasets by research
authors; it was often the case that a modern dataset simply did not
exist for their needs. The other motivation is that the dataset used was
never made public, either due to the author not having the resources to
distribute the dataset themselves or because of legal and privacy
concerns.

How are these datasets relevant to the development of synthesizers? The
datasets throughout this section have demonstrated that many datasets
have proved to be relevant in digital forensics research, even if not
immediately evident. In turn, any effort to streamline the development
of forensic datasets should be able to cover as many use cases as
possible. Synthesizers must fulfill two requirements to achieve this:

\begin{itemize}
\tightlist
\item
  A synthesizer should be able to generate artifacts present in existing
  datasets, provided that the underlying technologies are still
  available.
\item
  A synthesizer should be able to account for developments in operating
  systems, applications, or other technologies without requiring
  significant changes to the underlying architecture.
\end{itemize}

Indeed, the synthesizers described in the following section have
explicitly addressed these two concerns. For example, many of these
synthesizers have focused on implementing features that reflect the
qualities of real-world datasets. These features include the ability to
execute malware samples in a virtualized environment, send emails to
arbitrary email servers, and insert data on removable drives -- all of
which generate forensic artifacts present in previously used datasets.
Similarly, modern synthesizers are capable of generating disk images,
network captures, and volatile memory dumps, in addition to extracting
specific artifacts such as application-specific files. The gradual
progression in the ability of synthesizers to generate specific
artifacts is described in \autoref{analysis-of-existing-synthesizers}, which explores specific
contributions made by each synthesizer.

Of note is the generation of similar datasets that typically involve
significant human interaction, such as public email distribution lists,
photographs of human faces, and other transcripts of conversations.
While prior synthesizers have not explored this in significant detail,
it is explored as part of the generative AI work done as part of AKF in
\autoref{chapter-six}.

The second issue, in which synthesizers must be extensible in such a way
that they can support new applications, has been approached in several
ways. This is described in greater detail in \autoref{chapter-three}, but has been a significant design consideration in the
development of most synthesizers.

\section{Analysis of existing
synthesizers}\label{analysis-of-existing-synthesizers}

Numerous frameworks have been built over the last two decades that aim
to significantly reduce the effort involved in creating synthetic images
from scratch by automating various application- and OS-specific actions
according to provided instructions. (This is a subset of broader
automation efforts throughout digital forensics, as described by
Michelet et al. \cite{micheletAutomationDigitalForensics2023}.) The
functionality and availability of the frameworks described throughout
the literature have varied considerably over time. However, the goal of
these frameworks has largely remained consistent: they all aim to
provide a rapid method for instructors to develop forensic labs for
students. Note that low-level implementation details are described in
later, more relevant sections throughout this thesis for clarity rather
than providing detailed analyses as part of this chapter. Before
analyzing and adapting the low-level implementations of prior
synthesizers, it is best to first contextualize their broader
contributions.

The first identified effort to streamline the creation of forensic labs
through a high-level language was published by Adelstein et al.~in 2005
through the development of \textbf{FALCON}, the Framework of Laboratory
Exercises Conducted Over Networks
\cite{adelsteinAutomaticallyCreatingRealistic2005}. It proposed an
architecture for automatically creating lab exercises, deploying them to
a virtual lab, and evaluating students' actions in the lab compared to
the ground truth. Very few implementation details are provided in the
paper, though some examples of its usage are included. Although not a
synthesizer, related concepts can be seen in the description of
\textbf{CYDEST}, the CYber DEfenSe Trainer
\cite{bruecknerAutomatedComputerForensics2008}, which similarly
provided complex virtualized lab environments to students via the
internet. Again, the authors provide limited implementation details, and
it is unclear if either FALCON or CYDEST are publicly available or
actively maintained.

\textbf{Forensig2}, described by Moch and Freiling in 2009 and revisited
in 2012, appears to be the first detailed description of an image
synthesizer
\cite{mochForensicImageGenerator2009,mochEvaluatingForensicImage2012}.
Users define scenarios through a Python 2 library that provides
abstractions around various virtual machine operations, such as
formatting disks, creating partitions, and copying files from the host
to the virtual machine. Actions are performed live through a Qemu-based
VM, eventually producing a ground truth report and (effectively) an
image to be analyzed by students. The source code for Forensig2 is not
currently maintained and appears to be unavailable.

The \textbf{Digital Forensic Evaluation Test (D-FET)} platform described
by William et al.~in 2011 provides a custom scripting language for users
to define system- and user-level actions
\cite{williamCloudbasedDigitalForensics2011}. Similar to FALCON and
CYDEST, it provided virtualized labs through VMware ESXi. Unlike most
other frameworks, it was primarily built to provide a platform to
efficiently evaluate digital forensic tools on a scalable infrastructure
rather than streamline the process of building and distributing labs for
instructors (though it is mentioned that the infrastructure was also
used to host student labs). It is unclear if a public implementation is
available.

Russell et al.~define an XML specification, the \textbf{Summarized
Forensic XML (SFX)} language, to describe scenarios as a sequence of
various actions on various partitions of a disk
\cite{russellForensicImageDescription2012}. This can be passed into
an interpreter to produce a disk image that can be analyzed by students.
Besides high-level operations such as disk partitioning and file
copying, it also provides Windows- and Linux-specific routines for
minimizing partition sizes to reduce the size of files irrelevant to a
scenario, often those associated with the OS. It also briefly describes
approaches for updating web browsers and the Windows registry to reflect
certain actions. Limited implementation details are provided, and it is
unclear if a public implementation is available.

\textbf{Yannikos et al.} define a framework for describing scenarios as
a series of Markov chains. The framework allows users to visually define
scenarios through a graph view in which individual nodes represent
distinct actions to be taken by the synthesizer, with probabilistic
transitions deciding which node to execute next
\cite{yannikosDataCorporaDigital2014}. Its focus on non-determinism
makes it well-suited for quickly developing variations of the same
scenario, though it is unclear what application- or OS-specific routines
are provided.

The remaining frameworks are broadly similar in that they are all
implemented with Python, providing users with a Python library to define
and generate scenarios. Various functions in the library provide
abstractions to complex actions, such as Facebook browser activity or
sending emails on the Thunderbird application. Additionally, each
framework produces some form of detailed output that can be used as
answer keys or ground truth in an instructional setting. A brief summary
of the notable aspects of each of these frameworks relative to each
other is provided below:

\begin{itemize}
\tightlist
\item
  \textbf{ForGeOSI} \cite{maxfraggMaxfraggForGeOSI2023} introduced
  the use of the VirtualBox SDK to automate various operations, forming
  the basis for much of the work done as part of VMPOP.
\item
  \textbf{ForGe} \cite{vistiAutomaticCreationComputer2015} is
  specifically designed to generate NTFS and FAT32 images, focusing on
  placing data directly onto disk images without a virtual machine by
  maintaining and serializing custom data structures for supported
  filesystems.
\item
  \textbf{EviPlant} \cite{scanlonEviPlantEfficientDigital2017}
  encompasses a novel method for efficiently distributing generated disk
  images, which is achieved by generating and distributing differential
  ``evidence packages'' to apply to a base image file. An OS-specific
  injection tool creates relevant artifacts according to the evidence
  package. Since the base image must only be downloaded once, each
  reconstructed disk image is significantly smaller than if distributed
  as standalone images.
\item
  \textbf{VMPOP} \cite{parkTREDEVMPOPCultivating2018} provides an
  architecture for elaborate VirtualBox control (such as attaching USB
  devices and starting video captures) in addition to various
  OS-specific commands. Provided routines for Windows include creating
  restore points, installing programs, mapping network drives, setting
  registry values, and more. Although the architecture as a whole is
  platform-independent, the provided implementations operate with
  VirtualBox and Windows. \textbf{TraceGen}
  \cite{duTraceGenUserActivity2021} is similar in that it makes use
  of the VirtualBox API for various operations but is more like hystck
  in its use of a Python-based agent to carry out application-specific
  actions.
\item
  \textbf{hystck} \cite{gobelNovelApproachGenerating2020} provides
  routines for automating OS- and application-specific commands through
  both YAML configuration files (passed through an intermediate
  interpreter script) and/or Python scripts (executed normally). The
  framework produces network captures and disk images; similar to
  EviPlant, it supports ``differential'' images that can be distributed
  and applied to ``template'' images.
\item
  \textbf{ForTrace} \cite{gobelForTraceHolisticForensic2022} is the
  most recently developed synthesizer, which directly builds upon hystck
  by providing volatile memory captures (alongside disk and network
  captures) in addition to various other new features (such as the
  ability to execute PowerShell scripts to create Windows artifacts),
  with a focus on a modular architecture. A variant focusing on Android
  artifact generation was developed in 2024
  \cite{demmelDataSynthesisGoing2024}.
\end{itemize}

Clearly, a considerable amount of work has been done to streamline the
process of developing images, although the availability and
functionality of each framework vary greatly. Notably, with the
exception of ForTrace, none of these works are direct extensions of past
works, implying that at a fundamental level, it was necessary for new
frameworks to be developed from scratch to support the framework
authors' needs. This is not directly stated in any of these works, with
the exception of hystck \cite{gobelNovelApproachGenerating2020}.
However, it can be reasonably concluded that the lack of maturity,
availability, and maintenance of prior works contributed to the
independent development of the other frameworks. These issues -- and
potential solutions -- will be addressed in more detail in \autoref{chapter-three}.

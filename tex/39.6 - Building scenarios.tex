This chapter addresses the modules responsible for allowing users to
invoke the framework through a standard Python script and a high-level
YAML file. It also addresses the generative AI modules that can assist a
user in building a scenario, as depicted in the partial architectural
diagram below.

!\textbf{39.6 - Building scenarios 2025-02-08 17.23.40.excalidraw}

At this point, we have provided the implementations for automating
artifact generation in a near-deterministic manner with comprehensive
logging and reporting. However, there is still the challenge of exposing
this functionality in a user-friendly manner. At a high level, there are
two primary ways to define an input to a synthesizer:

\begin{itemize}
\tightlist
\item
  An \textbf{imperative} format, in which the synthesizer is provided
  instructions in an imperative programming language, and the developer
  must provide the exact instructions for the synthesizer to take
  through some exposed API.
\item
  A \textbf{declarative} format, in which the synthesizer is provided a
  file that describes the desired elements of the result, and it is up
  to the synthesizer to execute the instructions necessary to achieve
  the result.
\end{itemize}

These formats describe the \emph{process} of placing artifacts and
performing actions. However, the challenge of deciding \emph{what}
actions to perform remains. It is still mainly the responsibility of
instructors to provide background noise and other realistic artifacts to
insert into a scenario. Although the high-level languages provided by
many of these frameworks make it easy to place files at desired
locations or visit websites that are part of a scenario, these must all
be defined and created ahead of time.

This chapter addresses the challenges of providing APIs for complex
GUI-driven applications and creating background noise. More precisely,
we address two questions -- how do we invoke AKF's automation systems,
and how does AKF assist a user in building a scenario? Here, we explore
AKF's imperative and declarative syntaxes and the viability of using
large language models (LLMs) to assist in building individual files and
complete scenario descriptions.

\section{Scripting background}\label{scripting-background}

We begin by analyzing how synthesizers accept instructions for execution
-- more precisely, how do users define the sequence of operations that
the synthesizer should take to create the dataset?

For many of the frameworks created in the last decade, users define
scenarios by using a Python library to interact with the framework. The
library is responsible for setting up the virtualized environment and
performing high-level actions on the environment, abstracting away the
underlying calls to the hypervisor from the scenario developer. This
code-based approach represents an \emph{imperative} strategy for
scenario creation, where the user describes how the dataset should be
created by defining the exact order and means to perform synthesizer
actions. (It is worth noting that the language used to interact with the
synthesizer's API does not need to match the language used to implement
the synthesizer itself (although this is often the case). For example,
the automation framework Playwright is implemented in TypeScript,
initially exposing a Node.JS API
\cite{MicrosoftPlaywrightpython2025}. Today, Playwright provides
APIs in Python, Java, and C\#.)

In contrast, custom scenario formats provided by D-FET
\cite{williamCloudbasedDigitalForensics2011}, SFX
\cite{russellForensicImageDescription2012}, and Yannikos et al.
\cite{yannikosDataCorporaDigital2014} follow a different approach.
For these synthesizers, a custom high-level language describes the
desired final state of the dataset. Instead of importing libraries and
writing code, users state the desired elements of the forensic dataset,
allowing the synthesizer to decide how to create the desired dataset --
a \emph{declarative} strategy for scenario creation. The specifics of
state management and execution are delegated to the synthesizer.

Consider the following declarative SFX code taken from Russell et al.
\cite{russellForensicImageDescription2012}:

\begin{lstlisting}[language=XML]
<disk>
    <partition index="p1" hidden="0" size="48M" type="ntfs">
        <base os="windows7x64"/>
        <user username = "Gordon">
             <browserhistory browser="firefox">
                 <url link="[http://bbc.co.uk"](http://bbc.co.uk") time="13:14:00 1 Jan 2013"/>
             </browserhistory>
        </user>
    </partition>
</disk>
\end{lstlisting}

Here, a Windows 7 partition is created as part of a larger disk image.
The partition is loaded in a virtual machine to create a user called
``Gordon,'' who uses Firefox to browse the internet. (This simple web
browsing scenario will be reused throughout this chapter to demonstrate
several code examples.)

The same might be expressed in ForTrace
\cite{gobelForTraceHolisticForensic2022} as the following, excluding
additional code required for ground truth and disk image generation:

\begin{lstlisting}[language=Python]
import logging

from fortrace.core.vmm import Vmm
from fortrace.utility.logger_helper import create_logger
from fortrace.core.vmm import GuestListener

logger = logging.getLogger(__name__)

if __name__ == "__main__":
    logger = create_logger('fortraceManager', logging.DEBUG)
    macsInUse = []
    guests = []
    
    guestListener = GuestListener(guests, logger)
    virtual_machine_monitor1 = Vmm(macsInUse, guests, logger)
    # boottime expressed as "%Y-%m-%d %H:%M:%S"
    guest = virtual_machine_monitor1.create_guest(guest_name="w-guest01", platform="windows", boottime="2013-01-01 13:14:00")

    browser_obj = guest.application("webBrowserFirefox", {'webBrowser': "firefox"})
    browser_obj.open(url="[http://bbc.co.uk")](http://bbc.co.uk"))
    while browser_obj.is_busy:
        time.sleep(2)
    browser_obj.close()
\end{lstlisting}

Although these two code blocks have the same expressive power (that is,
they achieve the same overall outcomes), there is a clear difference in
the complexity and length between them. It is significantly easier to
read and write the declarative XML in the first code block, as it
abstracts away the need to instantiate various synthesizer objects and
call specific methods. By extension, this also allows for a common
declarative syntax to be used across multiple synthesizers since
low-level synthesizer details do not need to be exposed as part of the
declarative syntax.

The primary benefit of an imperative approach to generation is its
flexibility; on a Python-based synthesizer, one can simply import
another library to extend the functionality of the base scenario
definition. This flexibility naturally comes at the expense of a greater
learning curve. Although many digital forensic specialists are likely to
have programming experience, it is far easier to learn a restricted
declarative specification (like XML) than an entire programming
language, which may entail additional setup (such as installing an IDE,
dependencies, and so on).

When accessibility is preferred over functionality, declarative syntaxes
can be more valuable than imperative syntaxes. Some scenario developers,
such as classroom instructors, may not need the low-level control
provided by an imperative syntax or a complete programming language such
as Python. It also takes time to learn about the functionality exposed
by the synthesizer's library, not to mention learning the programming
language itself. These are the primary motivators behind supporting
well-defined declarative syntaxes.

Of course, low-level control is still important, especially when
external libraries must be used to implement functionality not
inherently exposed by a synthesizer. For this reason, some synthesizers
support both declarative and imperative scripts to generate scenarios.
For example, the Python-based hystck and ForTrace frameworks
\cite{gobelNovelApproachGenerating2020,gobelForTraceHolisticForensic2022}
allow users to write YAML scripts to execute actions. Support for both
formats can be implemented through various means, such as
declarative-to-imperative translators. (While not explored in this
thesis, it is also worth noting the GUI-based interfaces provided by
Yannikos et al. \cite{yannikosDataCorporaDigital2014} and ForGe
\cite{vistiAutomaticCreationComputer2015} for building scenarios.)

AKF supports an imperative syntax (through its Python API) and a custom
declarative syntax. Unlike prior synthesizers, AKF's declarative syntax
supports both execution and declarative-to-imperative translation,
allowing users to quickly create and modify imperative scripts from
high-level declarative descriptions.

\section{Setup and basic usage}\label{setup-and-basic-usage}

Like many of its predecessors, AKF implements its functionality and
exposes its API in the same language, Python 3. There are numerous
advantages to a Python-based API; besides the relatively low difficulty
of setting up and using Python, its rich ecosystem allows scenarios to
be extended through other libraries from the Python ecosystem. For
example, if a user wanted to conditionally execute certain parts of a
scenario by testing if a particular remote service is currently online,
a user could use the Requests library \cite{Requests31Documentation}
to issue an HTTP request out-of-band before performing the same action
in a virtualized environment.

Users must install two foundational technologies for AKF to operate --
Python 3.11 (or later) and a supported hypervisor. AKF currently only
supports VirtualBox as its hypervisor, though QEMU/libvirt has also been
used in prior synthesizers. AKF uses
\passthrough{\lstinline!pyproject.toml!} to define Python library
dependencies, which can be installed into a virtual environment using a
package manager such as \passthrough{\lstinline!pip!} or
\passthrough{\lstinline!uv!}.

At this point, a virtual machine must be prepared for use with AKF. As
with prior synthesizers, it is possible to manually configure a machine
by downloading a supported operating system and creating a new virtual
machine from scratch. The manual process, which is similar to that of
other synthesizers, is as follows:

\begin{itemize}
\tightlist
\item
  Download an ISO or pre-prepared virtual machine from a distributor
  with the desired operating system.
\item
  If necessary, install the operating system on a new virtual machine.
\item
  Configure the virtual machine with the desired host resources,
  including two network interfaces -- one connected to the NAT adapter
  for general internet usage and one connected to the host-only adapter
  for agent communications.
\item
  Create an administrative user with known credentials. Configure the
  operating system as desired to reduce friction with the synthesizer
  (such as disabling UAC prompts, enabling auto-logon, and so on.)
\item
  Build and copy the OS-specific AKF agent to the virtual machine,
  configuring it as a startup application. Add firewall rules to ensure
  that the host and agent are able to communicate.
\end{itemize}

After this process, the virtual machine can be cloned and reused in
multiple AKF scenarios. Although relatively straightforward, this
process is still time-consuming, especially when adapting this process
to new operating systems. While a prepared AKF virtual machine can
theoretically be distributed (in a virtual appliance format such as
OVF), this can run into legal issues if the software on the underlying
operating system is copyrighted.

As a result, AKF uses modern infrastructure-as-code solutions to vastly
simplify the setup of new virtual machines. Vagrant, developed by
HashiCorp, is a tool for rapidly building development environments
\cite{HashicorpVagrant2025}. It allows users to define and build
virtual machines on several virtualization platforms, including
VirtualBox and VMWare. Virtual machines are built by configuring a base
image according to a Vagrantfile, which describes hypervisor-specific
configuration options and instructions to configure the machine. The
Vagrantfile can be distributed to users, allowing them to build the same
virtual machine without distributing virtual drives. (A similar approach
of distributing the ``differences'' of base images is used to reduce the
size of distributed forensic datasets by EviPlant
\cite{scanlonEviPlantEfficientDigital2017}, as described in
\autoref{distribution}.)

The AKF Windows agent includes a Vagrantfile for creating a new Windows
11 virtual machine with the agent installed and configured, which can
easily be adapted for other platforms and hypervisors. The
Vagrantfile(s) used to generate a dataset should be included with the
dataset itself to maximize reproducibility, as described in \autoref{distribution-and-community-reproducibility}. A robust ecosystem of Vagrant boxes for varying Linux
distributions and Windows versions exists, many of which can be pulled
from the Vagrant public registry
\cite{hashicorpHashiCorpCloudPlatform}. When combined with the
flexibility of Vagrant over multiple virtualization platforms, this can
significantly improve the reproducibility and usability of AKF across
many platforms. It should also be noted that Vagrant can configure and
build larger environments with multiple machines. For organizations that
can express corporate environments as Vagrantfiles, AKF could perform
artifact generation at scale, allowing for incident response scenarios
reflecting real-world networks and events.

Following setup, developers can build scenarios using the AKF core
libraries (\passthrough{\lstinline!akflib!}) and the API of the
platform-specific agent installed onto the virtual machine. This
reflects typical imperative usage, in which environment setup, artifact
generation, and output generation are handled explicitly through a
script executed through the Python interpreter.

A simple AKF script achieving the same outcomes as the ForTrace and SFX
scripts above follows:

\section{Declarative usage}\label{declarative-usage}

As described previously, declarative inputs are well-structured files
with a fixed set of available actions -- effectively forming an API --
where each entry in the file specifies an action or artifact to be
generated as part of a dataset. Individual entries may contain
additional configuration data that modifies how that specific action or
artifact is generated. An \emph{interpreter} is responsible for parsing
and acting on the entries in the declarative file.

Interpreters can act on declarative formats in one of two ways. The
first is \emph{execution}, in which the elements of the declarative
script are directly interpreted to generate imperative API calls. This
is characteristic of synthesizers that only support declarative script
inputs, exposing no low-level APIs. Execution takes advantage of the
high-level nature of declarative scripts; a declarative script can
remain the same even if the libraries that execute it change, so long as
the interpreter is updated accordingly. The second is
\emph{translation}, in which the declarative script is used to generate
an equivalent imperative script adhering to a particular synthesizer's
API. This allows the declarative script to be used as a ``base'' for
creating imperative scripts, such that an experienced scenario developer
can modify the generated imperative script as needed. Imperative scripts
can be regenerated from the same declarative script to reflect updates
in library usage so long as the interpreter is updated accordingly,
inheriting the perennial nature of declarative scripts.

It is important to note that a declarative syntax, which may be more
``rigid'' in structure, does not preclude the use of non-determinism or
randomness. One notable example of this is the discrete-time Markov
chains used by Yannikos et al.~to express scenarios in a probabilistic
manner, with each state of the Markov chain representing a particular
action taken by the synthesizer (such as sending an email or deleting a
file) \cite{yannikosDataCorporaDigital2014}. These chains are
evaluated at runtime to generate multiple unique datasets from a single
description.

The challenges of defining a suitable declarative syntax for a
particular synthesizer are not unlike the challenges faced in general
programming language design. There are several key factors to the
success of imperative programming languages that extend to declarative
syntaxes, some of which are derived from Finkel and described as follows
\cite{finkel1996advanced}:

\begin{itemize}
\tightlist
\item
  The language should be \textbf{simple}, using as few basic concepts as
  possible. This makes code easier to read and write, an important
  aspect for users with limited programming experience.
\item
  The language should be \textbf{modular}, such that the role and
  interfaces of individual program units are clear.
\item
  The language should be \textbf{predictable}, such that users can apply
  their existing knowledge of a synthesizer to quickly implement or add
  new features to a scenario.
\item
  The language should \textbf{abstract} as much as possible away from
  the user, such that the minimum information needed to fulfill artifact
  generation is exposed to the user.
\end{itemize}

The most important factor, however, is an awareness of the
\emph{purpose} of the declarative syntax. The purpose of a synthesizer
is to make it easier to generate forensic artifacts and datasets. The
declarative language should reflect this, focusing on making actions and
artifacts as easy to declare and customize as possible.

In designing the AKF declarative syntax, the declarative syntaxes of
prior synthesizers and unrelated technologies were evaluated. An
analysis of some of these syntaxes, with examples, is described briefly
in \autoref{historical-declarative-syntaxes}.
However, two syntaxes in particular contributed the most to the AKF
declarative syntax: those of ForTrace and Ansible, described in the
following section.

\subsection{Existing declarative
syntaxes}\label{existing-declarative-syntaxes}

ForTrace \cite{gobelForTraceHolisticForensic2022} uses YAML to
express scenarios in a declarative manner. ForTrace significantly
influenced both the AKF declarative syntax itself and the implementation
of the declarative interpreter, primarily because it was the sole
synthesizer with both imperative and declarative support known to be
open-source.

Below is a simple example of a ForTrace declarative scenario:

\begin{lstlisting}
name: haystack-example
description: A example action suite to generate a haystack (traffic)
author: MPSE Group
seed: 1234
collections:
  c-http-0:
    type: http
    urls: ./generator/friendly_urls.txt
settings:
  host_nfs_path:
  guest_nfs_path:
applications:
hay:
  h-http-0:
    application: http
    amount: 3
    collection: c-http-0
needles:
  n-http-0:
    application: http
    file: [https://dasec.h-da.de/](https://dasec.h-da.de/)
    amount: 1
dumps:
  d-dump-0:
    dump-type: mem
    dump-path: /home/fortrace/gendump.file
\end{lstlisting}

At a high level, ForTrace scenarios contain five distinct elements:

\begin{itemize}
\tightlist
\item
  Metadata about the scenario, such as the scenario's name, description,
  and author.
\item
  ``Collections'' of data that can be reused throughout the scenario in
  supported application types, such as a newline-separated list of URLs.
\item
  Configuration options that may be applied to all actions of a
  particular action type or the entire scenario.
\item
  The actual artifacts to create as part of the
  \passthrough{\lstinline!hay!} and \passthrough{\lstinline!needles!}
  sections, where \passthrough{\lstinline!hay!} includes artifacts that
  should be considered background noise, and
  \passthrough{\lstinline!needles!} includes artifacts that should be
  considered significant. Each artifact contains a unique ID, an
  application type (the \passthrough{\lstinline!application!} key), and
  arguments specific to the application responsible for generating the
  artifact, such as the URLs for web browsing.
\item
  Any core outputs that should be created as part of the scenario.
\end{itemize}

This file is passed into a ``generator,'' which parses the contents of
the YAML file to prepare various internal data structures, initialize
the virtual machine, and then execute the actions specified in the
\passthrough{\lstinline!hay!} and \passthrough{\lstinline!needles!}
sections in a random order according to the
\passthrough{\lstinline!seed!} key. Depending on the value of the
\passthrough{\lstinline!application!} key, the data for that action is
passed to an application-specific handler that interacts with the
running virtual machine using existing ForTrace libraries. Once all the
actions have been executed, the generator creates any requested outputs
(such as volatile memory dumps) and shuts down the virtual machine.

This analysis provided insight into the design decisions and
functionality required to execute actions from the high-level
descriptions of a scenario. In particular, it demonstrates the need for
actions or artifacts to be defined consistently and flexibly so that
program state and other data can be passed to application-specific
libraries as needed. It also demonstrates the need for various levels of
configuration, including scenario-wide configuration,
application-specific configuration, and action/artifact-specific
configuration. ForTrace implements these features in a somewhat awkward
manner; in fact, nearly all declarative language support is contained in
a single file, with a hardcoded ``router'' handling each unique
\passthrough{\lstinline!application!} type. This makes it difficult to
add support for new applications without significant effort,
particularly because the generator must be aware of every possible
action/artifact type ahead of time.

With these priorities and issues from ForTrace identified, are there
ideas from other technologies that can be used to address them? That is,
are there other technologies designed to execute a large set of complex
actions using a simple but flexible and configurable syntax, and how
does it work? Ansible, the second major inspiration for the AKF
declarative syntax, precisely fills this need.

Ansible \cite{AnsibleAnsible2025} is an open-source automation
framework often used to remotely configure Windows and Linux machines at
scale, allowing organizations to manage many machines at once without
installing orchestration software on these machines in advance. To
achieve this, users write \emph{playbooks}, which are simple YAML files
that contain one or more \emph{plays}. Each play is simply a set of
\emph{tasks} run on multiple machines simultaneously, and each task
depends on a \emph{module} designed to achieve a single, specific
outcome.

The following is a simple Ansible playbook with a single play, derived
from the official Ansible documentation for playbooks
\cite{ansibleprojectcontributorsAnsiblePlaybooks}:

\begin{lstlisting}

- name: Update web servers
  hosts: webservers
  remote_user: root
  tasks:
  - name: Ensure apache is at the latest version
    ansible.builtin.yum:
      name: httpd
      state: latest
  - name: Write the apache config file
    ansible.builtin.template:
      src: /srv/httpd.j2
      dest: /etc/httpd.conf
\end{lstlisting}

This play uses the \passthrough{\lstinline!yum!} package manager to
install Apache before copying a local configuration file to the remote
host. \passthrough{\lstinline!ansible.builtin.yum!} and
\passthrough{\lstinline!ansible.builtin.template!} are both modules,
which accept parameters passed as a YAML dictionary. These modules are
part of the \passthrough{\lstinline!ansible.builtin!} collection
included with all default Ansible installations.

Although Ansible contains many features that contribute to its
flexibility, the two important concepts relevant to AKF are \emph{roles}
and \emph{modules}. Roles are a collection of Ansible resources that
typically achieve some ``larger'' reproducible goal, typically by
running multiple tasks and leveraging variables, configuration options,
and files included as part of the role. Roles can include
\emph{modules}, which are standalone imperative scripts (typically in
Python) that accept arguments, execute code based on those arguments,
and return data using well-structured interfaces. As shown above, these
modules can be called and executed from playbooks; they can also be
executed independently on the command line.

These concepts are highly relevant to synthesizers, which must support
application-specific actions and group these actions together in a
flexible, well-defined manner. As described in \autoref{the-akf-agent}, each RPyC subservice of an AKF agent
exposes a group of application-specific automation methods. The
functionality of each of these groups must be re-exposed in a
declarative manner, which can be achieved by adapting the concept of
Ansible roles and modules to AKF.

Together, the ForTrace and Ansible syntaxes provide three concepts that
are reflected in the AKF syntax, described further in the following
section:

\begin{itemize}
\tightlist
\item
  The overall structure and contents of the declarative YAML file.
\item
  An action syntax that allows us to declare individual actions,
  referring to those actions by name, and pass arguments directly to
  that action for \emph{translation} or \emph{execution}.
\item
  A modular architecture that allows us to define the supported
  arguments of each action and expose them to the declarative
  interpreter while also being decoupled from the standard imperative
  library as much as possible.
\end{itemize}

\subsection{The AKF declarative
syntax}\label{the-akf-declarative-syntax}

The AKF declarative syntax is very similar to the Ansible playbook
syntax. Declarative scripts are comprised of metadata, global
configuration, libraries to import, and individual tasks to execute as
part of the scenario. Each task refers to a single \emph{module} by name
using a qualified Python import path, accepting a dictionary of
arguments in addition to global configuration overrides.

Like Ansible, individual modules are implemented as well-defined
subclasses of \passthrough{\lstinline!AKFModule!}, an abstract base
class that serves as the root of all AKF declarative modules. Each
module must define Pydantic models that specify the arguments accepted
by the module. The arguments declared in the YAML file are then passed
to these module-specific Pydantic models for validation, after which the
\passthrough{\lstinline!AKFModule!} must perform one of two tasks:

\begin{itemize}
\tightlist
\item
  \textbf{Execution}: When instructed to perform actions directly from
  the declarative script, the \passthrough{\lstinline!AKFModule!} should
  import AKF core libraries and agent APIs to perform the required
  actions.
\item
  \textbf{Translation}: When instructed to translate the declarative
  script, the \passthrough{\lstinline!AKFModule!} should generate the
  equivalent code that \emph{would} perform related actions if executed
  through a standard Python interpreter with the necessary libraries
  installed.
\end{itemize}

The ability of AKF to both execute and translate declarative scripts
provides significant flexibility to scenario developers. To the best of
our knowledge, prior synthesizers have only supported direct execution
from declarative scripts, which limits the opportunities to use
declarative scripts as a ``starting point'' for writing more complex
imperative scripts.

In both cases, modules can read and modify a global state dictionary
that allows otherwise independent modules to cooperate. For example,
suppose that a module creates a new context manager in a
\passthrough{\lstinline!with!} block, causing the indentation level of
the translated code to increase. Successive modules can read the state
dictionary to retrieve variable names and correctly indent generated
code. Additionally, this allows for ``outputs,'' such as CASE bundles,
to be passed and gradually constructed across modules. This design
allows for context-aware code generation and action execution.

An example of a declarative AKF scenario, carrying out the same actions
as the SFX, ForTrace, and imperative AKF script above, is shown below:

\begin{lstlisting}
name: sample scenario
description: sample scenario
author: lgactna
seed: "0"
libraries:
  - akflib.actions
actions:
  - name: Run the sample action
    module: akflib.actions.sample.SampleModule
    args:
      arg1: "value1"
      arg2: "value2"
\end{lstlisting}

AKF declarative scripts, which are simply large YAML dictionaries,
contain three distinct elements. The first is a set of high-level
metadata keys associated with the scenario. The second is a set of
scenario-wide configurations; in particular, it lists the libraries
containing the necessary modules to execute or translate this imperative
script. Finally, scripts list a sequence of actions, typically a
\passthrough{\lstinline!module!} specified by name and a dictionary of
\passthrough{\lstinline!args!}.

The execution flow of the declarative interpreter itself is relatively
straightforward. Given a path to a YAML script, the interpreter will
load the necessary libraries and configuration keys defined in the file
and instantiate resources accordingly. This may include setting global
configuration variables, locating and starting a virtual machine by
name, setting the \passthrough{\lstinline!random!} seed, and so on.
Then, the interpreter runs each module under the
\passthrough{\lstinline!actions!} key with the provided arguments and
configuration in order, continuing until all
\passthrough{\lstinline!actions!} have been completed.

Modules are located and executed using Python's dynamic import system.
(For efficiency and safety, all modules in the script are located and
``cached'' at the start of script execution.) These modules can be
located in any library so long as they can be found through Python's
import system. For example, both \passthrough{\lstinline!akflib!} and
the AKF Windows agent contain their own declarative module libraries,
leveraging functionality specific to each code repository.

This design allows declarative modules to be written independently of
the libraries they depend on, reducing the ``impact'' of supporting
declarative features on the core imperative libraries. In fact, this
independence allows for the AKF module system to be used in
general-purpose scripting, similar to Ansible; it is not tightly bound
to the creation of forensic scenarios and artifacts.

While the AKF imperative and declarative scripts provide users with
significant flexibility in \emph{using} AKF, the challenge of building
artifacts and scenarios to generate through these execution options
remains. The remainder of this chapter addresses this challenge.

\section{Using generative AI for individual
artifacts}\label{using-generative-ai-for-individual-artifacts}

Users of synthesizers must still perform a significant amount of work
when generating individual artifacts. More precisely -- while AKF and
other synthesizers can streamline the process of placing artifacts on a
dataset, users must still provide the actual artifacts themselves. For
example:

\begin{itemize}
\tightlist
\item
  If a user wants to place 100 photos on the drive to simulate real
  usage, the user needs to create and provide 100 realistic images.
\item
  If a user wants to simulate an email or other online conversation, the
  user needs to provide the entirety of the conversation to simulate.
\item
  If a user wants to generate ``proprietary'' documents to emulate some
  form of corporate sabotage, the user would need to create and provide
  a variety of Microsoft Office, PDF, or other files in these formats.
\end{itemize}

This is particularly relevant when adding background noise often present
in real-world datasets, namely the gigabytes of documents created as
part of a user's benign activity over time. Although creating forensic
datasets can be accomplished with the work presented thus far, creating
realistic images that reflect real-world scenarios still requires
extensive work. While it is true that images should often be small
enough to allow students to explore a single specific technique,
real-world scenarios encountered by analysts are rarely limited by time
or size. An analyst might have to deal with a disk image used over a
decade to store many photographs and send many messages. Such scenarios
are valuable training material for courses that encapsulate a long
period of forensic study, allowing students to apply many techniques
learned throughout a forensic curriculum to analyze a realistic
scenario.

With recent advancements in generative AI, popularized by services such
as Midjourney and ChatGPT, it is now significantly easier to generate
realistic images and text content from short, high-level descriptions.
Additionally, various services exist for creating realistic audio and
video files that emulate a particular person's voice or facial
movements; these can be used to generate additional scenario content of
interest, especially if the scenario is based on a real-world event.

It holds that generative AI can quickly populate forensic datasets with
realistic conversations and images consistent with an arbitrary
scenario. For example, a corporate espionage case could be built by
providing a large language model such as ChatGPT with prompts to
describe complex machinery in both technical and conversational styles.
Simultaneously, similar prompts can be passed into an image synthesizer
such as Midjourney to produce related images. The images and text
produced can then be used to create documents describing an unreleased
product of high value, providing a pipeline through which significant
artifacts can be placed onto a forensic dataset.

This idea can be extended further by training models on specific
datasets; for example, if an instructor wished to create a fictional
scenario in which a user frequently interacts with users of a particular
online community, a large language model could be trained on available
conversations to provide a degree of realism to the scenario. However,
as mentioned before, this faces the challenges of ownership, privacy,
and legality behind works derived from publicly available information
that was (likely) not published with the expectation of its usage in an
AI model.

It is important to note that the inclusion of generative AI into
synthesizers does not necessarily require deep integration with the
framework itself. Many existing synthesizers could be extended to use
documents, images, or other data sourced from generative AI instead of
user-defined files without the need to change the synthesizer's
architecture. However, as advancements in AI continue, it may make sense
to directly integrate AI-driven actions into synthesizers. For example,
there may come a time in which synthesizers can be provided natural
language prompts (such as ``Open Firefox and browse to news-related
websites'') that directly lead to the generation of relevant artifacts
without the need to explicitly program the process of browsing to a
website in advance.
